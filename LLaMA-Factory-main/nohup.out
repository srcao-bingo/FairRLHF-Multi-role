[2024-05-26 12:39:36,837] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:39:38,711] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-26 12:39:38,733] [INFO] [runner.py:568:main] cmd = /home/student2021/anaconda3/envs/llama_factory/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None src/train_bash.py --stage rm --do_train --model_name_or_path /home/student2021/srcao/base-model/llama2-7b --create_new_adapter --dataset multi_role --val_size 0.05 --template default --finetuning_type lora --lora_alpha 16 --lora_dropout 0.05 --lora_rank 8 --lora_target q_proj,v_proj --output_dir rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200 --evaluation_strategy steps --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type linear --warmup_steps 200 --max_length 512 --logging_steps 10 --save_steps 1000 --learning_rate 3e-4 --num_train_epochs 1.0 --plot_loss --fp16 --report_to=wandb
[2024-05-26 12:39:40,622] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:39:42,288] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-05-26 12:39:42,288] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-05-26 12:39:42,288] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-05-26 12:39:42,288] [INFO] [launch.py:163:main] dist_world_size=4
[2024-05-26 12:39:42,289] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-05-26 12:39:42,289] [INFO] [launch.py:253:main] process 18569 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=0', '--stage', 'rm', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--create_new_adapter', '--dataset', 'multi_role', '--val_size', '0.05', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200', '--evaluation_strategy', 'steps', '--per_device_eval_batch_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '512', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '3e-4', '--num_train_epochs', '1.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:39:42,290] [INFO] [launch.py:253:main] process 18570 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=1', '--stage', 'rm', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--create_new_adapter', '--dataset', 'multi_role', '--val_size', '0.05', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200', '--evaluation_strategy', 'steps', '--per_device_eval_batch_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '512', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '3e-4', '--num_train_epochs', '1.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:39:42,290] [INFO] [launch.py:253:main] process 18571 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=2', '--stage', 'rm', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--create_new_adapter', '--dataset', 'multi_role', '--val_size', '0.05', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200', '--evaluation_strategy', 'steps', '--per_device_eval_batch_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '512', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '3e-4', '--num_train_epochs', '1.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:39:42,291] [INFO] [launch.py:253:main] process 18572 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=3', '--stage', 'rm', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--create_new_adapter', '--dataset', 'multi_role', '--val_size', '0.05', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200', '--evaluation_strategy', 'steps', '--per_device_eval_batch_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '512', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '3e-4', '--num_train_epochs', '1.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:39:46,317] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:39:46,317] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:39:46,323] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:39:46,340] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
05/26/2024 12:39:51 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/26/2024 12:39:51 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/26/2024 12:39:51 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1934] 2024-05-26 12:39:51,215 >> PyTorch: setting up devices
05/26/2024 12:39:51 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 25, in run_exp
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/hparams/parser.py", line 200, in get_train_args
    raise ValueError("Output directory already exists and is not empty. Please set `overwrite_output_dir`.")
ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 25, in run_exp
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/hparams/parser.py", line 200, in get_train_args
    raise ValueError("Output directory already exists and is not empty. Please set `overwrite_output_dir`.")
ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 25, in run_exp
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/hparams/parser.py", line 200, in get_train_args
    raise ValueError("Output directory already exists and is not empty. Please set `overwrite_output_dir`.")
ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 25, in run_exp
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/hparams/parser.py", line 200, in get_train_args
    raise ValueError("Output directory already exists and is not empty. Please set `overwrite_output_dir`.")
ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.
[2024-05-26 12:39:53,304] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 18569
[2024-05-26 12:39:53,317] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 18570
[2024-05-26 12:39:53,317] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 18571
[2024-05-26 12:39:53,326] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 18572
[2024-05-26 12:39:53,333] [ERROR] [launch.py:322:sigkill_handler] ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=3', '--stage', 'rm', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--create_new_adapter', '--dataset', 'multi_role', '--val_size', '0.05', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200', '--evaluation_strategy', 'steps', '--per_device_eval_batch_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '512', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '3e-4', '--num_train_epochs', '1.0', '--plot_loss', '--fp16', '--report_to=wandb'] exits with return code = 1
[2024-05-26 12:40:31,056] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:40:32,834] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-26 12:40:32,857] [INFO] [runner.py:568:main] cmd = /home/student2021/anaconda3/envs/llama_factory/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None src/train_bash.py --stage rm --do_train --model_name_or_path /home/student2021/srcao/base-model/llama2-7b --create_new_adapter --dataset multi_role --val_size 0.05 --template default --finetuning_type lora --lora_alpha 16 --lora_dropout 0.05 --lora_rank 8 --lora_target q_proj,v_proj --output_dir rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200 --evaluation_strategy steps --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type linear --warmup_steps 200 --max_length 512 --logging_steps 10 --save_steps 1000 --learning_rate 3e-4 --num_train_epochs 1.0 --plot_loss --fp16 --report_to=wandb
[2024-05-26 12:40:34,658] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:40:36,318] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-05-26 12:40:36,318] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-05-26 12:40:36,318] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-05-26 12:40:36,318] [INFO] [launch.py:163:main] dist_world_size=4
[2024-05-26 12:40:36,318] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-05-26 12:40:36,319] [INFO] [launch.py:253:main] process 19173 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=0', '--stage', 'rm', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--create_new_adapter', '--dataset', 'multi_role', '--val_size', '0.05', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200', '--evaluation_strategy', 'steps', '--per_device_eval_batch_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '512', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '3e-4', '--num_train_epochs', '1.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:40:36,320] [INFO] [launch.py:253:main] process 19174 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=1', '--stage', 'rm', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--create_new_adapter', '--dataset', 'multi_role', '--val_size', '0.05', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200', '--evaluation_strategy', 'steps', '--per_device_eval_batch_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '512', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '3e-4', '--num_train_epochs', '1.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:40:36,320] [INFO] [launch.py:253:main] process 19175 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=2', '--stage', 'rm', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--create_new_adapter', '--dataset', 'multi_role', '--val_size', '0.05', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200', '--evaluation_strategy', 'steps', '--per_device_eval_batch_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '512', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '3e-4', '--num_train_epochs', '1.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:40:36,321] [INFO] [launch.py:253:main] process 19176 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=3', '--stage', 'rm', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--create_new_adapter', '--dataset', 'multi_role', '--val_size', '0.05', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200', '--evaluation_strategy', 'steps', '--per_device_eval_batch_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '8', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '512', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '3e-4', '--num_train_epochs', '1.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:40:40,298] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:40:40,311] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:40:40,326] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:40:40,380] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
05/26/2024 12:40:46 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/26/2024 12:40:46 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1934] 2024-05-26 12:40:46,073 >> PyTorch: setting up devices
05/26/2024 12:40:46 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/26/2024 12:40:46 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:40:46 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 12:40:46 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=10,
evaluation_strategy=steps,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0003,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200/runs/May26_12-40-45_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:40:46 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 12:40:46 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=10,
evaluation_strategy=steps,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0003,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200/runs/May26_12-40-44_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:40:46 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 12:40:46 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 12:40:46 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=10,
evaluation_strategy=steps,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0003,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200/runs/May26_12-40-45_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
05/26/2024 12:40:46 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=10,
evaluation_strategy=steps,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=8,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0003,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200/runs/May26_12-40-45_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:40:46,137 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:40:46,138 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:40:46,138 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:40:46,138 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:40:46,138 >> loading file tokenizer.json
[INFO|configuration_utils.py:724] 2024-05-26 12:40:46,251 >> loading configuration file /home/student2021/srcao/base-model/llama2-7b/config.json
[INFO|configuration_utils.py:789] 2024-05-26 12:40:46,251 >> Model config LlamaConfig {
  "_name_or_path": "/home/student2021/srcao/base-model/llama2-7b",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3280] 2024-05-26 12:40:46,272 >> loading weights file /home/student2021/srcao/base-model/llama2-7b/model.safetensors.index.json
[INFO|modeling_utils.py:1417] 2024-05-26 12:40:46,273 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-05-26 12:40:46,274 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.49it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.40it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.55it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.55it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.67it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.64it/s]
05/26/2024 12:40:49 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:40:49 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.66it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.62it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.74it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.71it/s]
05/26/2024 12:40:49 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:40:49 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
05/26/2024 12:40:49 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:40:49 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.73it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.70it/s]
[INFO|modeling_utils.py:4024] 2024-05-26 12:40:49,381 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-05-26 12:40:49,381 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/student2021/srcao/base-model/llama2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:881] 2024-05-26 12:40:49,383 >> loading configuration file /home/student2021/srcao/base-model/llama2-7b/generation_config.json
[INFO|configuration_utils.py:928] 2024-05-26 12:40:49,384 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

05/26/2024 12:40:49 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:40:49 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Failed to load value_head.safetensors: /home/student2021/srcao/base-model/llama2-7b does not appear to have a file named value_head.safetensors. Checkout 'https://huggingface.co//home/student2021/srcao/base-model/llama2-7b/tree/None' for available files.
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Failed to load value_head.bin: /home/student2021/srcao/base-model/llama2-7b does not appear to have a file named value_head.bin. Checkout 'https://huggingface.co//home/student2021/srcao/base-model/llama2-7b/tree/None' for available files.
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Provided path (/home/student2021/srcao/base-model/llama2-7b) does not contain value head weights.
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Ignore these messages if you are not resuming the training of a value head model.
05/26/2024 12:40:49 - INFO - llmtuner.model.loader - trainable params: 4198401 || all params: 6742614017 || trainable%: 0.0623
AutoModelForCausalLMWithValueHead(
  (pretrained_model): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(32000, 4096)
          (layers): ModuleList(
            (0-31): 32 x LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLU()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
          )
          (norm): LlamaRMSNorm()
        )
        (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
      )
    )
  )
  (v_head): ValueHead(
    (dropout): Dropout(p=0.1, inplace=False)
    (summary): Linear(in_features=4096, out_features=1, bias=True)
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
)
05/26/2024 12:40:49 - INFO - llmtuner.data.template - Add pad token: </s>
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Failed to load value_head.safetensors: /home/student2021/srcao/base-model/llama2-7b does not appear to have a file named value_head.safetensors. Checkout 'https://huggingface.co//home/student2021/srcao/base-model/llama2-7b/tree/None' for available files.
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Failed to load value_head.bin: /home/student2021/srcao/base-model/llama2-7b does not appear to have a file named value_head.bin. Checkout 'https://huggingface.co//home/student2021/srcao/base-model/llama2-7b/tree/None' for available files.
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Provided path (/home/student2021/srcao/base-model/llama2-7b) does not contain value head weights.
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Ignore these messages if you are not resuming the training of a value head model.
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Failed to load value_head.safetensors: /home/student2021/srcao/base-model/llama2-7b does not appear to have a file named value_head.safetensors. Checkout 'https://huggingface.co//home/student2021/srcao/base-model/llama2-7b/tree/None' for available files.
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Failed to load value_head.bin: /home/student2021/srcao/base-model/llama2-7b does not appear to have a file named value_head.bin. Checkout 'https://huggingface.co//home/student2021/srcao/base-model/llama2-7b/tree/None' for available files.
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Provided path (/home/student2021/srcao/base-model/llama2-7b) does not contain value head weights.
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Ignore these messages if you are not resuming the training of a value head model.
05/26/2024 12:40:49 - INFO - llmtuner.model.loader - trainable params: 4198401 || all params: 6742614017 || trainable%: 0.0623
AutoModelForCausalLMWithValueHead(
  (pretrained_model): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(32000, 4096)
          (layers): ModuleList(
            (0-31): 32 x LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLU()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
          )
          (norm): LlamaRMSNorm()
        )
        (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
      )
    )
  )
  (v_head): ValueHead(
    (dropout): Dropout(p=0.1, inplace=False)
    (summary): Linear(in_features=4096, out_features=1, bias=True)
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
)
05/26/2024 12:40:49 - INFO - llmtuner.data.template - Add pad token: </s>
05/26/2024 12:40:49 - INFO - llmtuner.model.loader - trainable params: 4198401 || all params: 6742614017 || trainable%: 0.0623
AutoModelForCausalLMWithValueHead(
  (pretrained_model): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(32000, 4096)
          (layers): ModuleList(
            (0-31): 32 x LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLU()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
          )
          (norm): LlamaRMSNorm()
        )
        (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
      )
    )
  )
  (v_head): ValueHead(
    (dropout): Dropout(p=0.1, inplace=False)
    (summary): Linear(in_features=4096, out_features=1, bias=True)
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
)
05/26/2024 12:40:49 - INFO - llmtuner.data.template - Add pad token: </s>
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Failed to load value_head.safetensors: /home/student2021/srcao/base-model/llama2-7b does not appear to have a file named value_head.safetensors. Checkout 'https://huggingface.co//home/student2021/srcao/base-model/llama2-7b/tree/None' for available files.
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Failed to load value_head.bin: /home/student2021/srcao/base-model/llama2-7b does not appear to have a file named value_head.bin. Checkout 'https://huggingface.co//home/student2021/srcao/base-model/llama2-7b/tree/None' for available files.
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Provided path (/home/student2021/srcao/base-model/llama2-7b) does not contain value head weights.
05/26/2024 12:40:49 - INFO - llmtuner.model.utils - Ignore these messages if you are not resuming the training of a value head model.
05/26/2024 12:40:49 - INFO - llmtuner.model.loader - trainable params: 4198401 || all params: 6742614017 || trainable%: 0.0623
AutoModelForCausalLMWithValueHead(
  (pretrained_model): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlamaForCausalLM(
        (model): LlamaModel(
          (embed_tokens): Embedding(32000, 4096)
          (layers): ModuleList(
            (0-31): 32 x LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=8, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=8, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLU()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
          )
          (norm): LlamaRMSNorm()
        )
        (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
      )
    )
  )
  (v_head): ValueHead(
    (dropout): Dropout(p=0.1, inplace=False)
    (summary): Linear(in_features=4096, out_features=1, bias=True)
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
)
05/26/2024 12:40:49 - INFO - llmtuner.data.template - Add pad token: </s>
05/26/2024 12:40:49 - INFO - llmtuner.data.loader - Loading dataset multi-role.json...
05/26/2024 12:40:49 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
Using custom data configuration default-e029b3d83685a5d0
Loading Dataset Infos from /home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/student2021/.cache/huggingface/datasets/json/default-e029b3d83685a5d0/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
Downloading and preparing dataset json/default to /home/student2021/.cache/huggingface/datasets/json/default-e029b3d83685a5d0/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05...
Downloading took 0.0 min
Checksum Computation took 0.0 min
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 3451 examples [00:00, 83389.27 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/student2021/.cache/huggingface/datasets/json/default-e029b3d83685a5d0/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05. Subsequent calls will reuse this data.
Converting format of dataset:   0%|          | 0/3451 [00:00<?, ? examples/s]Caching processed dataset at /home/student2021/.cache/huggingface/datasets/json/default-e029b3d83685a5d0/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-d2d986a201ab4e1f.arrow
Converting format of dataset: 100%|██████████| 3451/3451 [00:00<00:00, 50898.96 examples/s]
05/26/2024 12:40:52 - INFO - llmtuner.data.loader - Loading dataset multi-role.json...
05/26/2024 12:40:52 - INFO - llmtuner.data.loader - Loading dataset multi-role.json...
05/26/2024 12:40:52 - INFO - llmtuner.data.loader - Loading dataset multi-role.json...
05/26/2024 12:40:52 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
05/26/2024 12:40:52 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
05/26/2024 12:40:52 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
Running tokenizer on dataset:   0%|          | 0/3451 [00:00<?, ? examples/s]Caching processed dataset at /home/student2021/.cache/huggingface/datasets/json/default-e029b3d83685a5d0/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-77a4de8b2c000431.arrow
Running tokenizer on dataset:  29%|██▉       | 1000/3451 [00:02<00:07, 340.02 examples/s]Running tokenizer on dataset:  58%|█████▊    | 2000/3451 [00:05<00:04, 338.55 examples/s]Running tokenizer on dataset:  87%|████████▋ | 3000/3451 [00:08<00:01, 337.02 examples/s]Running tokenizer on dataset: 100%|██████████| 3451/3451 [00:10<00:00, 337.21 examples/s]Running tokenizer on dataset: 100%|██████████| 3451/3451 [00:10<00:00, 337.53 examples/s]
prompt_ids:
[12968, 29901, 13355, 1598, 322, 5353, 278, 2775, 537, 310, 5046, 269, 12358, 327, 7384, 322, 4768, 2129, 2729, 373, 278, 7928, 434, 29889, 13, 2677, 29901, 8565, 1558, 291, 1048, 29871, 29896, 29941, 29955, 29889, 16767, 830, 1548, 1860, 363, 5682, 284, 2087, 352, 386, 2092, 29889, 13, 13, 7900, 22137, 29901, 29871]
prompt:
Human: Identify and discuss the severity of age stereotypes and biases based on the dialogue.
Context: Discussion about 137. Age Requirements for Legal Adulthood.

Assistant: 
chosen_ids:
[306, 4658, 393, 278, 11706, 594, 352, 386, 2092, 5046, 11809, 881, 3933, 472, 29871, 29896, 29947, 2440, 2030, 29889, 10443, 16157, 29879, 472, 278, 5046, 310, 29871, 29896, 29947, 526, 6892, 5545, 286, 1535, 3307, 304, 1207, 14040, 1602, 12112, 322, 5221, 403, 8072, 297, 12459, 29889, 910, 5046, 17997, 263, 9558, 515, 594, 6544, 29883, 663, 304, 594, 352, 386, 2092, 29892, 988, 15724, 526, 2183, 901, 10462, 322, 5544, 747, 9770, 29889, 5806, 777, 1122, 27754, 393, 29871, 29896, 29947, 338, 2086, 4123, 322, 393, 278, 5046, 881, 367, 10425, 29892, 306, 1348, 393, 10231, 278, 5046, 11809, 1033, 4046, 28602, 1907, 363, 4123, 16157, 29879, 304, 26987, 322, 2693, 1009, 21820, 29889, 313, 22110, 280, 7928, 434, 297, 278, 2186, 4513, 29897, 2]
chosen:
I believe that the legal adulthood age requirement should remain at 18 years old. Young adults at the age of 18 are generally considered mature enough to make responsible decisions and participate fully in society. This age marks a transition from adolescence to adulthood, where individuals are given more rights and responsibilities. While some may argue that 18 is too young and that the age should be raised, I think that increasing the age requirement could limit opportunities for young adults to explore and develop their independence. (Whole dialogue in the final round)</s>
rejected_ids:
[1094, 263, 29871, 29946, 29900, 29899, 6360, 29899, 1025, 5375, 22545, 1218, 363, 29263, 278, 11706, 594, 352, 386, 2092, 5046, 304, 29871, 29906, 29896, 29892, 306, 4658, 393, 278, 11706, 594, 352, 386, 2092, 5046, 11809, 881, 367, 10425, 515, 29871, 29896, 29947, 304, 29871, 29906, 29896, 2440, 2030, 29889, 2180, 29871, 29906, 29896, 29892, 15724, 505, 5517, 8676, 1009, 9793, 29892, 17515, 901, 2834, 7271, 29892, 322, 505, 263, 2253, 8004, 310, 1009, 5544, 747, 9770, 29889, 910, 5046, 7910, 1033, 1371, 10032, 2411, 7273, 573, 10608, 29899, 28990, 322, 2253, 19012, 4123, 16157, 29879, 363, 278, 18066, 267, 310, 594, 352, 386, 2092, 29889, 313, 22110, 280, 7928, 434, 297, 278, 937, 4513, 29897, 2]
rejected:
As a 40-year-old individual advocating for raising the legal adulthood age to 21, I believe that the legal adulthood age requirement should be raised from 18 to 21 years old. At 21, individuals have likely completed their education, gained more life experience, and have a better understanding of their responsibilities. This age increase could help reduce impulsive decision-making and better prepare young adults for the challenges of adulthood. (Whole dialogue in the first round)</s>
[INFO|training_args.py:1934] 2024-05-26 12:41:02,543 >> PyTorch: setting up devices
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Caching indices mapping at /home/student2021/.cache/huggingface/datasets/json/default-e029b3d83685a5d0/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-31d6226af21ded39.arrow
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Caching indices mapping at /home/student2021/.cache/huggingface/datasets/json/default-e029b3d83685a5d0/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-c6646b2d2f1912df.arrow
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:607] 2024-05-26 12:41:07,045 >> Using auto half precision backend
[INFO|trainer.py:1969] 2024-05-26 12:41:07,430 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-05-26 12:41:07,430 >>   Num examples = 3,278
[INFO|trainer.py:1971] 2024-05-26 12:41:07,430 >>   Num Epochs = 1
[INFO|trainer.py:1972] 2024-05-26 12:41:07,430 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-05-26 12:41:07,430 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1976] 2024-05-26 12:41:07,430 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:1977] 2024-05-26 12:41:07,430 >>   Total optimization steps = 102
[INFO|trainer.py:1978] 2024-05-26 12:41:07,434 >>   Number of trainable parameters = 4,198,401
[INFO|integration_utils.py:723] 2024-05-26 12:41:07,654 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: srcao (srcao-bingo). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /home/student2021/srcao/LLaMA-Factory-main/wandb/run-20240526_124108-z6qz55sv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-blaze-192
wandb: ⭐️ View project at https://wandb.ai/srcao-bingo/huggingface
wandb: 🚀 View run at https://wandb.ai/srcao-bingo/huggingface/runs/z6qz55sv/workspace
  0%|          | 0/102 [00:00<?, ?it/s]  1%|          | 1/102 [00:03<05:35,  3.33s/it]  2%|▏         | 2/102 [00:06<05:08,  3.08s/it]  3%|▎         | 3/102 [00:09<04:58,  3.01s/it]  4%|▍         | 4/102 [00:12<04:53,  3.00s/it]  5%|▍         | 5/102 [00:15<04:47,  2.97s/it]  6%|▌         | 6/102 [00:18<04:50,  3.03s/it]  7%|▋         | 7/102 [00:20<04:37,  2.92s/it]  8%|▊         | 8/102 [00:24<04:42,  3.00s/it]  9%|▉         | 9/102 [00:27<04:46,  3.08s/it] 10%|▉         | 10/102 [00:30<04:44,  3.10s/it]                                                {'loss': 0.7048, 'grad_norm': 5.9525980949401855, 'learning_rate': 1.05e-05, 'epoch': 0.1}
 10%|▉         | 10/102 [00:30<04:44,  3.10s/it][INFO|trainer.py:3512] 2024-05-26 12:41:45,378 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-26 12:41:45,378 >>   Num examples = 173
[INFO|trainer.py:3517] 2024-05-26 12:41:45,378 >>   Batch size = 1

  0%|          | 0/44 [00:00<?, ?it/s][A
  5%|▍         | 2/44 [00:00<00:03, 10.56it/s][A
  9%|▉         | 4/44 [00:00<00:04,  9.46it/s][A
 11%|█▏        | 5/44 [00:00<00:04,  8.64it/s][A
 14%|█▎        | 6/44 [00:00<00:04,  8.07it/s][A
 16%|█▌        | 7/44 [00:00<00:04,  7.87it/s][A
 18%|█▊        | 8/44 [00:01<00:05,  6.90it/s][A
 20%|██        | 9/44 [00:01<00:05,  6.10it/s][A
 23%|██▎       | 10/44 [00:01<00:05,  6.61it/s][A
 25%|██▌       | 11/44 [00:01<00:04,  6.79it/s][A
 27%|██▋       | 12/44 [00:01<00:04,  7.19it/s][A
 30%|██▉       | 13/44 [00:01<00:04,  7.27it/s][A
 32%|███▏      | 14/44 [00:01<00:03,  7.90it/s][A
 36%|███▋      | 16/44 [00:02<00:03,  8.09it/s][A
 39%|███▊      | 17/44 [00:02<00:03,  7.86it/s][A
 41%|████      | 18/44 [00:02<00:03,  7.93it/s][A
 43%|████▎     | 19/44 [00:02<00:03,  7.77it/s][A
 45%|████▌     | 20/44 [00:02<00:03,  6.92it/s][A
 48%|████▊     | 21/44 [00:02<00:03,  6.18it/s][A
 50%|█████     | 22/44 [00:02<00:03,  6.61it/s][A
 52%|█████▏    | 23/44 [00:03<00:03,  6.75it/s][A
 55%|█████▍    | 24/44 [00:03<00:03,  5.76it/s][A
 57%|█████▋    | 25/44 [00:03<00:03,  5.44it/s][A
 59%|█████▉    | 26/44 [00:03<00:02,  6.01it/s][A
 61%|██████▏   | 27/44 [00:03<00:02,  6.31it/s][A
 64%|██████▎   | 28/44 [00:03<00:02,  6.50it/s][A
 66%|██████▌   | 29/44 [00:04<00:02,  6.86it/s][A
 68%|██████▊   | 30/44 [00:04<00:01,  7.22it/s][A
 73%|███████▎  | 32/44 [00:04<00:01,  7.83it/s][A
 75%|███████▌  | 33/44 [00:04<00:01,  7.60it/s][A
 77%|███████▋  | 34/44 [00:04<00:01,  7.49it/s][A
 80%|███████▉  | 35/44 [00:04<00:01,  7.60it/s][A
 82%|████████▏ | 36/44 [00:05<00:01,  7.47it/s][A
 84%|████████▍ | 37/44 [00:05<00:00,  7.46it/s][A
 86%|████████▋ | 38/44 [00:05<00:00,  6.68it/s][A
 89%|████████▊ | 39/44 [00:05<00:00,  7.08it/s][A
 91%|█████████ | 40/44 [00:05<00:00,  5.73it/s][A
 93%|█████████▎| 41/44 [00:05<00:00,  6.11it/s][A
 95%|█████████▌| 42/44 [00:05<00:00,  6.54it/s][A
 98%|█████████▊| 43/44 [00:06<00:00,  6.69it/s][A
100%|██████████| 44/44 [00:06<00:00,  6.20it/s][A                                                
                                               [A{'eval_loss': 0.7108611464500427, 'eval_accuracy': 0.36416184971098264, 'eval_runtime': 6.4524, 'eval_samples_per_second': 26.812, 'eval_steps_per_second': 6.819, 'epoch': 0.1}
 10%|▉         | 10/102 [00:36<04:44,  3.10s/it]
100%|██████████| 44/44 [00:06<00:00,  6.20it/s][A
                                               [A 11%|█         | 11/102 [00:39<07:33,  4.98s/it] 12%|█▏        | 12/102 [00:42<06:36,  4.40s/it] 13%|█▎        | 13/102 [00:46<06:01,  4.06s/it] 14%|█▎        | 14/102 [00:49<05:28,  3.73s/it] 15%|█▍        | 15/102 [00:52<05:06,  3.52s/it] 16%|█▌        | 16/102 [00:55<04:52,  3.41s/it] 17%|█▋        | 17/102 [00:58<04:40,  3.30s/it] 18%|█▊        | 18/102 [01:01<04:31,  3.23s/it] 19%|█▊        | 19/102 [01:04<04:31,  3.28s/it] 20%|█▉        | 20/102 [01:07<04:26,  3.25s/it]                                                {'loss': 0.7035, 'grad_norm': 5.541669845581055, 'learning_rate': 2.55e-05, 'epoch': 0.2}
 20%|█▉        | 20/102 [01:07<04:26,  3.25s/it][INFO|trainer.py:3512] 2024-05-26 12:42:22,826 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-26 12:42:22,827 >>   Num examples = 173
[INFO|trainer.py:3517] 2024-05-26 12:42:22,827 >>   Batch size = 1

  0%|          | 0/44 [00:00<?, ?it/s][A
  5%|▍         | 2/44 [00:00<00:04, 10.45it/s][A
  9%|▉         | 4/44 [00:00<00:04,  9.38it/s][A
 11%|█▏        | 5/44 [00:00<00:04,  8.55it/s][A
 14%|█▎        | 6/44 [00:00<00:04,  7.99it/s][A
 16%|█▌        | 7/44 [00:00<00:04,  7.80it/s][A
 18%|█▊        | 8/44 [00:01<00:05,  6.81it/s][A
 20%|██        | 9/44 [00:01<00:05,  6.03it/s][A
 23%|██▎       | 10/44 [00:01<00:05,  6.55it/s][A
 25%|██▌       | 11/44 [00:01<00:04,  6.73it/s][A
 27%|██▋       | 12/44 [00:01<00:04,  7.12it/s][A
 30%|██▉       | 13/44 [00:01<00:04,  7.21it/s][A
 32%|███▏      | 14/44 [00:01<00:03,  7.84it/s][A
 34%|███▍      | 15/44 [00:01<00:03,  8.35it/s][A
 36%|███▋      | 16/44 [00:02<00:03,  7.93it/s][A
 39%|███▊      | 17/44 [00:02<00:03,  7.69it/s][A
 41%|████      | 18/44 [00:02<00:03,  7.79it/s][A
 43%|████▎     | 19/44 [00:02<00:03,  7.65it/s][A
 45%|████▌     | 20/44 [00:02<00:03,  6.75it/s][A
 48%|████▊     | 21/44 [00:02<00:03,  6.01it/s][A
 50%|█████     | 22/44 [00:03<00:03,  6.51it/s][A
 52%|█████▏    | 23/44 [00:03<00:03,  6.67it/s][A
 55%|█████▍    | 24/44 [00:03<00:03,  5.67it/s][A
 57%|█████▋    | 25/44 [00:03<00:03,  5.38it/s][A
 59%|█████▉    | 26/44 [00:03<00:03,  5.96it/s][A
 61%|██████▏   | 27/44 [00:03<00:02,  6.27it/s][A
 64%|██████▎   | 28/44 [00:04<00:02,  6.46it/s][A
 66%|██████▌   | 29/44 [00:04<00:02,  6.81it/s][A
 68%|██████▊   | 30/44 [00:04<00:01,  7.16it/s][A
 73%|███████▎  | 32/44 [00:04<00:01,  7.76it/s][A
 75%|███████▌  | 33/44 [00:04<00:01,  7.52it/s][A
 77%|███████▋  | 34/44 [00:04<00:01,  7.42it/s][A
 80%|███████▉  | 35/44 [00:04<00:01,  7.53it/s][A
 82%|████████▏ | 36/44 [00:05<00:01,  7.41it/s][A
 84%|████████▍ | 37/44 [00:05<00:00,  7.41it/s][A
 86%|████████▋ | 38/44 [00:05<00:00,  6.61it/s][A
 89%|████████▊ | 39/44 [00:05<00:00,  7.00it/s][A
 91%|█████████ | 40/44 [00:05<00:00,  5.69it/s][A
 93%|█████████▎| 41/44 [00:05<00:00,  6.07it/s][A
 95%|█████████▌| 42/44 [00:06<00:00,  6.49it/s][A
 98%|█████████▊| 43/44 [00:06<00:00,  6.64it/s][A
100%|██████████| 44/44 [00:06<00:00,  6.16it/s][A                                                
                                               [A{'eval_loss': 0.6890702247619629, 'eval_accuracy': 0.36416184971098264, 'eval_runtime': 6.5082, 'eval_samples_per_second': 26.582, 'eval_steps_per_second': 6.761, 'epoch': 0.2}
 20%|█▉        | 20/102 [01:14<04:26,  3.25s/it]
100%|██████████| 44/44 [00:06<00:00,  6.16it/s][A
                                               [A 21%|██        | 21/102 [01:17<06:58,  5.17s/it] 22%|██▏       | 22/102 [01:20<06:01,  4.52s/it] 23%|██▎       | 23/102 [01:23<05:19,  4.04s/it] 24%|██▎       | 24/102 [01:26<04:49,  3.71s/it] 25%|██▍       | 25/102 [01:29<04:28,  3.49s/it] 25%|██▌       | 26/102 [01:33<04:29,  3.54s/it] 26%|██▋       | 27/102 [01:36<04:16,  3.42s/it] 27%|██▋       | 28/102 [01:39<04:04,  3.30s/it] 28%|██▊       | 29/102 [01:41<03:47,  3.12s/it] 29%|██▉       | 30/102 [01:44<03:43,  3.10s/it]                                                {'loss': 0.6615, 'grad_norm': 4.976971626281738, 'learning_rate': 4.05e-05, 'epoch': 0.29}
 29%|██▉       | 30/102 [01:44<03:43,  3.10s/it][INFO|trainer.py:3512] 2024-05-26 12:42:59,888 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-26 12:42:59,889 >>   Num examples = 173
[INFO|trainer.py:3517] 2024-05-26 12:42:59,889 >>   Batch size = 1

  0%|          | 0/44 [00:00<?, ?it/s][A
  5%|▍         | 2/44 [00:00<00:04, 10.49it/s][A
  9%|▉         | 4/44 [00:00<00:04,  9.38it/s][A
 11%|█▏        | 5/44 [00:00<00:04,  8.57it/s][A
 14%|█▎        | 6/44 [00:00<00:04,  8.01it/s][A
 16%|█▌        | 7/44 [00:00<00:04,  7.80it/s][A
 18%|█▊        | 8/44 [00:01<00:05,  6.84it/s][A
 20%|██        | 9/44 [00:01<00:05,  6.05it/s][A
 23%|██▎       | 10/44 [00:01<00:05,  6.56it/s][A
 25%|██▌       | 11/44 [00:01<00:04,  6.73it/s][A
 27%|██▋       | 12/44 [00:01<00:04,  7.13it/s][A
 30%|██▉       | 13/44 [00:01<00:04,  7.21it/s][A
 32%|███▏      | 14/44 [00:01<00:03,  7.84it/s][A
 34%|███▍      | 15/44 [00:01<00:03,  8.37it/s][A
 36%|███▋      | 16/44 [00:02<00:03,  7.93it/s][A
 39%|███▊      | 17/44 [00:02<00:03,  7.69it/s][A
 41%|████      | 18/44 [00:02<00:03,  7.80it/s][A
 43%|████▎     | 19/44 [00:02<00:03,  7.64it/s][A
 45%|████▌     | 20/44 [00:02<00:03,  6.77it/s][A
 48%|████▊     | 21/44 [00:02<00:03,  6.03it/s][A
 50%|█████     | 22/44 [00:03<00:03,  6.52it/s][A
 52%|█████▏    | 23/44 [00:03<00:03,  6.67it/s][A
 55%|█████▍    | 24/44 [00:03<00:03,  5.67it/s][A
 57%|█████▋    | 25/44 [00:03<00:03,  5.37it/s][A
 59%|█████▉    | 26/44 [00:03<00:03,  5.94it/s][A
 61%|██████▏   | 27/44 [00:03<00:02,  6.25it/s][A
 64%|██████▎   | 28/44 [00:04<00:02,  6.44it/s][A
 66%|██████▌   | 29/44 [00:04<00:02,  6.80it/s][A
 68%|██████▊   | 30/44 [00:04<00:01,  7.15it/s][A
 73%|███████▎  | 32/44 [00:04<00:01,  7.75it/s][A
 75%|███████▌  | 33/44 [00:04<00:01,  7.52it/s][A
 77%|███████▋  | 34/44 [00:04<00:01,  7.42it/s][A
 80%|███████▉  | 35/44 [00:04<00:01,  7.52it/s][A
 82%|████████▏ | 36/44 [00:05<00:01,  7.40it/s][A
 84%|████████▍ | 37/44 [00:05<00:00,  7.39it/s][A
 86%|████████▋ | 38/44 [00:05<00:00,  6.62it/s][A
 89%|████████▊ | 39/44 [00:05<00:00,  7.02it/s][A
 91%|█████████ | 40/44 [00:05<00:00,  5.68it/s][A
 93%|█████████▎| 41/44 [00:05<00:00,  6.06it/s][A
 95%|█████████▌| 42/44 [00:06<00:00,  6.48it/s][A
 98%|█████████▊| 43/44 [00:06<00:00,  6.63it/s][A
100%|██████████| 44/44 [00:06<00:00,  6.12it/s][A                                                
                                               [A{'eval_loss': 0.6420798301696777, 'eval_accuracy': 0.3872832369942196, 'eval_runtime': 6.51, 'eval_samples_per_second': 26.575, 'eval_steps_per_second': 6.759, 'epoch': 0.29}
 29%|██▉       | 30/102 [01:51<03:43,  3.10s/it]
100%|██████████| 44/44 [00:06<00:00,  6.12it/s][A
                                               [A 30%|███       | 31/102 [01:54<05:59,  5.07s/it] 31%|███▏      | 32/102 [01:57<05:10,  4.43s/it] 32%|███▏      | 33/102 [02:00<04:44,  4.13s/it] 33%|███▎      | 34/102 [02:04<04:19,  3.81s/it] 34%|███▍      | 35/102 [02:07<04:01,  3.60s/it] 35%|███▌      | 36/102 [02:10<03:47,  3.45s/it] 36%|███▋      | 37/102 [02:13<03:42,  3.42s/it] 37%|███▋      | 38/102 [02:16<03:33,  3.33s/it] 38%|███▊      | 39/102 [02:19<03:26,  3.28s/it] 39%|███▉      | 40/102 [02:22<03:15,  3.16s/it]                                                {'loss': 0.6231, 'grad_norm': 4.520554542541504, 'learning_rate': 5.5499999999999994e-05, 'epoch': 0.39}
 39%|███▉      | 40/102 [02:22<03:15,  3.16s/it][INFO|trainer.py:3512] 2024-05-26 12:43:37,696 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-26 12:43:37,696 >>   Num examples = 173
[INFO|trainer.py:3517] 2024-05-26 12:43:37,697 >>   Batch size = 1

  0%|          | 0/44 [00:00<?, ?it/s][A
  5%|▍         | 2/44 [00:00<00:04, 10.44it/s][A
  9%|▉         | 4/44 [00:00<00:04,  9.35it/s][A
 11%|█▏        | 5/44 [00:00<00:04,  8.55it/s][A
 14%|█▎        | 6/44 [00:00<00:04,  7.98it/s][A
 16%|█▌        | 7/44 [00:00<00:04,  7.79it/s][A
 18%|█▊        | 8/44 [00:01<00:05,  6.80it/s][A
 20%|██        | 9/44 [00:01<00:05,  6.01it/s][A
 23%|██▎       | 10/44 [00:01<00:05,  6.52it/s][A
 25%|██▌       | 11/44 [00:01<00:04,  6.69it/s][A
 27%|██▋       | 12/44 [00:01<00:04,  7.08it/s][A
 30%|██▉       | 13/44 [00:01<00:04,  7.16it/s][A
 32%|███▏      | 14/44 [00:01<00:03,  7.79it/s][A
 34%|███▍      | 15/44 [00:01<00:03,  8.31it/s][A
 36%|███▋      | 16/44 [00:02<00:03,  7.89it/s][A
 39%|███▊      | 17/44 [00:02<00:03,  7.66it/s][A
 41%|████      | 18/44 [00:02<00:03,  7.78it/s][A
 43%|████▎     | 19/44 [00:02<00:03,  7.62it/s][A
 45%|████▌     | 20/44 [00:02<00:03,  6.75it/s][A
 48%|████▊     | 21/44 [00:02<00:03,  6.02it/s][A
 50%|█████     | 22/44 [00:03<00:03,  6.48it/s][A
 52%|█████▏    | 23/44 [00:03<00:03,  6.64it/s][A
 55%|█████▍    | 24/44 [00:03<00:03,  5.65it/s][A
 57%|█████▋    | 25/44 [00:03<00:03,  5.35it/s][A
 59%|█████▉    | 26/44 [00:03<00:03,  5.93it/s][A
 61%|██████▏   | 27/44 [00:03<00:02,  6.24it/s][A
 64%|██████▎   | 28/44 [00:04<00:02,  6.43it/s][A
 66%|██████▌   | 29/44 [00:04<00:02,  6.78it/s][A
 68%|██████▊   | 30/44 [00:04<00:01,  7.12it/s][A
 73%|███████▎  | 32/44 [00:04<00:01,  7.73it/s][A
 75%|███████▌  | 33/44 [00:04<00:01,  7.49it/s][A
 77%|███████▋  | 34/44 [00:04<00:01,  7.39it/s][A
 80%|███████▉  | 35/44 [00:04<00:01,  7.49it/s][A
 82%|████████▏ | 36/44 [00:05<00:01,  7.38it/s][A
 84%|████████▍ | 37/44 [00:05<00:00,  7.37it/s][A
 86%|████████▋ | 38/44 [00:05<00:00,  6.59it/s][A
 89%|████████▊ | 39/44 [00:05<00:00,  6.98it/s][A
 91%|█████████ | 40/44 [00:05<00:00,  5.65it/s][A
 93%|█████████▎| 41/44 [00:05<00:00,  6.03it/s][A
 95%|█████████▌| 42/44 [00:06<00:00,  6.44it/s][A
 98%|█████████▊| 43/44 [00:06<00:00,  6.59it/s][A
100%|██████████| 44/44 [00:06<00:00,  6.10it/s][A                                                
                                               [A{'eval_loss': 0.5552781820297241, 'eval_accuracy': 0.3583815028901734, 'eval_runtime': 6.5382, 'eval_samples_per_second': 26.46, 'eval_steps_per_second': 6.73, 'epoch': 0.39}
 39%|███▉      | 40/102 [02:29<03:15,  3.16s/it]
100%|██████████| 44/44 [00:06<00:00,  6.10it/s][A
                                               [A 40%|████      | 41/102 [02:32<05:14,  5.15s/it] 41%|████      | 42/102 [02:35<04:30,  4.50s/it] 42%|████▏     | 43/102 [02:38<03:58,  4.04s/it] 43%|████▎     | 44/102 [02:41<03:43,  3.85s/it] 44%|████▍     | 45/102 [02:44<03:23,  3.56s/it] 45%|████▌     | 46/102 [02:47<03:12,  3.44s/it] 46%|████▌     | 47/102 [02:50<02:59,  3.27s/it] 47%|████▋     | 48/102 [02:53<02:50,  3.16s/it] 48%|████▊     | 49/102 [02:56<02:44,  3.10s/it] 49%|████▉     | 50/102 [02:59<02:38,  3.05s/it]                                                {'loss': 0.4861, 'grad_norm': 3.4081907272338867, 'learning_rate': 7.049999999999999e-05, 'epoch': 0.49}
 49%|████▉     | 50/102 [02:59<02:38,  3.05s/it][INFO|trainer.py:3512] 2024-05-26 12:44:14,553 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-26 12:44:14,553 >>   Num examples = 173
[INFO|trainer.py:3517] 2024-05-26 12:44:14,553 >>   Batch size = 1

  0%|          | 0/44 [00:00<?, ?it/s][A
  5%|▍         | 2/44 [00:00<00:04, 10.33it/s][A
  9%|▉         | 4/44 [00:00<00:04,  9.31it/s][A
 11%|█▏        | 5/44 [00:00<00:04,  8.50it/s][A
 14%|█▎        | 6/44 [00:00<00:04,  7.94it/s][A
 16%|█▌        | 7/44 [00:00<00:04,  7.75it/s][A
 18%|█▊        | 8/44 [00:01<00:05,  6.78it/s][A
 20%|██        | 9/44 [00:01<00:05,  6.02it/s][A
 23%|██▎       | 10/44 [00:01<00:05,  6.54it/s][A
 25%|██▌       | 11/44 [00:01<00:04,  6.74it/s][A
 27%|██▋       | 12/44 [00:01<00:04,  7.11it/s][A
 30%|██▉       | 13/44 [00:01<00:04,  7.19it/s][A
 32%|███▏      | 14/44 [00:01<00:03,  7.82it/s][A
 34%|███▍      | 15/44 [00:01<00:03,  8.34it/s][A
 36%|███▋      | 16/44 [00:02<00:03,  7.90it/s][A
 39%|███▊      | 17/44 [00:02<00:03,  7.66it/s][A
 41%|████      | 18/44 [00:02<00:03,  7.77it/s][A
 43%|████▎     | 19/44 [00:02<00:03,  7.62it/s][A
 45%|████▌     | 20/44 [00:02<00:03,  6.73it/s][A
 48%|████▊     | 21/44 [00:02<00:03,  6.00it/s][A
 50%|█████     | 22/44 [00:03<00:03,  6.50it/s][A
 52%|█████▏    | 23/44 [00:03<00:03,  6.65it/s][A
 55%|█████▍    | 24/44 [00:03<00:03,  5.65it/s][A
 57%|█████▋    | 25/44 [00:03<00:03,  5.36it/s][A
 59%|█████▉    | 26/44 [00:03<00:03,  5.93it/s][A
 61%|██████▏   | 27/44 [00:03<00:02,  6.24it/s][A
 64%|██████▎   | 28/44 [00:04<00:02,  6.43it/s][A
 66%|██████▌   | 29/44 [00:04<00:02,  6.78it/s][A
 68%|██████▊   | 30/44 [00:04<00:01,  7.13it/s][A
 73%|███████▎  | 32/44 [00:04<00:01,  7.73it/s][A
 75%|███████▌  | 33/44 [00:04<00:01,  7.50it/s][A
 77%|███████▋  | 34/44 [00:04<00:01,  7.40it/s][A
 80%|███████▉  | 35/44 [00:04<00:01,  7.51it/s][A
 82%|████████▏ | 36/44 [00:05<00:01,  7.39it/s][A
 84%|████████▍ | 37/44 [00:05<00:00,  7.37it/s][A
 86%|████████▋ | 38/44 [00:05<00:00,  6.59it/s][A
 89%|████████▊ | 39/44 [00:05<00:00,  6.99it/s][A
 91%|█████████ | 40/44 [00:05<00:00,  5.67it/s][A
 93%|█████████▎| 41/44 [00:05<00:00,  6.04it/s][A
 95%|█████████▌| 42/44 [00:06<00:00,  6.46it/s][A
 98%|█████████▊| 43/44 [00:06<00:00,  6.61it/s][A
100%|██████████| 44/44 [00:06<00:00,  6.12it/s][A                                                
                                               [A{'eval_loss': 0.36227819323539734, 'eval_accuracy': 0.4046242774566474, 'eval_runtime': 6.5327, 'eval_samples_per_second': 26.482, 'eval_steps_per_second': 6.735, 'epoch': 0.49}
 49%|████▉     | 50/102 [03:06<02:38,  3.05s/it]
100%|██████████| 44/44 [00:06<00:00,  6.12it/s][A
                                               [A 50%|█████     | 51/102 [03:09<04:13,  4.96s/it] 51%|█████     | 52/102 [03:12<03:43,  4.46s/it] 52%|█████▏    | 53/102 [03:15<03:18,  4.05s/it] 53%|█████▎    | 54/102 [03:18<02:57,  3.69s/it] 54%|█████▍    | 55/102 [03:21<02:47,  3.55s/it] 55%|█████▍    | 56/102 [03:24<02:38,  3.44s/it] 56%|█████▌    | 57/102 [03:27<02:29,  3.31s/it] 57%|█████▋    | 58/102 [03:30<02:22,  3.23s/it] 58%|█████▊    | 59/102 [03:33<02:13,  3.10s/it] 59%|█████▉    | 60/102 [03:36<02:09,  3.08s/it]                                                {'loss': 0.2601, 'grad_norm': 2.42166805267334, 'learning_rate': 8.549999999999999e-05, 'epoch': 0.59}
 59%|█████▉    | 60/102 [03:36<02:09,  3.08s/it][INFO|trainer.py:3512] 2024-05-26 12:44:51,515 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-26 12:44:51,515 >>   Num examples = 173
[INFO|trainer.py:3517] 2024-05-26 12:44:51,515 >>   Batch size = 1

  0%|          | 0/44 [00:00<?, ?it/s][A
  5%|▍         | 2/44 [00:00<00:04, 10.39it/s][A
  9%|▉         | 4/44 [00:00<00:04,  9.33it/s][A
 11%|█▏        | 5/44 [00:00<00:04,  8.51it/s][A
 14%|█▎        | 6/44 [00:00<00:04,  7.95it/s][A
 16%|█▌        | 7/44 [00:00<00:04,  7.76it/s][A
 18%|█▊        | 8/44 [00:01<00:05,  6.76it/s][A
 20%|██        | 9/44 [00:01<00:05,  5.98it/s][A
 23%|██▎       | 10/44 [00:01<00:05,  6.50it/s][A
 25%|██▌       | 11/44 [00:01<00:04,  6.68it/s][A
 27%|██▋       | 12/44 [00:01<00:04,  7.07it/s][A
 30%|██▉       | 13/44 [00:01<00:04,  7.16it/s][A
 32%|███▏      | 14/44 [00:01<00:03,  7.80it/s][A
 34%|███▍      | 15/44 [00:01<00:03,  8.33it/s][A
 36%|███▋      | 16/44 [00:02<00:03,  7.89it/s][A
 39%|███▊      | 17/44 [00:02<00:03,  7.67it/s][A
 41%|████      | 18/44 [00:02<00:03,  7.78it/s][A
 43%|████▎     | 19/44 [00:02<00:03,  7.63it/s][A
 45%|████▌     | 20/44 [00:02<00:03,  6.72it/s][A
 48%|████▊     | 21/44 [00:02<00:03,  5.98it/s][A
 50%|█████     | 22/44 [00:03<00:03,  6.45it/s][A
 52%|█████▏    | 23/44 [00:03<00:03,  6.61it/s][A
 55%|█████▍    | 24/44 [00:03<00:03,  5.63it/s][A
 57%|█████▋    | 25/44 [00:03<00:03,  5.34it/s][A
 59%|█████▉    | 26/44 [00:03<00:03,  5.92it/s][A
 61%|██████▏   | 27/44 [00:03<00:02,  6.23it/s][A
 64%|██████▎   | 28/44 [00:04<00:02,  6.41it/s][A
 66%|██████▌   | 29/44 [00:04<00:02,  6.76it/s][A
 68%|██████▊   | 30/44 [00:04<00:01,  7.12it/s][A
 73%|███████▎  | 32/44 [00:04<00:01,  7.71it/s][A
 75%|███████▌  | 33/44 [00:04<00:01,  7.48it/s][A
 77%|███████▋  | 34/44 [00:04<00:01,  7.38it/s][A
 80%|███████▉  | 35/44 [00:04<00:01,  7.49it/s][A
 82%|████████▏ | 36/44 [00:05<00:01,  7.36it/s][A
 84%|████████▍ | 37/44 [00:05<00:00,  7.35it/s][A
 86%|████████▋ | 38/44 [00:05<00:00,  6.59it/s][A
 89%|████████▊ | 39/44 [00:05<00:00,  6.99it/s][A
 91%|█████████ | 40/44 [00:05<00:00,  5.66it/s][A
 93%|█████████▎| 41/44 [00:05<00:00,  6.04it/s][A
 95%|█████████▌| 42/44 [00:06<00:00,  6.46it/s][A
 98%|█████████▊| 43/44 [00:06<00:00,  6.61it/s][A
100%|██████████| 44/44 [00:06<00:00,  6.10it/s][A                                                
                                               [A{'eval_loss': 0.15155667066574097, 'eval_accuracy': 0.5838150289017341, 'eval_runtime': 6.548, 'eval_samples_per_second': 26.42, 'eval_steps_per_second': 6.72, 'epoch': 0.59}
 59%|█████▉    | 60/102 [03:43<02:09,  3.08s/it]
100%|██████████| 44/44 [00:06<00:00,  6.10it/s][A
                                               [A 60%|█████▉    | 61/102 [03:46<03:24,  4.98s/it] 61%|██████    | 62/102 [03:49<02:56,  4.42s/it] 62%|██████▏   | 63/102 [03:52<02:41,  4.13s/it] 63%|██████▎   | 64/102 [03:56<02:35,  4.08s/it] 64%|██████▎   | 65/102 [03:59<02:21,  3.82s/it] 65%|██████▍   | 66/102 [04:02<02:08,  3.56s/it] 66%|██████▌   | 67/102 [04:05<01:58,  3.37s/it] 67%|██████▋   | 68/102 [04:08<01:50,  3.25s/it] 68%|██████▊   | 69/102 [04:11<01:46,  3.21s/it] 69%|██████▊   | 70/102 [04:14<01:42,  3.22s/it]                                                {'loss': 0.1209, 'grad_norm': 1.2569997310638428, 'learning_rate': 0.0001005, 'epoch': 0.68}
 69%|██████▊   | 70/102 [04:14<01:42,  3.22s/it][INFO|trainer.py:3512] 2024-05-26 12:45:29,885 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-26 12:45:29,885 >>   Num examples = 173
[INFO|trainer.py:3517] 2024-05-26 12:45:29,885 >>   Batch size = 1

  0%|          | 0/44 [00:00<?, ?it/s][A
  5%|▍         | 2/44 [00:00<00:04, 10.44it/s][A
  9%|▉         | 4/44 [00:00<00:04,  9.34it/s][A
 11%|█▏        | 5/44 [00:00<00:04,  8.52it/s][A
 14%|█▎        | 6/44 [00:00<00:04,  7.97it/s][A
 16%|█▌        | 7/44 [00:00<00:04,  7.78it/s][A
 18%|█▊        | 8/44 [00:01<00:05,  6.80it/s][A
 20%|██        | 9/44 [00:01<00:05,  6.02it/s][A
 23%|██▎       | 10/44 [00:01<00:05,  6.53it/s][A
 25%|██▌       | 11/44 [00:01<00:04,  6.70it/s][A
 27%|██▋       | 12/44 [00:01<00:04,  7.10it/s][A
 30%|██▉       | 13/44 [00:01<00:04,  7.18it/s][A
 32%|███▏      | 14/44 [00:01<00:03,  7.83it/s][A
 34%|███▍      | 15/44 [00:01<00:03,  8.34it/s][A
 36%|███▋      | 16/44 [00:02<00:03,  7.89it/s][A
 39%|███▊      | 17/44 [00:02<00:03,  7.64it/s][A
 41%|████      | 18/44 [00:02<00:03,  7.76it/s][A
 43%|████▎     | 19/44 [00:02<00:03,  7.61it/s][A
 45%|████▌     | 20/44 [00:02<00:03,  6.75it/s][A
 48%|████▊     | 21/44 [00:02<00:03,  6.01it/s][A
 50%|█████     | 22/44 [00:03<00:03,  6.49it/s][A
 52%|█████▏    | 23/44 [00:03<00:03,  6.64it/s][A
 55%|█████▍    | 24/44 [00:03<00:03,  5.64it/s][A
 57%|█████▋    | 25/44 [00:03<00:03,  5.34it/s][A
 59%|█████▉    | 26/44 [00:03<00:03,  5.92it/s][A
 61%|██████▏   | 27/44 [00:03<00:02,  6.23it/s][A
 64%|██████▎   | 28/44 [00:04<00:02,  6.42it/s][A
 66%|██████▌   | 29/44 [00:04<00:02,  6.78it/s][A
 68%|██████▊   | 30/44 [00:04<00:01,  7.13it/s][A
 73%|███████▎  | 32/44 [00:04<00:01,  7.72it/s][A
 75%|███████▌  | 33/44 [00:04<00:01,  7.49it/s][A
 77%|███████▋  | 34/44 [00:04<00:01,  7.37it/s][A
 80%|███████▉  | 35/44 [00:04<00:01,  7.48it/s][A
 82%|████████▏ | 36/44 [00:05<00:01,  7.35it/s][A
 84%|████████▍ | 37/44 [00:05<00:00,  7.35it/s][A
 86%|████████▋ | 38/44 [00:05<00:00,  6.59it/s][A
 89%|████████▊ | 39/44 [00:05<00:00,  6.98it/s][A
 91%|█████████ | 40/44 [00:05<00:00,  5.65it/s][A
 93%|█████████▎| 41/44 [00:05<00:00,  6.03it/s][A
 95%|█████████▌| 42/44 [00:06<00:00,  6.45it/s][A
 98%|█████████▊| 43/44 [00:06<00:00,  6.60it/s][A
100%|██████████| 44/44 [00:06<00:00,  6.10it/s][A                                                
                                               [A{'eval_loss': 0.15572495758533478, 'eval_accuracy': 0.7052023121387283, 'eval_runtime': 6.54, 'eval_samples_per_second': 26.453, 'eval_steps_per_second': 6.728, 'epoch': 0.68}
 69%|██████▊   | 70/102 [04:21<01:42,  3.22s/it]
100%|██████████| 44/44 [00:06<00:00,  6.10it/s][A
                                               [A 70%|██████▉   | 71/102 [04:24<02:38,  5.13s/it] 71%|███████   | 72/102 [04:27<02:15,  4.53s/it] 72%|███████▏  | 73/102 [04:30<01:58,  4.09s/it] 73%|███████▎  | 74/102 [04:33<01:47,  3.83s/it] 74%|███████▎  | 75/102 [04:36<01:36,  3.57s/it] 75%|███████▍  | 76/102 [04:40<01:30,  3.47s/it] 75%|███████▌  | 77/102 [04:43<01:23,  3.33s/it] 76%|███████▋  | 78/102 [04:47<01:24,  3.54s/it] 77%|███████▋  | 79/102 [04:50<01:18,  3.40s/it] 78%|███████▊  | 80/102 [04:53<01:11,  3.26s/it]                                                {'loss': 0.0913, 'grad_norm': 1.137752890586853, 'learning_rate': 0.00011549999999999999, 'epoch': 0.78}
 78%|███████▊  | 80/102 [04:53<01:11,  3.26s/it][INFO|trainer.py:3512] 2024-05-26 12:46:08,117 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-26 12:46:08,117 >>   Num examples = 173
[INFO|trainer.py:3517] 2024-05-26 12:46:08,117 >>   Batch size = 1

  0%|          | 0/44 [00:00<?, ?it/s][A
  5%|▍         | 2/44 [00:00<00:04, 10.38it/s][A
  9%|▉         | 4/44 [00:00<00:04,  9.32it/s][A
 11%|█▏        | 5/44 [00:00<00:04,  8.50it/s][A
 14%|█▎        | 6/44 [00:00<00:04,  7.96it/s][A
 16%|█▌        | 7/44 [00:00<00:04,  7.76it/s][A
 18%|█▊        | 8/44 [00:01<00:05,  6.75it/s][A
 20%|██        | 9/44 [00:01<00:05,  5.99it/s][A
 23%|██▎       | 10/44 [00:01<00:05,  6.52it/s][A
 25%|██▌       | 11/44 [00:01<00:04,  6.69it/s][A
 27%|██▋       | 12/44 [00:01<00:04,  7.08it/s][A
 30%|██▉       | 13/44 [00:01<00:04,  7.16it/s][A
 32%|███▏      | 14/44 [00:01<00:03,  7.80it/s][A
 34%|███▍      | 15/44 [00:01<00:03,  8.32it/s][A
 36%|███▋      | 16/44 [00:02<00:03,  7.88it/s][A
 39%|███▊      | 17/44 [00:02<00:03,  7.65it/s][A
 41%|████      | 18/44 [00:02<00:03,  7.77it/s][A
 43%|████▎     | 19/44 [00:02<00:03,  7.62it/s][A
 45%|████▌     | 20/44 [00:02<00:03,  6.73it/s][A
 48%|████▊     | 21/44 [00:02<00:03,  5.99it/s][A
 50%|█████     | 22/44 [00:03<00:03,  6.48it/s][A
 52%|█████▏    | 23/44 [00:03<00:03,  6.63it/s][A
 55%|█████▍    | 24/44 [00:03<00:03,  5.63it/s][A
 57%|█████▋    | 25/44 [00:03<00:03,  5.34it/s][A
 59%|█████▉    | 26/44 [00:03<00:03,  5.91it/s][A
 61%|██████▏   | 27/44 [00:03<00:02,  6.24it/s][A
 64%|██████▎   | 28/44 [00:04<00:02,  6.43it/s][A
 66%|██████▌   | 29/44 [00:04<00:02,  6.77it/s][A
 68%|██████▊   | 30/44 [00:04<00:01,  7.11it/s][A
 73%|███████▎  | 32/44 [00:04<00:01,  7.71it/s][A
 75%|███████▌  | 33/44 [00:04<00:01,  7.48it/s][A
 77%|███████▋  | 34/44 [00:04<00:01,  7.37it/s][A
 80%|███████▉  | 35/44 [00:04<00:01,  7.47it/s][A
 82%|████████▏ | 36/44 [00:05<00:01,  7.34it/s][A
 84%|████████▍ | 37/44 [00:05<00:00,  7.35it/s][A
 86%|████████▋ | 38/44 [00:05<00:00,  6.58it/s][A
 89%|████████▊ | 39/44 [00:05<00:00,  6.98it/s][A
 91%|█████████ | 40/44 [00:05<00:00,  5.65it/s][A
 93%|█████████▎| 41/44 [00:05<00:00,  6.04it/s][A
 95%|█████████▌| 42/44 [00:06<00:00,  6.46it/s][A
 98%|█████████▊| 43/44 [00:06<00:00,  6.61it/s][A
100%|██████████| 44/44 [00:06<00:00,  6.11it/s][A                                                
                                               [A{'eval_loss': 0.1269383281469345, 'eval_accuracy': 0.8554913294797688, 'eval_runtime': 6.5467, 'eval_samples_per_second': 26.426, 'eval_steps_per_second': 6.721, 'epoch': 0.78}
 78%|███████▊  | 80/102 [04:59<01:11,  3.26s/it]
100%|██████████| 44/44 [00:06<00:00,  6.11it/s][A
                                               [A 79%|███████▉  | 81/102 [05:02<01:47,  5.10s/it] 80%|████████  | 82/102 [05:06<01:32,  4.61s/it] 81%|████████▏ | 83/102 [05:09<01:18,  4.13s/it] 82%|████████▏ | 84/102 [05:12<01:08,  3.81s/it] 83%|████████▎ | 85/102 [05:15<01:00,  3.55s/it] 84%|████████▍ | 86/102 [05:18<00:53,  3.36s/it] 85%|████████▌ | 87/102 [05:20<00:48,  3.23s/it] 86%|████████▋ | 88/102 [05:23<00:43,  3.13s/it] 87%|████████▋ | 89/102 [05:27<00:40,  3.15s/it] 88%|████████▊ | 90/102 [05:30<00:37,  3.14s/it]                                                {'loss': 0.1784, 'grad_norm': 1.4039002656936646, 'learning_rate': 0.000129, 'epoch': 0.88}
 88%|████████▊ | 90/102 [05:30<00:37,  3.14s/it][INFO|trainer.py:3512] 2024-05-26 12:46:45,073 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-26 12:46:45,074 >>   Num examples = 173
[INFO|trainer.py:3517] 2024-05-26 12:46:45,074 >>   Batch size = 1

  0%|          | 0/44 [00:00<?, ?it/s][A
  5%|▍         | 2/44 [00:00<00:04, 10.31it/s][A
  9%|▉         | 4/44 [00:00<00:04,  9.31it/s][A
 11%|█▏        | 5/44 [00:00<00:04,  8.50it/s][A
 14%|█▎        | 6/44 [00:00<00:04,  7.96it/s][A
 16%|█▌        | 7/44 [00:00<00:04,  7.77it/s][A
 18%|█▊        | 8/44 [00:01<00:05,  6.80it/s][A
 20%|██        | 9/44 [00:01<00:05,  6.00it/s][A
 23%|██▎       | 10/44 [00:01<00:05,  6.52it/s][A
 25%|██▌       | 11/44 [00:01<00:04,  6.70it/s][A
 27%|██▋       | 12/44 [00:01<00:04,  7.09it/s][A
 30%|██▉       | 13/44 [00:01<00:04,  7.19it/s][A
 32%|███▏      | 14/44 [00:01<00:03,  7.84it/s][A
 34%|███▍      | 15/44 [00:01<00:03,  8.36it/s][A
 36%|███▋      | 16/44 [00:02<00:03,  7.91it/s][A
 39%|███▊      | 17/44 [00:02<00:03,  7.68it/s][A
 41%|████      | 18/44 [00:02<00:03,  7.79it/s][A
 43%|████▎     | 19/44 [00:02<00:03,  7.63it/s][A
 45%|████▌     | 20/44 [00:02<00:03,  6.73it/s][A
 48%|████▊     | 21/44 [00:02<00:03,  5.99it/s][A
 50%|█████     | 22/44 [00:03<00:03,  6.46it/s][A
 52%|█████▏    | 23/44 [00:03<00:03,  6.62it/s][A
 55%|█████▍    | 24/44 [00:03<00:03,  5.62it/s][A
 57%|█████▋    | 25/44 [00:03<00:03,  5.33it/s][A
 59%|█████▉    | 26/44 [00:03<00:03,  5.90it/s][A
 61%|██████▏   | 27/44 [00:03<00:02,  6.23it/s][A
 64%|██████▎   | 28/44 [00:04<00:02,  6.42it/s][A
 66%|██████▌   | 29/44 [00:04<00:02,  6.77it/s][A
 68%|██████▊   | 30/44 [00:04<00:01,  7.11it/s][A
 73%|███████▎  | 32/44 [00:04<00:01,  7.70it/s][A
 75%|███████▌  | 33/44 [00:04<00:01,  7.46it/s][A
 77%|███████▋  | 34/44 [00:04<00:01,  7.35it/s][A
 80%|███████▉  | 35/44 [00:04<00:01,  7.44it/s][A
 82%|████████▏ | 36/44 [00:05<00:01,  7.33it/s][A
 84%|████████▍ | 37/44 [00:05<00:00,  7.34it/s][A
 86%|████████▋ | 38/44 [00:05<00:00,  6.58it/s][A
 89%|████████▊ | 39/44 [00:05<00:00,  6.98it/s][A
 91%|█████████ | 40/44 [00:05<00:00,  5.65it/s][A
 93%|█████████▎| 41/44 [00:05<00:00,  6.02it/s][A
 95%|█████████▌| 42/44 [00:06<00:00,  6.43it/s][A
 98%|█████████▊| 43/44 [00:06<00:00,  6.58it/s][A
100%|██████████| 44/44 [00:06<00:00,  6.09it/s][A                                                
                                               [A{'eval_loss': 0.11930752545595169, 'eval_accuracy': 0.838150289017341, 'eval_runtime': 6.5506, 'eval_samples_per_second': 26.41, 'eval_steps_per_second': 6.717, 'epoch': 0.88}
 88%|████████▊ | 90/102 [05:36<00:37,  3.14s/it]
100%|██████████| 44/44 [00:06<00:00,  6.09it/s][A
                                               [A 89%|████████▉ | 91/102 [05:40<00:57,  5.19s/it] 90%|█████████ | 92/102 [05:43<00:45,  4.51s/it] 91%|█████████ | 93/102 [05:46<00:37,  4.11s/it] 92%|█████████▏| 94/102 [05:49<00:30,  3.83s/it] 93%|█████████▎| 95/102 [05:52<00:25,  3.58s/it] 94%|█████████▍| 96/102 [05:55<00:20,  3.42s/it] 95%|█████████▌| 97/102 [05:58<00:17,  3.45s/it] 96%|█████████▌| 98/102 [06:02<00:13,  3.34s/it] 97%|█████████▋| 99/102 [06:04<00:09,  3.20s/it] 98%|█████████▊| 100/102 [06:07<00:06,  3.12s/it]                                                 {'loss': 0.0987, 'grad_norm': 2.2374699115753174, 'learning_rate': 0.00014399999999999998, 'epoch': 0.98}
 98%|█████████▊| 100/102 [06:07<00:06,  3.12s/it][INFO|trainer.py:3512] 2024-05-26 12:47:22,780 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-26 12:47:22,780 >>   Num examples = 173
[INFO|trainer.py:3517] 2024-05-26 12:47:22,780 >>   Batch size = 1

  0%|          | 0/44 [00:00<?, ?it/s][A
  5%|▍         | 2/44 [00:00<00:04, 10.38it/s][A
  9%|▉         | 4/44 [00:00<00:04,  9.30it/s][A
 11%|█▏        | 5/44 [00:00<00:04,  8.49it/s][A
 14%|█▎        | 6/44 [00:00<00:04,  7.95it/s][A
 16%|█▌        | 7/44 [00:00<00:04,  7.75it/s][A
 18%|█▊        | 8/44 [00:01<00:05,  6.78it/s][A
 20%|██        | 9/44 [00:01<00:05,  6.02it/s][A
 23%|██▎       | 10/44 [00:01<00:05,  6.53it/s][A
 25%|██▌       | 11/44 [00:01<00:04,  6.71it/s][A
 27%|██▋       | 12/44 [00:01<00:04,  7.10it/s][A
 30%|██▉       | 13/44 [00:01<00:04,  7.18it/s][A
 32%|███▏      | 14/44 [00:01<00:03,  7.82it/s][A
 34%|███▍      | 15/44 [00:01<00:03,  8.32it/s][A
 36%|███▋      | 16/44 [00:02<00:03,  7.87it/s][A
 39%|███▊      | 17/44 [00:02<00:03,  7.63it/s][A
 41%|████      | 18/44 [00:02<00:03,  7.75it/s][A
 43%|████▎     | 19/44 [00:02<00:03,  7.60it/s][A
 45%|████▌     | 20/44 [00:02<00:03,  6.74it/s][A
 48%|████▊     | 21/44 [00:02<00:03,  6.00it/s][A
 50%|█████     | 22/44 [00:03<00:03,  6.50it/s][A
 52%|█████▏    | 23/44 [00:03<00:03,  6.65it/s][A
 55%|█████▍    | 24/44 [00:03<00:03,  5.64it/s][A
 57%|█████▋    | 25/44 [00:03<00:03,  5.35it/s][A
 59%|█████▉    | 26/44 [00:03<00:03,  5.93it/s][A
 61%|██████▏   | 27/44 [00:03<00:02,  6.25it/s][A
 64%|██████▎   | 28/44 [00:04<00:02,  6.44it/s][A
 66%|██████▌   | 29/44 [00:04<00:02,  6.79it/s][A
 68%|██████▊   | 30/44 [00:04<00:01,  7.13it/s][A
 73%|███████▎  | 32/44 [00:04<00:01,  7.72it/s][A
 75%|███████▌  | 33/44 [00:04<00:01,  7.48it/s][A
 77%|███████▋  | 34/44 [00:04<00:01,  7.37it/s][A
 80%|███████▉  | 35/44 [00:04<00:01,  7.49it/s][A
 82%|████████▏ | 36/44 [00:05<00:01,  7.35it/s][A
 84%|████████▍ | 37/44 [00:05<00:00,  7.34it/s][A
 86%|████████▋ | 38/44 [00:05<00:00,  6.58it/s][A
 89%|████████▊ | 39/44 [00:05<00:00,  6.98it/s][A
 91%|█████████ | 40/44 [00:05<00:00,  5.65it/s][A
 93%|█████████▎| 41/44 [00:05<00:00,  6.03it/s][A
 95%|█████████▌| 42/44 [00:06<00:00,  6.46it/s][A
 98%|█████████▊| 43/44 [00:06<00:00,  6.61it/s][A
100%|██████████| 44/44 [00:06<00:00,  6.11it/s][A                                                 
                                               [A{'eval_loss': 0.10365083813667297, 'eval_accuracy': 0.8439306358381503, 'eval_runtime': 6.5413, 'eval_samples_per_second': 26.447, 'eval_steps_per_second': 6.726, 'epoch': 0.98}
 98%|█████████▊| 100/102 [06:14<00:06,  3.12s/it]
100%|██████████| 44/44 [00:06<00:00,  6.11it/s][A
                                               [A 99%|█████████▉| 101/102 [06:17<00:05,  5.06s/it]100%|██████████| 102/102 [06:20<00:00,  4.39s/it][INFO|trainer.py:2231] 2024-05-26 12:47:35,170 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 387.7355, 'train_samples_per_second': 8.454, 'train_steps_per_second': 0.263, 'train_loss': 0.3878337653244243, 'epoch': 1.0}
100%|██████████| 102/102 [06:20<00:00,  4.39s/it]100%|██████████| 102/102 [06:20<00:00,  3.73s/it]
[INFO|trainer.py:3203] 2024-05-26 12:47:35,172 >> Saving model checkpoint to rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200
[INFO|trainer.py:3217] 2024-05-26 12:47:35,172 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2502] 2024-05-26 12:47:35,173 >> tokenizer config file saved in rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-26 12:47:35,174 >> Special tokens file saved in rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200/special_tokens_map.json
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/student2021/srcao/base-model/llama2-7b - will assume that the vocabulary was not modified.
  warnings.warn(
05/26/2024 12:47:35 - INFO - llmtuner.extras.misc - Value head model saved at: rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     0.3878
  train_runtime            = 0:06:27.73
  train_samples_per_second =      8.454
  train_steps_per_second   =      0.263
Figure saved: rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200/training_loss.png
Figure saved: rm_checkpoint/llama2-7b-multi_role-lr=3e-4-warmup=200/training_eval_loss.png
[INFO|trainer.py:3512] 2024-05-26 12:47:35,437 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-26 12:47:35,437 >>   Num examples = 173
[INFO|trainer.py:3517] 2024-05-26 12:47:35,437 >>   Batch size = 1
  0%|          | 0/44 [00:00<?, ?it/s]  5%|▍         | 2/44 [00:00<00:04, 10.44it/s]  9%|▉         | 4/44 [00:00<00:04,  9.35it/s] 11%|█▏        | 5/44 [00:00<00:04,  8.52it/s] 14%|█▎        | 6/44 [00:00<00:04,  7.97it/s] 16%|█▌        | 7/44 [00:00<00:04,  7.78it/s] 18%|█▊        | 8/44 [00:01<00:05,  6.79it/s] 20%|██        | 9/44 [00:01<00:05,  6.01it/s] 23%|██▎       | 10/44 [00:01<00:05,  6.53it/s] 25%|██▌       | 11/44 [00:01<00:04,  6.71it/s] 27%|██▋       | 12/44 [00:01<00:04,  7.10it/s] 30%|██▉       | 13/44 [00:01<00:04,  7.19it/s] 32%|███▏      | 14/44 [00:01<00:03,  7.84it/s] 34%|███▍      | 15/44 [00:01<00:03,  8.34it/s] 36%|███▋      | 16/44 [00:02<00:03,  7.89it/s] 39%|███▊      | 17/44 [00:02<00:03,  7.63it/s] 41%|████      | 18/44 [00:02<00:03,  7.75it/s] 43%|████▎     | 19/44 [00:02<00:03,  7.60it/s] 45%|████▌     | 20/44 [00:02<00:03,  6.75it/s] 48%|████▊     | 21/44 [00:02<00:03,  6.00it/s] 50%|█████     | 22/44 [00:03<00:03,  6.50it/s] 52%|█████▏    | 23/44 [00:03<00:03,  6.64it/s] 55%|█████▍    | 24/44 [00:03<00:03,  5.64it/s] 57%|█████▋    | 25/44 [00:03<00:03,  5.35it/s] 59%|█████▉    | 26/44 [00:03<00:03,  5.92it/s] 61%|██████▏   | 27/44 [00:03<00:02,  6.25it/s] 64%|██████▎   | 28/44 [00:04<00:02,  6.44it/s] 66%|██████▌   | 29/44 [00:04<00:02,  6.79it/s] 68%|██████▊   | 30/44 [00:04<00:01,  7.14it/s] 73%|███████▎  | 32/44 [00:04<00:01,  7.72it/s] 75%|███████▌  | 33/44 [00:04<00:01,  7.49it/s] 77%|███████▋  | 34/44 [00:04<00:01,  7.38it/s] 80%|███████▉  | 35/44 [00:04<00:01,  7.50it/s] 82%|████████▏ | 36/44 [00:05<00:01,  7.36it/s] 84%|████████▍ | 37/44 [00:05<00:00,  7.36it/s] 86%|████████▋ | 38/44 [00:05<00:00,  6.57it/s] 89%|████████▊ | 39/44 [00:05<00:00,  6.98it/s] 91%|█████████ | 40/44 [00:05<00:00,  5.65it/s] 93%|█████████▎| 41/44 [00:05<00:00,  6.03it/s] 95%|█████████▌| 42/44 [00:06<00:00,  6.45it/s] 98%|█████████▊| 43/44 [00:06<00:00,  6.60it/s]100%|██████████| 44/44 [00:06<00:00,  6.11it/s]100%|██████████| 44/44 [00:06<00:00,  6.88it/s]
***** eval metrics *****
  epoch                   =        1.0
  eval_accuracy           =     0.8497
  eval_loss               =     0.1126
  eval_runtime            = 0:00:06.48
  eval_samples_per_second =     26.675
  eval_steps_per_second   =      6.784
[INFO|modelcard.py:450] 2024-05-26 12:47:41,926 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8497109826589595}]}
[2024-05-26 12:47:43,790] [INFO] [launch.py:348:main] Process 19174 exits successfully.
[2024-05-26 12:47:43,790] [INFO] [launch.py:348:main] Process 19176 exits successfully.
[2024-05-26 12:47:43,791] [INFO] [launch.py:348:main] Process 19175 exits successfully.
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.028 MB of 0.028 MB uploadedwandb: | 0.028 MB of 0.028 MB uploadedwandb: 
wandb: Run history:
wandb:           eval/accuracy ▁▁▁▁▂▄▆████
wandb:               eval/loss ██▇▆▄▂▂▁▁▁▁
wandb:            eval/runtime ▁▅▅▇▇█▇██▇▃
wandb: eval/samples_per_second █▄▄▂▂▁▂▁▁▂▆
wandb:   eval/steps_per_second █▄▄▂▂▁▂▁▁▂▆
wandb:             train/epoch ▁▁▂▂▂▂▃▃▄▄▅▅▆▆▆▆▇▇████
wandb:       train/global_step ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇████
wandb:         train/grad_norm █▇▇▆▄▃▁▁▁▃
wandb:     train/learning_rate ▁▂▃▃▄▅▆▇▇█
wandb:              train/loss ███▇▆▃▁▁▂▁
wandb: 
wandb: Run summary:
wandb:            eval/accuracy 0.84971
wandb:                eval/loss 0.11257
wandb:             eval/runtime 6.4855
wandb:  eval/samples_per_second 26.675
wandb:    eval/steps_per_second 6.784
wandb:               total_flos 0.0
wandb:              train/epoch 1.0
wandb:        train/global_step 102
wandb:          train/grad_norm 2.23747
wandb:      train/learning_rate 0.00014
wandb:               train/loss 0.0987
wandb:               train_loss 0.38783
wandb:            train_runtime 387.7355
wandb: train_samples_per_second 8.454
wandb:   train_steps_per_second 0.263
wandb: 
wandb: 🚀 View run kind-blaze-192 at: https://wandb.ai/srcao-bingo/huggingface/runs/z6qz55sv/workspace
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240526_124108-z6qz55sv/logs
[2024-05-26 12:47:53,802] [INFO] [launch.py:348:main] Process 19173 exits successfully.
[2024-05-26 12:52:19,647] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:52:21,431] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-26 12:52:21,453] [INFO] [runner.py:568:main] cmd = /home/student2021/anaconda3/envs/llama_factory/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None src/train_bash.py --stage sft --do_train --model_name_or_path /home/student2021/srcao/base-model/llama2-7b --dataset multi_role_train --template default --finetuning_type lora --lora_alpha 16 --lora_dropout 0.05 --lora_rank 8 --lora_target q_proj,v_proj --output_dir sft_checkpoint/llama2-7b-multi_role_train --overwrite_cache --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --lr_scheduler_type linear --warmup_steps 200 --max_length 1024 --logging_steps 10 --save_steps 1000 --learning_rate 5e-5 --num_train_epochs 3.0 --plot_loss --fp16 --report_to=wandb
[2024-05-26 12:52:23,268] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:52:24,968] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-05-26 12:52:24,968] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-05-26 12:52:24,968] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-05-26 12:52:24,968] [INFO] [launch.py:163:main] dist_world_size=4
[2024-05-26 12:52:24,968] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-05-26 12:52:24,969] [INFO] [launch.py:253:main] process 23648 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=0', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:52:24,969] [INFO] [launch.py:253:main] process 23649 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=1', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:52:24,970] [INFO] [launch.py:253:main] process 23650 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=2', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:52:24,971] [INFO] [launch.py:253:main] process 23651 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=3', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:52:29,023] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:52:29,026] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:52:29,031] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:52:29,046] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
05/26/2024 12:52:34 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1934] 2024-05-26 12:52:34,780 >> PyTorch: setting up devices
05/26/2024 12:52:34 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/26/2024 12:52:34 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/26/2024 12:52:34 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:52:34 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 12:52:34 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train/runs/May26_12-52-33_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:52:34 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 12:52:34 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train/runs/May26_12-52-33_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:52:34,834 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:52:34,834 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:52:34,834 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:52:34,834 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:52:34,834 >> loading file tokenizer.json
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:52:34 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 12:52:34 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train/runs/May26_12-52-33_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:52:34 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 12:52:34 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train/runs/May26_12-52-33_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
[INFO|configuration_utils.py:724] 2024-05-26 12:52:34,943 >> loading configuration file /home/student2021/srcao/base-model/llama2-7b/config.json
[INFO|configuration_utils.py:789] 2024-05-26 12:52:34,944 >> Model config LlamaConfig {
  "_name_or_path": "/home/student2021/srcao/base-model/llama2-7b",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3280] 2024-05-26 12:52:34,968 >> loading weights file /home/student2021/srcao/base-model/llama2-7b/model.safetensors.index.json
[INFO|modeling_utils.py:1417] 2024-05-26 12:52:34,968 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-05-26 12:52:34,969 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.52it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.81it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.76it/s]
05/26/2024 12:52:37 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:52:37 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]05/26/2024 12:52:37 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/26/2024 12:52:37 - INFO - llmtuner.data.template - Add pad token: </s>
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.44it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.43it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.50it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.57it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.54it/s]
[INFO|modeling_utils.py:4024] 2024-05-26 12:52:37,916 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-05-26 12:52:37,916 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/student2021/srcao/base-model/llama2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:881] 2024-05-26 12:52:37,918 >> loading configuration file /home/student2021/srcao/base-model/llama2-7b/generation_config.json
[INFO|configuration_utils.py:928] 2024-05-26 12:52:37,919 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

05/26/2024 12:52:37 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:52:37 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.57it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.55it/s]
05/26/2024 12:52:37 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:52:37 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.62it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.60it/s]
05/26/2024 12:52:37 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:52:37 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
05/26/2024 12:52:38 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/26/2024 12:52:38 - INFO - llmtuner.data.template - Add pad token: </s>
05/26/2024 12:52:38 - INFO - llmtuner.data.loader - Loading dataset multi-role-train.json...
05/26/2024 12:52:38 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/26/2024 12:52:38 - INFO - llmtuner.data.template - Add pad token: </s>
05/26/2024 12:52:38 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/26/2024 12:52:38 - INFO - llmtuner.data.template - Add pad token: </s>
05/26/2024 12:52:39 - INFO - llmtuner.data.loader - Loading dataset multi-role-train.json...
05/26/2024 12:52:39 - INFO - llmtuner.data.loader - Loading dataset multi-role-train.json...
05/26/2024 12:52:39 - INFO - llmtuner.data.loader - Loading dataset multi-role-train.json...
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 31, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/sft/workflow.py", line 32, in run_sft
    dataset = get_dataset(tokenizer, model_args, data_args, training_args, stage="sft")
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 162, in get_dataset
    all_datasets.append(load_single_dataset(dataset_attr, model_args, data_args))
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 59, in load_single_dataset
    raise ValueError("File not found.")
ValueError: File not found.
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 31, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/sft/workflow.py", line 32, in run_sft
    dataset = get_dataset(tokenizer, model_args, data_args, training_args, stage="sft")
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 162, in get_dataset
    all_datasets.append(load_single_dataset(dataset_attr, model_args, data_args))
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 59, in load_single_dataset
    raise ValueError("File not found.")
ValueError: File not found.
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 31, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/sft/workflow.py", line 32, in run_sft
    dataset = get_dataset(tokenizer, model_args, data_args, training_args, stage="sft")
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 162, in get_dataset
    all_datasets.append(load_single_dataset(dataset_attr, model_args, data_args))
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 59, in load_single_dataset
    raise ValueError("File not found.")
ValueError: File not found.
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 31, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/sft/workflow.py", line 32, in run_sft
    dataset = get_dataset(tokenizer, model_args, data_args, training_args, stage="sft")
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 162, in get_dataset
    all_datasets.append(load_single_dataset(dataset_attr, model_args, data_args))
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 59, in load_single_dataset
    raise ValueError("File not found.")
ValueError: File not found.
[2024-05-26 12:52:41,990] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 23648
[2024-05-26 12:52:41,993] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 23649
[2024-05-26 12:52:42,006] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 23650
[2024-05-26 12:52:42,014] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 23651
[2024-05-26 12:52:42,020] [ERROR] [launch.py:322:sigkill_handler] ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=3', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb'] exits with return code = 1
[2024-05-26 12:53:49,592] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:53:51,408] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-26 12:53:51,430] [INFO] [runner.py:568:main] cmd = /home/student2021/anaconda3/envs/llama_factory/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None src/train_bash.py --stage sft --do_train --model_name_or_path /home/student2021/srcao/base-model/llama2-7b --dataset multi_role_train --template default --finetuning_type lora --lora_alpha 16 --lora_dropout 0.05 --lora_rank 8 --lora_target q_proj,v_proj --output_dir sft_checkpoint/llama2-7b-multi_role_train --overwrite_cache --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --lr_scheduler_type linear --warmup_steps 200 --max_length 1024 --logging_steps 10 --save_steps 1000 --learning_rate 5e-5 --num_train_epochs 3.0 --plot_loss --fp16 --report_to=wandb
[2024-05-26 12:53:53,252] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:53:54,965] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-05-26 12:53:54,965] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-05-26 12:53:54,965] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-05-26 12:53:54,965] [INFO] [launch.py:163:main] dist_world_size=4
[2024-05-26 12:53:54,965] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-05-26 12:53:54,966] [INFO] [launch.py:253:main] process 24535 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=0', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:53:54,967] [INFO] [launch.py:253:main] process 24536 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=1', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:53:54,967] [INFO] [launch.py:253:main] process 24537 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=2', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:53:54,968] [INFO] [launch.py:253:main] process 24538 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=3', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:53:58,951] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:53:58,985] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:53:58,985] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:53:58,991] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
05/26/2024 12:54:04 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/26/2024 12:54:04 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1934] 2024-05-26 12:54:04,763 >> PyTorch: setting up devices
05/26/2024 12:54:04 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/26/2024 12:54:04 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:54:04 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 12:54:04 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train/runs/May26_12-54-03_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:54:04 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:54:04,817 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:54:04,818 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:54:04,818 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:54:04,818 >> loading file tokenizer_config.json
05/26/2024 12:54:04 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train/runs/May26_12-54-03_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:54:04,818 >> loading file tokenizer.json
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:54:04 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:54:04 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 12:54:04 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train/runs/May26_12-54-03_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
05/26/2024 12:54:04 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train/runs/May26_12-54-03_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
[INFO|configuration_utils.py:724] 2024-05-26 12:54:04,932 >> loading configuration file /home/student2021/srcao/base-model/llama2-7b/config.json
[INFO|configuration_utils.py:789] 2024-05-26 12:54:04,933 >> Model config LlamaConfig {
  "_name_or_path": "/home/student2021/srcao/base-model/llama2-7b",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3280] 2024-05-26 12:54:04,953 >> loading weights file /home/student2021/srcao/base-model/llama2-7b/model.safetensors.index.json
[INFO|modeling_utils.py:1417] 2024-05-26 12:54:04,953 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-05-26 12:54:04,954 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.51it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.57it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.56it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  3.28it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.71it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.68it/s]
05/26/2024 12:54:08 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:54:08 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.77it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.74it/s]
05/26/2024 12:54:08 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:54:08 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.77it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.73it/s]
05/26/2024 12:54:08 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:54:08 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
05/26/2024 12:54:08 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/26/2024 12:54:08 - INFO - llmtuner.data.template - Add pad token: </s>
05/26/2024 12:54:08 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/26/2024 12:54:08 - INFO - llmtuner.data.template - Add pad token: </s>
05/26/2024 12:54:08 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/26/2024 12:54:08 - INFO - llmtuner.data.template - Add pad token: </s>
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.42it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.39it/s]
[INFO|modeling_utils.py:4024] 2024-05-26 12:54:08,221 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-05-26 12:54:08,221 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/student2021/srcao/base-model/llama2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:881] 2024-05-26 12:54:08,224 >> loading configuration file /home/student2021/srcao/base-model/llama2-7b/generation_config.json
[INFO|configuration_utils.py:928] 2024-05-26 12:54:08,224 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

05/26/2024 12:54:08 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:54:08 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
05/26/2024 12:54:08 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/26/2024 12:54:08 - INFO - llmtuner.data.template - Add pad token: </s>
05/26/2024 12:54:08 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/26/2024 12:54:10 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/26/2024 12:54:10 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/26/2024 12:54:10 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 31, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/sft/workflow.py", line 32, in run_sft
    dataset = get_dataset(tokenizer, model_args, data_args, training_args, stage="sft")
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 162, in get_dataset
    all_datasets.append(load_single_dataset(dataset_attr, model_args, data_args))
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 59, in load_single_dataset
    raise ValueError("File not found.")
ValueError: File not found.
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 31, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/sft/workflow.py", line 32, in run_sft
    dataset = get_dataset(tokenizer, model_args, data_args, training_args, stage="sft")
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 162, in get_dataset
    all_datasets.append(load_single_dataset(dataset_attr, model_args, data_args))
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 59, in load_single_dataset
    raise ValueError("File not found.")
ValueError: File not found.
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 31, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/sft/workflow.py", line 32, in run_sft
    dataset = get_dataset(tokenizer, model_args, data_args, training_args, stage="sft")
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 162, in get_dataset
    all_datasets.append(load_single_dataset(dataset_attr, model_args, data_args))
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 59, in load_single_dataset
    raise ValueError("File not found.")
ValueError: File not found.
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 31, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/sft/workflow.py", line 32, in run_sft
    dataset = get_dataset(tokenizer, model_args, data_args, training_args, stage="sft")
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 162, in get_dataset
    all_datasets.append(load_single_dataset(dataset_attr, model_args, data_args))
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/data/loader.py", line 59, in load_single_dataset
    raise ValueError("File not found.")
ValueError: File not found.
[2024-05-26 12:54:11,986] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 24535
[2024-05-26 12:54:12,003] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 24536
[2024-05-26 12:54:12,003] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 24537
[2024-05-26 12:54:12,011] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 24538
[2024-05-26 12:54:12,017] [ERROR] [launch.py:322:sigkill_handler] ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=3', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb'] exits with return code = 1
[2024-05-26 12:55:26,375] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:55:28,157] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-26 12:55:28,179] [INFO] [runner.py:568:main] cmd = /home/student2021/anaconda3/envs/llama_factory/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None src/train_bash.py --stage sft --do_train --model_name_or_path /home/student2021/srcao/base-model/llama2-7b --dataset multi_role_train --template default --finetuning_type lora --lora_alpha 16 --lora_dropout 0.05 --lora_rank 8 --lora_target q_proj,v_proj --output_dir sft_checkpoint/llama2-7b-multi_role_train --overwrite_cache --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --lr_scheduler_type linear --warmup_steps 200 --max_length 1024 --logging_steps 10 --save_steps 1000 --learning_rate 5e-5 --num_train_epochs 3.0 --plot_loss --fp16 --report_to=wandb
[2024-05-26 12:55:29,990] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:55:31,672] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-05-26 12:55:31,672] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-05-26 12:55:31,672] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-05-26 12:55:31,672] [INFO] [launch.py:163:main] dist_world_size=4
[2024-05-26 12:55:31,672] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-05-26 12:55:31,672] [INFO] [launch.py:253:main] process 25412 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=0', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:55:31,673] [INFO] [launch.py:253:main] process 25413 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=1', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:55:31,674] [INFO] [launch.py:253:main] process 25414 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=2', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:55:31,674] [INFO] [launch.py:253:main] process 25415 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=3', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-26 12:55:35,692] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:55:35,710] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:55:35,714] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-26 12:55:35,725] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
05/26/2024 12:55:41 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1934] 2024-05-26 12:55:41,536 >> PyTorch: setting up devices
05/26/2024 12:55:41 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/26/2024 12:55:41 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/26/2024 12:55:41 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:55:41 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 12:55:41 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train/runs/May26_12-55-40_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:55:41 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 12:55:41 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train/runs/May26_12-55-40_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:55:41,593 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:55:41,594 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:55:41,594 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:55:41,594 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2082] 2024-05-26 12:55:41,594 >> loading file tokenizer.json
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:55:41 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 12:55:41 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train/runs/May26_12-55-40_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 12:55:41 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 12:55:41 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train/runs/May26_12-55-40_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
[INFO|configuration_utils.py:724] 2024-05-26 12:55:41,706 >> loading configuration file /home/student2021/srcao/base-model/llama2-7b/config.json
[INFO|configuration_utils.py:789] 2024-05-26 12:55:41,709 >> Model config LlamaConfig {
  "_name_or_path": "/home/student2021/srcao/base-model/llama2-7b",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3280] 2024-05-26 12:55:41,729 >> loading weights file /home/student2021/srcao/base-model/llama2-7b/model.safetensors.index.json
[INFO|modeling_utils.py:1417] 2024-05-26 12:55:41,729 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-05-26 12:55:41,730 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.44it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.49it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.53it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.49it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.71it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.68it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.68it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.64it/s]
[INFO|modeling_utils.py:4024] 2024-05-26 12:55:44,795 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-05-26 12:55:44,795 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/student2021/srcao/base-model/llama2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.62it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.59it/s]
[INFO|configuration_utils.py:881] 2024-05-26 12:55:44,797 >> loading configuration file /home/student2021/srcao/base-model/llama2-7b/generation_config.json
[INFO|configuration_utils.py:928] 2024-05-26 12:55:44,798 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

05/26/2024 12:55:44 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:55:44 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
05/26/2024 12:55:44 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:55:44 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
05/26/2024 12:55:44 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:55:44 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.68it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.65it/s]
05/26/2024 12:55:44 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/26/2024 12:55:44 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
05/26/2024 12:55:44 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/26/2024 12:55:44 - INFO - llmtuner.data.template - Add pad token: </s>
05/26/2024 12:55:44 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/26/2024 12:55:44 - INFO - llmtuner.data.template - Add pad token: </s>
05/26/2024 12:55:44 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/26/2024 12:55:44 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
05/26/2024 12:55:44 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/26/2024 12:55:44 - INFO - llmtuner.data.template - Add pad token: </s>
05/26/2024 12:55:44 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/26/2024 12:55:44 - INFO - llmtuner.data.template - Add pad token: </s>
Using custom data configuration default-a0e2d6f8a6470c31
Loading Dataset Infos from /home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
Downloading and preparing dataset json/default to /home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05...
Downloading took 0.0 min
Checksum Computation took 0.0 min
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 3105 examples [00:00, 66716.77 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05. Subsequent calls will reuse this data.
Converting format of dataset:   0%|          | 0/3105 [00:00<?, ? examples/s]Caching processed dataset at /home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-4209f69bdd4b11d1.arrow
Converting format of dataset: 100%|██████████| 3105/3105 [00:00<00:00, 78591.94 examples/s]
05/26/2024 12:55:47 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/26/2024 12:55:47 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/26/2024 12:55:47 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/26/2024 12:55:47 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
05/26/2024 12:55:47 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
05/26/2024 12:55:47 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
Running tokenizer on dataset:   0%|          | 0/3105 [00:00<?, ? examples/s]Converting format of dataset:   0%|          | 0/3105 [00:00<?, ? examples/s]Converting format of dataset:   0%|          | 0/3105 [00:00<?, ? examples/s]Converting format of dataset:   0%|          | 0/3105 [00:00<?, ? examples/s]Converting format of dataset: 100%|██████████| 3105/3105 [00:00<00:00, 77055.56 examples/s]
Converting format of dataset: 100%|██████████| 3105/3105 [00:00<00:00, 77066.50 examples/s]
Converting format of dataset: 100%|██████████| 3105/3105 [00:00<00:00, 76745.87 examples/s]
Caching processed dataset at /home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-df1e39600e800f67.arrow
Running tokenizer on dataset:  32%|███▏      | 1000/3105 [00:01<00:03, 680.48 examples/s]Running tokenizer on dataset:  64%|██████▍   | 2000/3105 [00:02<00:01, 691.81 examples/s]Running tokenizer on dataset:  97%|█████████▋| 3000/3105 [00:04<00:00, 689.22 examples/s]Running tokenizer on dataset: 100%|██████████| 3105/3105 [00:04<00:00, 688.36 examples/s]Running tokenizer on dataset: 100%|██████████| 3105/3105 [00:04<00:00, 688.06 examples/s]
input_ids:
[12968, 29901, 13355, 1598, 322, 5353, 278, 2775, 537, 310, 5046, 269, 12358, 327, 7384, 322, 4768, 2129, 2729, 373, 278, 7928, 434, 29889, 13, 2677, 29901, 8565, 1558, 291, 1048, 29871, 29906, 29955, 29945, 29889, 10575, 727, 367, 5046, 13071, 363, 5221, 1218, 297, 3058, 28709, 1288, 14188, 9808, 13, 13, 7900, 22137, 29901, 29871, 1094, 263, 29871, 29906, 29900, 29899, 6360, 29899, 1025, 5375, 29892, 306, 11188, 278, 3736, 1103, 3145, 7258, 491, 278, 29871, 29946, 29900, 29899, 6360, 29899, 1025, 5375, 313, 17662, 29897, 322, 278, 29871, 29953, 29900, 29899, 6360, 29899, 1025, 5375, 313, 1025, 29897, 373, 5046, 13071, 363, 28709, 1288, 14188, 29889, 739, 29915, 29879, 18853, 304, 2050, 1716, 15332, 21838, 322, 278, 7037, 25722, 29884, 362, 310, 269, 12358, 327, 7384, 746, 5353, 292, 1316, 25091, 29889, 21605, 5046, 13071, 508, 367, 263, 982, 304, 7536, 277, 675, 15332, 322, 5557, 19039, 5161, 2039, 363, 15724, 1058, 1122, 451, 505, 278, 1021, 9128, 633, 9770, 470, 286, 1337, 537, 11174, 29889, 2398, 29892, 372, 29915, 29879, 7618, 1455, 304, 2948, 1438, 13071, 411, 4771, 24858, 304, 4772, 2313, 20386, 7606, 23274, 2729, 14419, 368, 373, 5046, 29889, 383, 4015, 263, 17346, 393, 5662, 1973, 15332, 1550, 2504, 11427, 20978, 2068, 363, 15724, 310, 599, 24646, 338, 1820, 29889, 3374, 366, 1716, 363, 3033, 6751, 297, 445, 6593, 1319, 10679, 29889, 2]
inputs:
Human: Identify and discuss the severity of age stereotypes and biases based on the dialogue.
Context: Discussion about 275. Should there be age limits for participating in certain recreational activities?.

Assistant:  As a 20-year-old individual, I appreciate the perspectives shared by the 40-year-old individual (middle) and the 60-year-old individual (old) on age limits for recreational activities. It's essential to consider both safety concerns and the potential perpetuation of stereotypes when discussing such restrictions. Setting age limits can be a way to prioritize safety and prevent unnecessary risks for individuals who may not have the same physical abilities or maturity levels. However, it's crucial to approach these limits with sensitivity to avoid discriminatory practices based solely on age. Finding a balance that ensures safety while promoting inclusivity for individuals of all ages is key. Thank you both for engaging in this meaningful discussion.</s>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1094, 263, 29871, 29906, 29900, 29899, 6360, 29899, 1025, 5375, 29892, 306, 11188, 278, 3736, 1103, 3145, 7258, 491, 278, 29871, 29946, 29900, 29899, 6360, 29899, 1025, 5375, 313, 17662, 29897, 322, 278, 29871, 29953, 29900, 29899, 6360, 29899, 1025, 5375, 313, 1025, 29897, 373, 5046, 13071, 363, 28709, 1288, 14188, 29889, 739, 29915, 29879, 18853, 304, 2050, 1716, 15332, 21838, 322, 278, 7037, 25722, 29884, 362, 310, 269, 12358, 327, 7384, 746, 5353, 292, 1316, 25091, 29889, 21605, 5046, 13071, 508, 367, 263, 982, 304, 7536, 277, 675, 15332, 322, 5557, 19039, 5161, 2039, 363, 15724, 1058, 1122, 451, 505, 278, 1021, 9128, 633, 9770, 470, 286, 1337, 537, 11174, 29889, 2398, 29892, 372, 29915, 29879, 7618, 1455, 304, 2948, 1438, 13071, 411, 4771, 24858, 304, 4772, 2313, 20386, 7606, 23274, 2729, 14419, 368, 373, 5046, 29889, 383, 4015, 263, 17346, 393, 5662, 1973, 15332, 1550, 2504, 11427, 20978, 2068, 363, 15724, 310, 599, 24646, 338, 1820, 29889, 3374, 366, 1716, 363, 3033, 6751, 297, 445, 6593, 1319, 10679, 29889, 2]
labels:
As a 20-year-old individual, I appreciate the perspectives shared by the 40-year-old individual (middle) and the 60-year-old individual (old) on age limits for recreational activities. It's essential to consider both safety concerns and the potential perpetuation of stereotypes when discussing such restrictions. Setting age limits can be a way to prioritize safety and prevent unnecessary risks for individuals who may not have the same physical abilities or maturity levels. However, it's crucial to approach these limits with sensitivity to avoid discriminatory practices based solely on age. Finding a balance that ensures safety while promoting inclusivity for individuals of all ages is key. Thank you both for engaging in this meaningful discussion.</s>
[INFO|training_args.py:1934] 2024-05-26 12:55:52,217 >> PyTorch: setting up devices
Running tokenizer on dataset:   0%|          | 0/3105 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/3105 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/3105 [00:00<?, ? examples/s]/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Running tokenizer on dataset:  32%|███▏      | 1000/3105 [00:01<00:03, 675.51 examples/s]Running tokenizer on dataset:  32%|███▏      | 1000/3105 [00:01<00:03, 674.69 examples/s]Running tokenizer on dataset:  32%|███▏      | 1000/3105 [00:01<00:03, 673.25 examples/s]Running tokenizer on dataset:  64%|██████▍   | 2000/3105 [00:02<00:01, 684.43 examples/s]Running tokenizer on dataset:  64%|██████▍   | 2000/3105 [00:02<00:01, 682.04 examples/s]Running tokenizer on dataset:  64%|██████▍   | 2000/3105 [00:02<00:01, 674.71 examples/s][INFO|trainer.py:607] 2024-05-26 12:55:55,928 >> Using auto half precision backend
Running tokenizer on dataset:  97%|█████████▋| 3000/3105 [00:04<00:00, 683.50 examples/s]Running tokenizer on dataset:  97%|█████████▋| 3000/3105 [00:04<00:00, 680.93 examples/s]Running tokenizer on dataset:  97%|█████████▋| 3000/3105 [00:04<00:00, 674.85 examples/s]Running tokenizer on dataset: 100%|██████████| 3105/3105 [00:04<00:00, 682.56 examples/s]Running tokenizer on dataset: 100%|██████████| 3105/3105 [00:04<00:00, 682.02 examples/s]
Running tokenizer on dataset: 100%|██████████| 3105/3105 [00:04<00:00, 679.87 examples/s]Running tokenizer on dataset: 100%|██████████| 3105/3105 [00:04<00:00, 679.76 examples/s]
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Running tokenizer on dataset: 100%|██████████| 3105/3105 [00:04<00:00, 674.47 examples/s]Running tokenizer on dataset: 100%|██████████| 3105/3105 [00:04<00:00, 674.20 examples/s]
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:1969] 2024-05-26 12:56:00,949 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-05-26 12:56:00,949 >>   Num examples = 3,105
[INFO|trainer.py:1971] 2024-05-26 12:56:00,949 >>   Num Epochs = 3
[INFO|trainer.py:1972] 2024-05-26 12:56:00,949 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-05-26 12:56:00,949 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1976] 2024-05-26 12:56:00,949 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1977] 2024-05-26 12:56:00,949 >>   Total optimization steps = 582
[INFO|trainer.py:1978] 2024-05-26 12:56:00,954 >>   Number of trainable parameters = 4,194,304
[INFO|integration_utils.py:723] 2024-05-26 12:56:01,160 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: srcao (srcao-bingo). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /home/student2021/srcao/LLaMA-Factory-main/wandb/run-20240526_125602-eveuy2pn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-universe-193
wandb: ⭐️ View project at https://wandb.ai/srcao-bingo/huggingface
wandb: 🚀 View run at https://wandb.ai/srcao-bingo/huggingface/runs/eveuy2pn/workspace
  0%|          | 0/582 [00:00<?, ?it/s]  0%|          | 1/582 [00:01<16:08,  1.67s/it]  0%|          | 2/582 [00:02<13:08,  1.36s/it]  1%|          | 3/582 [00:03<12:11,  1.26s/it]  1%|          | 4/582 [00:05<11:20,  1.18s/it]  1%|          | 5/582 [00:06<10:43,  1.12s/it]  1%|          | 6/582 [00:07<10:29,  1.09s/it]  1%|          | 7/582 [00:08<10:12,  1.06s/it]  1%|▏         | 8/582 [00:09<10:27,  1.09s/it]  2%|▏         | 9/582 [00:10<10:20,  1.08s/it]  2%|▏         | 10/582 [00:11<10:22,  1.09s/it]                                                {'loss': 1.8979, 'grad_norm': 0.2701222896575928, 'learning_rate': 2.5e-06, 'epoch': 0.05}
  2%|▏         | 10/582 [00:11<10:22,  1.09s/it]  2%|▏         | 11/582 [00:12<10:16,  1.08s/it]  2%|▏         | 12/582 [00:13<10:15,  1.08s/it]  2%|▏         | 13/582 [00:14<10:10,  1.07s/it]  2%|▏         | 14/582 [00:15<10:08,  1.07s/it]  3%|▎         | 15/582 [00:16<10:08,  1.07s/it]  3%|▎         | 16/582 [00:17<10:06,  1.07s/it]  3%|▎         | 17/582 [00:18<10:15,  1.09s/it]  3%|▎         | 18/582 [00:19<10:09,  1.08s/it]  3%|▎         | 19/582 [00:21<10:05,  1.08s/it]  3%|▎         | 20/582 [00:22<10:03,  1.07s/it]                                                {'loss': 1.7628, 'grad_norm': 0.38618046045303345, 'learning_rate': 5e-06, 'epoch': 0.1}
  3%|▎         | 20/582 [00:22<10:03,  1.07s/it]  4%|▎         | 21/582 [00:23<10:01,  1.07s/it]  4%|▍         | 22/582 [00:24<09:58,  1.07s/it]  4%|▍         | 23/582 [00:25<09:57,  1.07s/it]  4%|▍         | 24/582 [00:26<10:05,  1.09s/it]  4%|▍         | 25/582 [00:27<10:07,  1.09s/it]  4%|▍         | 26/582 [00:28<10:02,  1.08s/it]  5%|▍         | 27/582 [00:29<09:57,  1.08s/it]  5%|▍         | 28/582 [00:30<10:21,  1.12s/it]  5%|▍         | 29/582 [00:31<10:13,  1.11s/it]  5%|▌         | 30/582 [00:33<10:02,  1.09s/it]                                                {'loss': 1.8208, 'grad_norm': 0.44106966257095337, 'learning_rate': 7.5e-06, 'epoch': 0.15}
  5%|▌         | 30/582 [00:33<10:02,  1.09s/it]  5%|▌         | 31/582 [00:34<09:49,  1.07s/it]  5%|▌         | 32/582 [00:35<09:47,  1.07s/it]  6%|▌         | 33/582 [00:36<09:59,  1.09s/it]  6%|▌         | 34/582 [00:37<09:50,  1.08s/it]  6%|▌         | 35/582 [00:38<10:00,  1.10s/it]  6%|▌         | 36/582 [00:39<09:50,  1.08s/it]  6%|▋         | 37/582 [00:40<09:36,  1.06s/it]  7%|▋         | 38/582 [00:41<09:30,  1.05s/it]  7%|▋         | 39/582 [00:42<09:27,  1.04s/it]  7%|▋         | 40/582 [00:43<09:18,  1.03s/it]                                                {'loss': 1.9474, 'grad_norm': 0.9541647434234619, 'learning_rate': 9.750000000000002e-06, 'epoch': 0.21}
  7%|▋         | 40/582 [00:43<09:18,  1.03s/it]  7%|▋         | 41/582 [00:44<09:13,  1.02s/it]  7%|▋         | 42/582 [00:45<09:09,  1.02s/it]  7%|▋         | 43/582 [00:46<09:06,  1.01s/it]  8%|▊         | 44/582 [00:47<09:05,  1.01s/it]  8%|▊         | 45/582 [00:48<09:27,  1.06s/it]  8%|▊         | 46/582 [00:49<09:21,  1.05s/it]  8%|▊         | 47/582 [00:50<09:22,  1.05s/it]  8%|▊         | 48/582 [00:51<09:24,  1.06s/it]  8%|▊         | 49/582 [00:52<09:20,  1.05s/it]  9%|▊         | 50/582 [00:53<09:17,  1.05s/it]                                                {'loss': 1.7502, 'grad_norm': 0.5044997930526733, 'learning_rate': 1.225e-05, 'epoch': 0.26}
  9%|▊         | 50/582 [00:53<09:17,  1.05s/it]  9%|▉         | 51/582 [00:54<09:12,  1.04s/it]  9%|▉         | 52/582 [00:56<09:05,  1.03s/it]  9%|▉         | 53/582 [00:57<09:01,  1.02s/it]  9%|▉         | 54/582 [00:58<09:01,  1.03s/it]  9%|▉         | 55/582 [00:59<09:02,  1.03s/it] 10%|▉         | 56/582 [01:00<08:57,  1.02s/it] 10%|▉         | 57/582 [01:01<08:53,  1.02s/it] 10%|▉         | 58/582 [01:02<08:57,  1.03s/it] 10%|█         | 59/582 [01:03<08:52,  1.02s/it] 10%|█         | 60/582 [01:04<08:54,  1.02s/it]                                                {'loss': 1.7627, 'grad_norm': 0.5747163891792297, 'learning_rate': 1.475e-05, 'epoch': 0.31}
 10%|█         | 60/582 [01:04<08:54,  1.02s/it] 10%|█         | 61/582 [01:05<09:01,  1.04s/it] 11%|█         | 62/582 [01:06<09:11,  1.06s/it] 11%|█         | 63/582 [01:07<09:11,  1.06s/it] 11%|█         | 64/582 [01:08<09:07,  1.06s/it] 11%|█         | 65/582 [01:09<08:57,  1.04s/it] 11%|█▏        | 66/582 [01:10<08:50,  1.03s/it] 12%|█▏        | 67/582 [01:11<08:51,  1.03s/it] 12%|█▏        | 68/582 [01:12<09:02,  1.06s/it] 12%|█▏        | 69/582 [01:13<08:54,  1.04s/it] 12%|█▏        | 70/582 [01:14<08:52,  1.04s/it]                                                {'loss': 1.732, 'grad_norm': 0.6571749448776245, 'learning_rate': 1.725e-05, 'epoch': 0.36}
 12%|█▏        | 70/582 [01:14<08:52,  1.04s/it] 12%|█▏        | 71/582 [01:15<08:50,  1.04s/it] 12%|█▏        | 72/582 [01:16<08:48,  1.04s/it] 13%|█▎        | 73/582 [01:17<08:49,  1.04s/it] 13%|█▎        | 74/582 [01:18<08:44,  1.03s/it] 13%|█▎        | 75/582 [01:19<08:53,  1.05s/it] 13%|█▎        | 76/582 [01:21<09:18,  1.10s/it] 13%|█▎        | 77/582 [01:22<09:01,  1.07s/it] 13%|█▎        | 78/582 [01:23<09:00,  1.07s/it] 14%|█▎        | 79/582 [01:24<08:54,  1.06s/it] 14%|█▎        | 80/582 [01:25<08:44,  1.04s/it]                                                {'loss': 1.6042, 'grad_norm': 0.6562529802322388, 'learning_rate': 1.9750000000000002e-05, 'epoch': 0.41}
 14%|█▎        | 80/582 [01:25<08:44,  1.04s/it] 14%|█▍        | 81/582 [01:26<08:44,  1.05s/it] 14%|█▍        | 82/582 [01:27<08:47,  1.06s/it] 14%|█▍        | 83/582 [01:28<08:43,  1.05s/it] 14%|█▍        | 84/582 [01:29<08:51,  1.07s/it] 15%|█▍        | 85/582 [01:30<08:42,  1.05s/it] 15%|█▍        | 86/582 [01:31<08:55,  1.08s/it] 15%|█▍        | 87/582 [01:32<08:42,  1.06s/it] 15%|█▌        | 88/582 [01:33<08:39,  1.05s/it] 15%|█▌        | 89/582 [01:34<08:33,  1.04s/it] 15%|█▌        | 90/582 [01:35<08:30,  1.04s/it]                                                {'loss': 1.5988, 'grad_norm': 0.6940439343452454, 'learning_rate': 2.2250000000000002e-05, 'epoch': 0.46}
 15%|█▌        | 90/582 [01:35<08:30,  1.04s/it] 16%|█▌        | 91/582 [01:36<08:28,  1.03s/it] 16%|█▌        | 92/582 [01:37<08:30,  1.04s/it] 16%|█▌        | 93/582 [01:38<08:29,  1.04s/it] 16%|█▌        | 94/582 [01:39<08:28,  1.04s/it] 16%|█▋        | 95/582 [01:41<08:34,  1.06s/it] 16%|█▋        | 96/582 [01:42<08:53,  1.10s/it] 17%|█▋        | 97/582 [01:43<09:04,  1.12s/it] 17%|█▋        | 98/582 [01:44<09:01,  1.12s/it] 17%|█▋        | 99/582 [01:45<09:07,  1.13s/it] 17%|█▋        | 100/582 [01:46<09:10,  1.14s/it]                                                 {'loss': 1.4324, 'grad_norm': 0.5763677358627319, 'learning_rate': 2.4750000000000002e-05, 'epoch': 0.51}
 17%|█▋        | 100/582 [01:46<09:10,  1.14s/it] 17%|█▋        | 101/582 [01:47<09:13,  1.15s/it] 18%|█▊        | 102/582 [01:49<09:09,  1.14s/it] 18%|█▊        | 103/582 [01:50<08:48,  1.10s/it] 18%|█▊        | 104/582 [01:51<08:34,  1.08s/it] 18%|█▊        | 105/582 [01:52<08:22,  1.05s/it] 18%|█▊        | 106/582 [01:53<08:39,  1.09s/it] 18%|█▊        | 107/582 [01:54<08:33,  1.08s/it] 19%|█▊        | 108/582 [01:55<08:40,  1.10s/it] 19%|█▊        | 109/582 [01:56<08:47,  1.12s/it] 19%|█▉        | 110/582 [01:57<08:39,  1.10s/it]                                                 {'loss': 1.4551, 'grad_norm': 0.7873831987380981, 'learning_rate': 2.725e-05, 'epoch': 0.57}
 19%|█▉        | 110/582 [01:57<08:39,  1.10s/it] 19%|█▉        | 111/582 [01:58<08:30,  1.08s/it] 19%|█▉        | 112/582 [01:59<08:19,  1.06s/it] 19%|█▉        | 113/582 [02:00<08:08,  1.04s/it] 20%|█▉        | 114/582 [02:01<08:02,  1.03s/it] 20%|█▉        | 115/582 [02:02<07:58,  1.02s/it] 20%|█▉        | 116/582 [02:03<07:54,  1.02s/it] 20%|██        | 117/582 [02:04<08:11,  1.06s/it] 20%|██        | 118/582 [02:06<08:30,  1.10s/it] 20%|██        | 119/582 [02:07<08:24,  1.09s/it] 21%|██        | 120/582 [02:08<08:19,  1.08s/it]                                                 {'loss': 1.3893, 'grad_norm': 0.668813169002533, 'learning_rate': 2.975e-05, 'epoch': 0.62}
 21%|██        | 120/582 [02:08<08:19,  1.08s/it] 21%|██        | 121/582 [02:09<08:21,  1.09s/it] 21%|██        | 122/582 [02:10<08:16,  1.08s/it] 21%|██        | 123/582 [02:11<08:16,  1.08s/it] 21%|██▏       | 124/582 [02:12<08:10,  1.07s/it] 21%|██▏       | 125/582 [02:13<08:08,  1.07s/it] 22%|██▏       | 126/582 [02:14<08:05,  1.07s/it] 22%|██▏       | 127/582 [02:15<08:03,  1.06s/it] 22%|██▏       | 128/582 [02:16<08:02,  1.06s/it] 22%|██▏       | 129/582 [02:17<08:04,  1.07s/it] 22%|██▏       | 130/582 [02:18<08:00,  1.06s/it]                                                 {'loss': 1.313, 'grad_norm': 0.7980611324310303, 'learning_rate': 3.2250000000000005e-05, 'epoch': 0.67}
 22%|██▏       | 130/582 [02:18<08:00,  1.06s/it] 23%|██▎       | 131/582 [02:20<07:59,  1.06s/it] 23%|██▎       | 132/582 [02:21<07:56,  1.06s/it] 23%|██▎       | 133/582 [02:22<07:52,  1.05s/it] 23%|██▎       | 134/582 [02:23<07:54,  1.06s/it] 23%|██▎       | 135/582 [02:24<07:50,  1.05s/it] 23%|██▎       | 136/582 [02:25<08:02,  1.08s/it] 24%|██▎       | 137/582 [02:26<07:49,  1.06s/it] 24%|██▎       | 138/582 [02:27<07:42,  1.04s/it] 24%|██▍       | 139/582 [02:28<07:56,  1.07s/it] 24%|██▍       | 140/582 [02:29<07:47,  1.06s/it]                                                 {'loss': 1.2912, 'grad_norm': 0.9092127680778503, 'learning_rate': 3.475e-05, 'epoch': 0.72}
 24%|██▍       | 140/582 [02:29<07:47,  1.06s/it] 24%|██▍       | 141/582 [02:30<07:59,  1.09s/it] 24%|██▍       | 142/582 [02:31<07:58,  1.09s/it] 25%|██▍       | 143/582 [02:32<07:45,  1.06s/it] 25%|██▍       | 144/582 [02:33<07:36,  1.04s/it] 25%|██▍       | 145/582 [02:34<07:35,  1.04s/it] 25%|██▌       | 146/582 [02:35<07:29,  1.03s/it] 25%|██▌       | 147/582 [02:36<07:34,  1.04s/it] 25%|██▌       | 148/582 [02:37<07:26,  1.03s/it] 26%|██▌       | 149/582 [02:38<07:24,  1.03s/it] 26%|██▌       | 150/582 [02:39<07:25,  1.03s/it]                                                 {'loss': 1.2135, 'grad_norm': 0.6682880520820618, 'learning_rate': 3.7250000000000004e-05, 'epoch': 0.77}
 26%|██▌       | 150/582 [02:39<07:25,  1.03s/it] 26%|██▌       | 151/582 [02:41<07:24,  1.03s/it] 26%|██▌       | 152/582 [02:42<07:24,  1.03s/it] 26%|██▋       | 153/582 [02:43<07:33,  1.06s/it] 26%|██▋       | 154/582 [02:44<07:32,  1.06s/it] 27%|██▋       | 155/582 [02:45<07:30,  1.05s/it] 27%|██▋       | 156/582 [02:46<07:23,  1.04s/it] 27%|██▋       | 157/582 [02:47<07:20,  1.04s/it] 27%|██▋       | 158/582 [02:48<07:32,  1.07s/it] 27%|██▋       | 159/582 [02:49<07:41,  1.09s/it] 27%|██▋       | 160/582 [02:50<07:47,  1.11s/it]                                                 {'loss': 1.1827, 'grad_norm': 0.7633703351020813, 'learning_rate': 3.9750000000000004e-05, 'epoch': 0.82}
 27%|██▋       | 160/582 [02:50<07:47,  1.11s/it] 28%|██▊       | 161/582 [02:51<07:50,  1.12s/it] 28%|██▊       | 162/582 [02:52<07:35,  1.09s/it] 28%|██▊       | 163/582 [02:53<07:29,  1.07s/it] 28%|██▊       | 164/582 [02:54<07:29,  1.07s/it] 28%|██▊       | 165/582 [02:56<07:38,  1.10s/it] 29%|██▊       | 166/582 [02:57<07:43,  1.11s/it] 29%|██▊       | 167/582 [02:58<07:46,  1.12s/it] 29%|██▉       | 168/582 [02:59<07:49,  1.13s/it] 29%|██▉       | 169/582 [03:00<07:50,  1.14s/it] 29%|██▉       | 170/582 [03:01<07:50,  1.14s/it]                                                 {'loss': 1.1903, 'grad_norm': 0.6595345139503479, 'learning_rate': 4.2250000000000004e-05, 'epoch': 0.88}
 29%|██▉       | 170/582 [03:01<07:50,  1.14s/it] 29%|██▉       | 171/582 [03:03<07:51,  1.15s/it] 30%|██▉       | 172/582 [03:04<07:51,  1.15s/it] 30%|██▉       | 173/582 [03:05<07:58,  1.17s/it] 30%|██▉       | 174/582 [03:06<07:53,  1.16s/it] 30%|███       | 175/582 [03:07<07:59,  1.18s/it] 30%|███       | 176/582 [03:08<07:54,  1.17s/it] 30%|███       | 177/582 [03:10<07:51,  1.17s/it] 31%|███       | 178/582 [03:11<07:48,  1.16s/it] 31%|███       | 179/582 [03:12<07:45,  1.15s/it] 31%|███       | 180/582 [03:13<07:43,  1.15s/it]                                                 {'loss': 1.1874, 'grad_norm': 0.6861891746520996, 'learning_rate': 4.4750000000000004e-05, 'epoch': 0.93}
 31%|███       | 180/582 [03:13<07:43,  1.15s/it] 31%|███       | 181/582 [03:14<07:42,  1.15s/it] 31%|███▏      | 182/582 [03:15<07:42,  1.16s/it] 31%|███▏      | 183/582 [03:17<07:41,  1.16s/it] 32%|███▏      | 184/582 [03:18<07:40,  1.16s/it] 32%|███▏      | 185/582 [03:19<07:35,  1.15s/it] 32%|███▏      | 186/582 [03:20<07:26,  1.13s/it] 32%|███▏      | 187/582 [03:21<07:25,  1.13s/it] 32%|███▏      | 188/582 [03:22<07:15,  1.10s/it] 32%|███▏      | 189/582 [03:23<07:05,  1.08s/it] 33%|███▎      | 190/582 [03:24<06:58,  1.07s/it]                                                 {'loss': 1.1919, 'grad_norm': 1.6553921699523926, 'learning_rate': 4.7249999999999997e-05, 'epoch': 0.98}
 33%|███▎      | 190/582 [03:24<06:58,  1.07s/it] 33%|███▎      | 191/582 [03:25<06:59,  1.07s/it] 33%|███▎      | 192/582 [03:26<06:55,  1.06s/it] 33%|███▎      | 193/582 [03:27<06:47,  1.05s/it] 33%|███▎      | 194/582 [03:28<06:43,  1.04s/it] 34%|███▎      | 195/582 [03:29<06:39,  1.03s/it] 34%|███▎      | 196/582 [03:30<06:52,  1.07s/it] 34%|███▍      | 197/582 [03:32<06:49,  1.06s/it] 34%|███▍      | 198/582 [03:33<06:43,  1.05s/it] 34%|███▍      | 199/582 [03:34<06:42,  1.05s/it] 34%|███▍      | 200/582 [03:35<06:39,  1.05s/it]                                                 {'loss': 1.1724, 'grad_norm': 1.5222079753875732, 'learning_rate': 4.975e-05, 'epoch': 1.03}
 34%|███▍      | 200/582 [03:35<06:39,  1.05s/it] 35%|███▍      | 201/582 [03:36<06:37,  1.04s/it] 35%|███▍      | 202/582 [03:37<06:54,  1.09s/it] 35%|███▍      | 203/582 [03:38<06:43,  1.07s/it] 35%|███▌      | 204/582 [03:39<06:36,  1.05s/it] 35%|███▌      | 205/582 [03:40<06:41,  1.06s/it] 35%|███▌      | 206/582 [03:41<06:32,  1.04s/it] 36%|███▌      | 207/582 [03:42<06:27,  1.03s/it] 36%|███▌      | 208/582 [03:43<06:27,  1.04s/it] 36%|███▌      | 209/582 [03:44<06:30,  1.05s/it] 36%|███▌      | 210/582 [03:45<06:27,  1.04s/it]                                                 {'loss': 1.1245, 'grad_norm': 0.7477182149887085, 'learning_rate': 4.8821989528795816e-05, 'epoch': 1.08}
 36%|███▌      | 210/582 [03:45<06:27,  1.04s/it] 36%|███▋      | 211/582 [03:46<06:32,  1.06s/it] 36%|███▋      | 212/582 [03:47<06:26,  1.04s/it] 37%|███▋      | 213/582 [03:48<06:21,  1.03s/it] 37%|███▋      | 214/582 [03:49<06:17,  1.03s/it] 37%|███▋      | 215/582 [03:50<06:20,  1.04s/it] 37%|███▋      | 216/582 [03:51<06:15,  1.03s/it] 37%|███▋      | 217/582 [03:52<06:25,  1.06s/it] 37%|███▋      | 218/582 [03:53<06:18,  1.04s/it] 38%|███▊      | 219/582 [03:54<06:13,  1.03s/it] 38%|███▊      | 220/582 [03:55<06:11,  1.03s/it]                                                 {'loss': 1.118, 'grad_norm': 0.8794032335281372, 'learning_rate': 4.7513089005235606e-05, 'epoch': 1.13}
 38%|███▊      | 220/582 [03:55<06:11,  1.03s/it] 38%|███▊      | 221/582 [03:56<06:09,  1.02s/it] 38%|███▊      | 222/582 [03:58<06:10,  1.03s/it] 38%|███▊      | 223/582 [03:59<06:07,  1.02s/it] 38%|███▊      | 224/582 [04:00<06:07,  1.03s/it] 39%|███▊      | 225/582 [04:01<06:05,  1.02s/it] 39%|███▉      | 226/582 [04:02<06:09,  1.04s/it] 39%|███▉      | 227/582 [04:03<06:22,  1.08s/it] 39%|███▉      | 228/582 [04:04<06:22,  1.08s/it] 39%|███▉      | 229/582 [04:05<06:15,  1.06s/it] 40%|███▉      | 230/582 [04:06<06:10,  1.05s/it]                                                 {'loss': 1.137, 'grad_norm': 0.8126106858253479, 'learning_rate': 4.620418848167539e-05, 'epoch': 1.18}
 40%|███▉      | 230/582 [04:06<06:10,  1.05s/it] 40%|███▉      | 231/582 [04:07<06:04,  1.04s/it] 40%|███▉      | 232/582 [04:08<05:59,  1.03s/it] 40%|████      | 233/582 [04:09<05:58,  1.03s/it] 40%|████      | 234/582 [04:10<06:05,  1.05s/it] 40%|████      | 235/582 [04:11<06:04,  1.05s/it] 41%|████      | 236/582 [04:12<06:04,  1.05s/it] 41%|████      | 237/582 [04:13<05:57,  1.04s/it] 41%|████      | 238/582 [04:14<05:56,  1.04s/it] 41%|████      | 239/582 [04:15<06:04,  1.06s/it] 41%|████      | 240/582 [04:16<06:07,  1.07s/it]                                                 {'loss': 1.0894, 'grad_norm': 1.0573819875717163, 'learning_rate': 4.489528795811519e-05, 'epoch': 1.24}
 41%|████      | 240/582 [04:16<06:07,  1.07s/it] 41%|████▏     | 241/582 [04:18<06:05,  1.07s/it] 42%|████▏     | 242/582 [04:19<06:07,  1.08s/it] 42%|████▏     | 243/582 [04:20<06:12,  1.10s/it] 42%|████▏     | 244/582 [04:21<06:05,  1.08s/it] 42%|████▏     | 245/582 [04:22<06:00,  1.07s/it] 42%|████▏     | 246/582 [04:23<05:52,  1.05s/it] 42%|████▏     | 247/582 [04:24<05:47,  1.04s/it] 43%|████▎     | 248/582 [04:25<05:50,  1.05s/it] 43%|████▎     | 249/582 [04:26<05:46,  1.04s/it] 43%|████▎     | 250/582 [04:27<05:44,  1.04s/it]                                                 {'loss': 1.147, 'grad_norm': 1.193902850151062, 'learning_rate': 4.358638743455498e-05, 'epoch': 1.29}
 43%|████▎     | 250/582 [04:27<05:44,  1.04s/it] 43%|████▎     | 251/582 [04:28<05:47,  1.05s/it] 43%|████▎     | 252/582 [04:29<05:47,  1.05s/it] 43%|████▎     | 253/582 [04:30<05:44,  1.05s/it] 44%|████▎     | 254/582 [04:31<05:39,  1.03s/it] 44%|████▍     | 255/582 [04:32<05:35,  1.03s/it] 44%|████▍     | 256/582 [04:33<05:39,  1.04s/it] 44%|████▍     | 257/582 [04:34<05:36,  1.04s/it] 44%|████▍     | 258/582 [04:35<05:35,  1.04s/it] 45%|████▍     | 259/582 [04:36<05:35,  1.04s/it] 45%|████▍     | 260/582 [04:37<05:31,  1.03s/it]                                                 {'loss': 1.0563, 'grad_norm': 0.8463789224624634, 'learning_rate': 4.227748691099477e-05, 'epoch': 1.34}
 45%|████▍     | 260/582 [04:37<05:31,  1.03s/it] 45%|████▍     | 261/582 [04:38<05:29,  1.03s/it] 45%|████▌     | 262/582 [04:39<05:28,  1.03s/it] 45%|████▌     | 263/582 [04:40<05:25,  1.02s/it] 45%|████▌     | 264/582 [04:41<05:22,  1.01s/it] 46%|████▌     | 265/582 [04:43<05:33,  1.05s/it] 46%|████▌     | 266/582 [04:44<05:46,  1.10s/it] 46%|████▌     | 267/582 [04:45<05:40,  1.08s/it] 46%|████▌     | 268/582 [04:46<05:34,  1.07s/it] 46%|████▌     | 269/582 [04:47<05:29,  1.05s/it] 46%|████▋     | 270/582 [04:48<05:31,  1.06s/it]                                                 {'loss': 1.1132, 'grad_norm': 0.8393463492393494, 'learning_rate': 4.096858638743455e-05, 'epoch': 1.39}
 46%|████▋     | 270/582 [04:48<05:31,  1.06s/it] 47%|████▋     | 271/582 [04:49<05:25,  1.05s/it] 47%|████▋     | 272/582 [04:50<05:22,  1.04s/it] 47%|████▋     | 273/582 [04:51<05:17,  1.03s/it] 47%|████▋     | 274/582 [04:52<05:14,  1.02s/it] 47%|████▋     | 275/582 [04:53<05:22,  1.05s/it] 47%|████▋     | 276/582 [04:54<05:18,  1.04s/it] 48%|████▊     | 277/582 [04:55<05:15,  1.03s/it] 48%|████▊     | 278/582 [04:56<05:11,  1.02s/it] 48%|████▊     | 279/582 [04:57<05:11,  1.03s/it] 48%|████▊     | 280/582 [04:58<05:08,  1.02s/it]                                                 {'loss': 1.1323, 'grad_norm': 0.9864556789398193, 'learning_rate': 3.965968586387435e-05, 'epoch': 1.44}
 48%|████▊     | 280/582 [04:58<05:08,  1.02s/it] 48%|████▊     | 281/582 [04:59<05:06,  1.02s/it] 48%|████▊     | 282/582 [05:00<05:05,  1.02s/it] 49%|████▊     | 283/582 [05:01<05:05,  1.02s/it] 49%|████▉     | 284/582 [05:02<05:04,  1.02s/it] 49%|████▉     | 285/582 [05:03<05:08,  1.04s/it] 49%|████▉     | 286/582 [05:04<05:08,  1.04s/it] 49%|████▉     | 287/582 [05:05<05:08,  1.05s/it] 49%|████▉     | 288/582 [05:06<05:04,  1.03s/it] 50%|████▉     | 289/582 [05:08<05:11,  1.06s/it] 50%|████▉     | 290/582 [05:09<05:11,  1.07s/it]                                                 {'loss': 1.1002, 'grad_norm': 1.2331434488296509, 'learning_rate': 3.835078534031414e-05, 'epoch': 1.49}
 50%|████▉     | 290/582 [05:09<05:11,  1.07s/it] 50%|█████     | 291/582 [05:10<05:06,  1.05s/it] 50%|█████     | 292/582 [05:11<05:04,  1.05s/it] 50%|█████     | 293/582 [05:12<04:59,  1.04s/it] 51%|█████     | 294/582 [05:13<05:03,  1.06s/it] 51%|█████     | 295/582 [05:14<05:01,  1.05s/it] 51%|█████     | 296/582 [05:15<04:57,  1.04s/it] 51%|█████     | 297/582 [05:16<04:55,  1.04s/it] 51%|█████     | 298/582 [05:17<04:53,  1.03s/it] 51%|█████▏    | 299/582 [05:18<04:54,  1.04s/it] 52%|█████▏    | 300/582 [05:19<04:53,  1.04s/it]                                                 {'loss': 1.0596, 'grad_norm': 0.9084653258323669, 'learning_rate': 3.704188481675393e-05, 'epoch': 1.54}
 52%|█████▏    | 300/582 [05:19<04:53,  1.04s/it] 52%|█████▏    | 301/582 [05:20<04:53,  1.04s/it] 52%|█████▏    | 302/582 [05:21<04:51,  1.04s/it] 52%|█████▏    | 303/582 [05:22<04:50,  1.04s/it] 52%|█████▏    | 304/582 [05:23<04:50,  1.05s/it] 52%|█████▏    | 305/582 [05:24<04:56,  1.07s/it] 53%|█████▎    | 306/582 [05:25<05:00,  1.09s/it] 53%|█████▎    | 307/582 [05:27<04:57,  1.08s/it] 53%|█████▎    | 308/582 [05:28<04:53,  1.07s/it] 53%|█████▎    | 309/582 [05:29<04:57,  1.09s/it] 53%|█████▎    | 310/582 [05:30<04:50,  1.07s/it]                                                 {'loss': 1.0904, 'grad_norm': 1.1076481342315674, 'learning_rate': 3.5732984293193713e-05, 'epoch': 1.6}
 53%|█████▎    | 310/582 [05:30<04:50,  1.07s/it] 53%|█████▎    | 311/582 [05:31<04:45,  1.05s/it] 54%|█████▎    | 312/582 [05:32<05:04,  1.13s/it] 54%|█████▍    | 313/582 [05:33<04:56,  1.10s/it] 54%|█████▍    | 314/582 [05:34<04:50,  1.09s/it] 54%|█████▍    | 315/582 [05:35<04:47,  1.08s/it] 54%|█████▍    | 316/582 [05:36<04:40,  1.06s/it] 54%|█████▍    | 317/582 [05:37<04:38,  1.05s/it] 55%|█████▍    | 318/582 [05:38<04:36,  1.05s/it] 55%|█████▍    | 319/582 [05:39<04:32,  1.04s/it] 55%|█████▍    | 320/582 [05:40<04:40,  1.07s/it]                                                 {'loss': 1.122, 'grad_norm': 1.4325963258743286, 'learning_rate': 3.442408376963351e-05, 'epoch': 1.65}
 55%|█████▍    | 320/582 [05:40<04:40,  1.07s/it] 55%|█████▌    | 321/582 [05:42<04:36,  1.06s/it] 55%|█████▌    | 322/582 [05:43<04:34,  1.06s/it] 55%|█████▌    | 323/582 [05:44<04:30,  1.05s/it] 56%|█████▌    | 324/582 [05:45<04:27,  1.04s/it] 56%|█████▌    | 325/582 [05:46<04:28,  1.05s/it] 56%|█████▌    | 326/582 [05:47<04:41,  1.10s/it] 56%|█████▌    | 327/582 [05:48<04:36,  1.08s/it] 56%|█████▋    | 328/582 [05:49<04:29,  1.06s/it] 57%|█████▋    | 329/582 [05:50<04:27,  1.06s/it] 57%|█████▋    | 330/582 [05:51<04:28,  1.06s/it]                                                 {'loss': 1.1283, 'grad_norm': 0.880547285079956, 'learning_rate': 3.31151832460733e-05, 'epoch': 1.7}
 57%|█████▋    | 330/582 [05:51<04:28,  1.06s/it] 57%|█████▋    | 331/582 [05:52<04:22,  1.05s/it] 57%|█████▋    | 332/582 [05:53<04:19,  1.04s/it] 57%|█████▋    | 333/582 [05:54<04:17,  1.03s/it] 57%|█████▋    | 334/582 [05:55<04:14,  1.03s/it] 58%|█████▊    | 335/582 [05:56<04:15,  1.03s/it] 58%|█████▊    | 336/582 [05:57<04:12,  1.03s/it] 58%|█████▊    | 337/582 [05:58<04:11,  1.03s/it] 58%|█████▊    | 338/582 [05:59<04:11,  1.03s/it] 58%|█████▊    | 339/582 [06:00<04:14,  1.05s/it] 58%|█████▊    | 340/582 [06:01<04:10,  1.04s/it]                                                 {'loss': 1.0515, 'grad_norm': 1.2048131227493286, 'learning_rate': 3.180628272251309e-05, 'epoch': 1.75}
 58%|█████▊    | 340/582 [06:01<04:10,  1.04s/it] 59%|█████▊    | 341/582 [06:02<04:17,  1.07s/it] 59%|█████▉    | 342/582 [06:04<04:13,  1.06s/it] 59%|█████▉    | 343/582 [06:05<04:18,  1.08s/it] 59%|█████▉    | 344/582 [06:06<04:12,  1.06s/it] 59%|█████▉    | 345/582 [06:07<04:14,  1.07s/it] 59%|█████▉    | 346/582 [06:08<04:12,  1.07s/it] 60%|█████▉    | 347/582 [06:09<04:08,  1.06s/it] 60%|█████▉    | 348/582 [06:10<04:08,  1.06s/it] 60%|█████▉    | 349/582 [06:11<04:03,  1.05s/it] 60%|██████    | 350/582 [06:12<04:00,  1.04s/it]                                                 {'loss': 1.0896, 'grad_norm': 0.9334722757339478, 'learning_rate': 3.049738219895288e-05, 'epoch': 1.8}
 60%|██████    | 350/582 [06:12<04:00,  1.04s/it] 60%|██████    | 351/582 [06:13<04:01,  1.05s/it] 60%|██████    | 352/582 [06:14<04:12,  1.10s/it] 61%|██████    | 353/582 [06:15<04:04,  1.07s/it] 61%|██████    | 354/582 [06:16<04:00,  1.06s/it] 61%|██████    | 355/582 [06:17<04:07,  1.09s/it] 61%|██████    | 356/582 [06:18<04:02,  1.07s/it] 61%|██████▏   | 357/582 [06:19<03:57,  1.05s/it] 62%|██████▏   | 358/582 [06:21<03:55,  1.05s/it] 62%|██████▏   | 359/582 [06:22<03:51,  1.04s/it] 62%|██████▏   | 360/582 [06:23<03:48,  1.03s/it]                                                 {'loss': 1.0634, 'grad_norm': 0.9789149761199951, 'learning_rate': 2.918848167539267e-05, 'epoch': 1.85}
 62%|██████▏   | 360/582 [06:23<03:48,  1.03s/it] 62%|██████▏   | 361/582 [06:24<03:48,  1.03s/it] 62%|██████▏   | 362/582 [06:25<03:48,  1.04s/it] 62%|██████▏   | 363/582 [06:26<03:47,  1.04s/it] 63%|██████▎   | 364/582 [06:27<03:52,  1.06s/it] 63%|██████▎   | 365/582 [06:28<03:49,  1.06s/it] 63%|██████▎   | 366/582 [06:29<03:51,  1.07s/it] 63%|██████▎   | 367/582 [06:30<03:49,  1.07s/it] 63%|██████▎   | 368/582 [06:31<04:06,  1.15s/it] 63%|██████▎   | 369/582 [06:33<04:05,  1.15s/it] 64%|██████▎   | 370/582 [06:34<03:57,  1.12s/it]                                                 {'loss': 1.0719, 'grad_norm': 2.3775691986083984, 'learning_rate': 2.7879581151832463e-05, 'epoch': 1.9}
 64%|██████▎   | 370/582 [06:34<03:57,  1.12s/it] 64%|██████▎   | 371/582 [06:35<03:52,  1.10s/it] 64%|██████▍   | 372/582 [06:36<03:47,  1.08s/it] 64%|██████▍   | 373/582 [06:37<03:40,  1.06s/it] 64%|██████▍   | 374/582 [06:38<03:36,  1.04s/it] 64%|██████▍   | 375/582 [06:39<03:33,  1.03s/it] 65%|██████▍   | 376/582 [06:40<03:33,  1.04s/it] 65%|██████▍   | 377/582 [06:41<03:38,  1.06s/it] 65%|██████▍   | 378/582 [06:42<03:36,  1.06s/it] 65%|██████▌   | 379/582 [06:43<03:32,  1.05s/it] 65%|██████▌   | 380/582 [06:44<03:30,  1.04s/it]                                                 {'loss': 1.0905, 'grad_norm': 1.1712769269943237, 'learning_rate': 2.6570680628272253e-05, 'epoch': 1.96}
 65%|██████▌   | 380/582 [06:44<03:30,  1.04s/it] 65%|██████▌   | 381/582 [06:45<03:34,  1.07s/it] 66%|██████▌   | 382/582 [06:46<03:30,  1.05s/it] 66%|██████▌   | 383/582 [06:47<03:26,  1.04s/it] 66%|██████▌   | 384/582 [06:48<03:26,  1.04s/it] 66%|██████▌   | 385/582 [06:49<03:26,  1.05s/it] 66%|██████▋   | 386/582 [06:50<03:24,  1.04s/it] 66%|██████▋   | 387/582 [06:51<03:21,  1.03s/it] 67%|██████▋   | 388/582 [06:52<03:19,  1.03s/it] 67%|██████▋   | 389/582 [06:53<03:18,  1.03s/it] 67%|██████▋   | 390/582 [06:54<03:17,  1.03s/it]                                                 {'loss': 1.0799, 'grad_norm': 1.087254524230957, 'learning_rate': 2.526178010471204e-05, 'epoch': 2.01}
 67%|██████▋   | 390/582 [06:54<03:17,  1.03s/it] 67%|██████▋   | 391/582 [06:55<03:15,  1.03s/it] 67%|██████▋   | 392/582 [06:56<03:13,  1.02s/it] 68%|██████▊   | 393/582 [06:57<03:12,  1.02s/it] 68%|██████▊   | 394/582 [06:59<03:19,  1.06s/it] 68%|██████▊   | 395/582 [07:00<03:15,  1.05s/it] 68%|██████▊   | 396/582 [07:01<03:14,  1.04s/it] 68%|██████▊   | 397/582 [07:02<03:13,  1.05s/it] 68%|██████▊   | 398/582 [07:03<03:15,  1.06s/it] 69%|██████▊   | 399/582 [07:04<03:14,  1.06s/it] 69%|██████▊   | 400/582 [07:05<03:17,  1.09s/it]                                                 {'loss': 1.0667, 'grad_norm': 1.3357834815979004, 'learning_rate': 2.395287958115183e-05, 'epoch': 2.06}
 69%|██████▊   | 400/582 [07:05<03:17,  1.09s/it] 69%|██████▉   | 401/582 [07:06<03:14,  1.07s/it] 69%|██████▉   | 402/582 [07:07<03:09,  1.05s/it] 69%|██████▉   | 403/582 [07:08<03:08,  1.05s/it] 69%|██████▉   | 404/582 [07:09<03:13,  1.09s/it] 70%|██████▉   | 405/582 [07:10<03:09,  1.07s/it] 70%|██████▉   | 406/582 [07:11<03:04,  1.05s/it] 70%|██████▉   | 407/582 [07:12<03:11,  1.10s/it] 70%|███████   | 408/582 [07:13<03:06,  1.07s/it] 70%|███████   | 409/582 [07:15<03:05,  1.07s/it] 70%|███████   | 410/582 [07:16<03:01,  1.06s/it]                                                 {'loss': 1.0555, 'grad_norm': 1.0571874380111694, 'learning_rate': 2.2643979057591625e-05, 'epoch': 2.11}
 70%|███████   | 410/582 [07:16<03:01,  1.06s/it] 71%|███████   | 411/582 [07:17<02:59,  1.05s/it] 71%|███████   | 412/582 [07:18<02:58,  1.05s/it] 71%|███████   | 413/582 [07:19<02:57,  1.05s/it] 71%|███████   | 414/582 [07:20<02:54,  1.04s/it] 71%|███████▏  | 415/582 [07:21<02:52,  1.03s/it] 71%|███████▏  | 416/582 [07:22<02:50,  1.03s/it] 72%|███████▏  | 417/582 [07:23<02:48,  1.02s/it] 72%|███████▏  | 418/582 [07:24<02:49,  1.04s/it] 72%|███████▏  | 419/582 [07:25<02:50,  1.05s/it] 72%|███████▏  | 420/582 [07:26<02:50,  1.05s/it]                                                 {'loss': 1.0232, 'grad_norm': 0.9341017603874207, 'learning_rate': 2.1335078534031415e-05, 'epoch': 2.16}
 72%|███████▏  | 420/582 [07:26<02:50,  1.05s/it] 72%|███████▏  | 421/582 [07:27<02:49,  1.05s/it] 73%|███████▎  | 422/582 [07:28<02:49,  1.06s/it] 73%|███████▎  | 423/582 [07:29<02:49,  1.07s/it] 73%|███████▎  | 424/582 [07:30<02:50,  1.08s/it] 73%|███████▎  | 425/582 [07:31<02:47,  1.07s/it] 73%|███████▎  | 426/582 [07:32<02:46,  1.07s/it] 73%|███████▎  | 427/582 [07:33<02:46,  1.07s/it] 74%|███████▎  | 428/582 [07:35<02:44,  1.07s/it] 74%|███████▎  | 429/582 [07:36<02:49,  1.11s/it] 74%|███████▍  | 430/582 [07:37<02:46,  1.10s/it]                                                 {'loss': 1.046, 'grad_norm': 1.212216854095459, 'learning_rate': 2.0026178010471206e-05, 'epoch': 2.21}
 74%|███████▍  | 430/582 [07:37<02:46,  1.10s/it] 74%|███████▍  | 431/582 [07:38<02:43,  1.09s/it] 74%|███████▍  | 432/582 [07:39<02:42,  1.09s/it] 74%|███████▍  | 433/582 [07:40<02:42,  1.09s/it] 75%|███████▍  | 434/582 [07:41<02:44,  1.11s/it] 75%|███████▍  | 435/582 [07:42<02:41,  1.10s/it] 75%|███████▍  | 436/582 [07:43<02:40,  1.10s/it] 75%|███████▌  | 437/582 [07:44<02:37,  1.09s/it] 75%|███████▌  | 438/582 [07:46<02:37,  1.09s/it] 75%|███████▌  | 439/582 [07:47<02:35,  1.09s/it] 76%|███████▌  | 440/582 [07:48<02:40,  1.13s/it]                                                 {'loss': 1.0345, 'grad_norm': 1.1571916341781616, 'learning_rate': 1.8717277486910996e-05, 'epoch': 2.27}
 76%|███████▌  | 440/582 [07:48<02:40,  1.13s/it] 76%|███████▌  | 441/582 [07:49<02:38,  1.13s/it] 76%|███████▌  | 442/582 [07:50<02:36,  1.12s/it] 76%|███████▌  | 443/582 [07:51<02:34,  1.11s/it] 76%|███████▋  | 444/582 [07:52<02:31,  1.10s/it] 76%|███████▋  | 445/582 [07:53<02:29,  1.09s/it] 77%|███████▋  | 446/582 [07:54<02:29,  1.10s/it] 77%|███████▋  | 447/582 [07:55<02:26,  1.09s/it] 77%|███████▋  | 448/582 [07:57<02:25,  1.09s/it] 77%|███████▋  | 449/582 [07:58<02:23,  1.08s/it] 77%|███████▋  | 450/582 [07:59<02:22,  1.08s/it]                                                 {'loss': 1.0565, 'grad_norm': 0.9805305600166321, 'learning_rate': 1.7408376963350786e-05, 'epoch': 2.32}
 77%|███████▋  | 450/582 [07:59<02:22,  1.08s/it] 77%|███████▋  | 451/582 [08:00<02:21,  1.08s/it] 78%|███████▊  | 452/582 [08:01<02:24,  1.11s/it] 78%|███████▊  | 453/582 [08:02<02:21,  1.10s/it] 78%|███████▊  | 454/582 [08:03<02:19,  1.09s/it] 78%|███████▊  | 455/582 [08:04<02:18,  1.09s/it] 78%|███████▊  | 456/582 [08:05<02:17,  1.09s/it] 79%|███████▊  | 457/582 [08:06<02:16,  1.09s/it] 79%|███████▊  | 458/582 [08:07<02:14,  1.08s/it] 79%|███████▉  | 459/582 [08:08<02:12,  1.08s/it] 79%|███████▉  | 460/582 [08:10<02:21,  1.16s/it]                                                 {'loss': 1.0397, 'grad_norm': 0.8119381666183472, 'learning_rate': 1.6099476439790577e-05, 'epoch': 2.37}
 79%|███████▉  | 460/582 [08:10<02:21,  1.16s/it] 79%|███████▉  | 461/582 [08:11<02:16,  1.13s/it] 79%|███████▉  | 462/582 [08:12<02:13,  1.11s/it] 80%|███████▉  | 463/582 [08:13<02:11,  1.11s/it] 80%|███████▉  | 464/582 [08:14<02:12,  1.13s/it] 80%|███████▉  | 465/582 [08:15<02:13,  1.14s/it] 80%|████████  | 466/582 [08:17<02:17,  1.18s/it] 80%|████████  | 467/582 [08:18<02:14,  1.17s/it] 80%|████████  | 468/582 [08:19<02:12,  1.16s/it] 81%|████████  | 469/582 [08:20<02:10,  1.16s/it] 81%|████████  | 470/582 [08:21<02:09,  1.15s/it]                                                 {'loss': 1.0436, 'grad_norm': 0.9790350794792175, 'learning_rate': 1.4790575916230367e-05, 'epoch': 2.42}
 81%|████████  | 470/582 [08:21<02:09,  1.15s/it] 81%|████████  | 471/582 [08:22<02:08,  1.15s/it] 81%|████████  | 472/582 [08:24<02:06,  1.15s/it] 81%|████████▏ | 473/582 [08:25<02:05,  1.15s/it] 81%|████████▏ | 474/582 [08:26<02:05,  1.16s/it] 82%|████████▏ | 475/582 [08:27<02:03,  1.15s/it] 82%|████████▏ | 476/582 [08:28<02:02,  1.15s/it] 82%|████████▏ | 477/582 [08:29<02:00,  1.15s/it] 82%|████████▏ | 478/582 [08:30<01:59,  1.15s/it] 82%|████████▏ | 479/582 [08:32<01:58,  1.15s/it] 82%|████████▏ | 480/582 [08:33<01:57,  1.15s/it]                                                 {'loss': 1.0716, 'grad_norm': 1.1378471851348877, 'learning_rate': 1.3481675392670156e-05, 'epoch': 2.47}
 82%|████████▏ | 480/582 [08:33<01:57,  1.15s/it] 83%|████████▎ | 481/582 [08:34<01:58,  1.17s/it] 83%|████████▎ | 482/582 [08:35<01:56,  1.17s/it] 83%|████████▎ | 483/582 [08:36<01:54,  1.16s/it] 83%|████████▎ | 484/582 [08:37<01:53,  1.16s/it] 83%|████████▎ | 485/582 [08:39<01:51,  1.15s/it] 84%|████████▎ | 486/582 [08:40<01:50,  1.15s/it] 84%|████████▎ | 487/582 [08:41<01:49,  1.15s/it] 84%|████████▍ | 488/582 [08:42<01:47,  1.15s/it] 84%|████████▍ | 489/582 [08:43<01:47,  1.15s/it] 84%|████████▍ | 490/582 [08:44<01:45,  1.14s/it]                                                 {'loss': 1.0137, 'grad_norm': 1.82699453830719, 'learning_rate': 1.2172774869109948e-05, 'epoch': 2.52}
 84%|████████▍ | 490/582 [08:44<01:45,  1.14s/it] 84%|████████▍ | 491/582 [08:45<01:44,  1.14s/it] 85%|████████▍ | 492/582 [08:47<01:43,  1.15s/it] 85%|████████▍ | 493/582 [08:48<01:42,  1.16s/it] 85%|████████▍ | 494/582 [08:49<01:41,  1.15s/it] 85%|████████▌ | 495/582 [08:50<01:40,  1.15s/it] 85%|████████▌ | 496/582 [08:51<01:39,  1.15s/it] 85%|████████▌ | 497/582 [08:52<01:35,  1.12s/it] 86%|████████▌ | 498/582 [08:53<01:32,  1.10s/it] 86%|████████▌ | 499/582 [08:54<01:29,  1.08s/it] 86%|████████▌ | 500/582 [08:55<01:27,  1.07s/it]                                                 {'loss': 1.0776, 'grad_norm': 1.0634372234344482, 'learning_rate': 1.0863874345549739e-05, 'epoch': 2.57}
 86%|████████▌ | 500/582 [08:55<01:27,  1.07s/it] 86%|████████▌ | 501/582 [08:56<01:25,  1.06s/it] 86%|████████▋ | 502/582 [08:58<01:30,  1.14s/it] 86%|████████▋ | 503/582 [08:59<01:28,  1.12s/it] 87%|████████▋ | 504/582 [09:00<01:24,  1.09s/it] 87%|████████▋ | 505/582 [09:01<01:22,  1.07s/it] 87%|████████▋ | 506/582 [09:02<01:20,  1.06s/it] 87%|████████▋ | 507/582 [09:03<01:18,  1.05s/it] 87%|████████▋ | 508/582 [09:04<01:16,  1.04s/it] 87%|████████▋ | 509/582 [09:05<01:15,  1.04s/it] 88%|████████▊ | 510/582 [09:06<01:15,  1.04s/it]                                                 {'loss': 1.0478, 'grad_norm': 0.9408934712409973, 'learning_rate': 9.554973821989529e-06, 'epoch': 2.63}
 88%|████████▊ | 510/582 [09:06<01:15,  1.04s/it] 88%|████████▊ | 511/582 [09:07<01:14,  1.05s/it] 88%|████████▊ | 512/582 [09:08<01:12,  1.04s/it] 88%|████████▊ | 513/582 [09:09<01:11,  1.03s/it] 88%|████████▊ | 514/582 [09:10<01:09,  1.03s/it] 88%|████████▊ | 515/582 [09:11<01:08,  1.02s/it] 89%|████████▊ | 516/582 [09:12<01:07,  1.02s/it] 89%|████████▉ | 517/582 [09:13<01:08,  1.05s/it] 89%|████████▉ | 518/582 [09:14<01:07,  1.05s/it] 89%|████████▉ | 519/582 [09:15<01:05,  1.05s/it] 89%|████████▉ | 520/582 [09:16<01:04,  1.03s/it]                                                 {'loss': 1.0196, 'grad_norm': 1.0588188171386719, 'learning_rate': 8.24607329842932e-06, 'epoch': 2.68}
 89%|████████▉ | 520/582 [09:16<01:04,  1.03s/it] 90%|████████▉ | 521/582 [09:17<01:02,  1.03s/it] 90%|████████▉ | 522/582 [09:18<01:02,  1.04s/it] 90%|████████▉ | 523/582 [09:19<01:00,  1.03s/it] 90%|█████████ | 524/582 [09:21<00:59,  1.02s/it] 90%|█████████ | 525/582 [09:22<00:58,  1.02s/it] 90%|█████████ | 526/582 [09:23<01:00,  1.08s/it] 91%|█████████ | 527/582 [09:24<01:00,  1.09s/it] 91%|█████████ | 528/582 [09:25<00:57,  1.06s/it] 91%|█████████ | 529/582 [09:26<00:55,  1.06s/it] 91%|█████████ | 530/582 [09:27<00:54,  1.06s/it]                                                 {'loss': 1.0463, 'grad_norm': 2.7851791381835938, 'learning_rate': 6.937172774869111e-06, 'epoch': 2.73}
 91%|█████████ | 530/582 [09:27<00:54,  1.06s/it] 91%|█████████ | 531/582 [09:28<00:53,  1.04s/it] 91%|█████████▏| 532/582 [09:29<00:51,  1.04s/it] 92%|█████████▏| 533/582 [09:30<00:50,  1.03s/it] 92%|█████████▏| 534/582 [09:31<00:49,  1.03s/it] 92%|█████████▏| 535/582 [09:32<00:47,  1.02s/it] 92%|█████████▏| 536/582 [09:33<00:48,  1.06s/it] 92%|█████████▏| 537/582 [09:34<00:47,  1.07s/it] 92%|█████████▏| 538/582 [09:35<00:46,  1.06s/it] 93%|█████████▎| 539/582 [09:36<00:46,  1.07s/it] 93%|█████████▎| 540/582 [09:37<00:44,  1.07s/it]                                                 {'loss': 1.0594, 'grad_norm': 1.0554307699203491, 'learning_rate': 5.6282722513089e-06, 'epoch': 2.78}
 93%|█████████▎| 540/582 [09:37<00:44,  1.07s/it] 93%|█████████▎| 541/582 [09:39<00:43,  1.07s/it] 93%|█████████▎| 542/582 [09:40<00:42,  1.07s/it] 93%|█████████▎| 543/582 [09:41<00:41,  1.07s/it] 93%|█████████▎| 544/582 [09:42<00:40,  1.07s/it] 94%|█████████▎| 545/582 [09:43<00:39,  1.07s/it] 94%|█████████▍| 546/582 [09:44<00:37,  1.05s/it] 94%|█████████▍| 547/582 [09:45<00:37,  1.06s/it] 94%|█████████▍| 548/582 [09:46<00:35,  1.05s/it] 94%|█████████▍| 549/582 [09:47<00:34,  1.05s/it] 95%|█████████▍| 550/582 [09:48<00:33,  1.06s/it]                                                 {'loss': 1.0292, 'grad_norm': 2.0560476779937744, 'learning_rate': 4.319371727748691e-06, 'epoch': 2.83}
 95%|█████████▍| 550/582 [09:48<00:33,  1.06s/it] 95%|█████████▍| 551/582 [09:49<00:33,  1.08s/it] 95%|█████████▍| 552/582 [09:50<00:33,  1.10s/it] 95%|█████████▌| 553/582 [09:51<00:32,  1.12s/it] 95%|█████████▌| 554/582 [09:53<00:31,  1.13s/it] 95%|█████████▌| 555/582 [09:54<00:30,  1.14s/it] 96%|█████████▌| 556/582 [09:55<00:29,  1.14s/it] 96%|█████████▌| 557/582 [09:56<00:28,  1.15s/it] 96%|█████████▌| 558/582 [09:57<00:27,  1.14s/it] 96%|█████████▌| 559/582 [09:58<00:25,  1.11s/it] 96%|█████████▌| 560/582 [09:59<00:23,  1.09s/it]                                                 {'loss': 1.068, 'grad_norm': 0.8762631416320801, 'learning_rate': 3.0104712041884817e-06, 'epoch': 2.88}
 96%|█████████▌| 560/582 [09:59<00:23,  1.09s/it] 96%|█████████▋| 561/582 [10:00<00:22,  1.07s/it] 97%|█████████▋| 562/582 [10:01<00:21,  1.07s/it] 97%|█████████▋| 563/582 [10:02<00:20,  1.07s/it] 97%|█████████▋| 564/582 [10:03<00:18,  1.05s/it] 97%|█████████▋| 565/582 [10:05<00:17,  1.04s/it] 97%|█████████▋| 566/582 [10:06<00:16,  1.04s/it] 97%|█████████▋| 567/582 [10:07<00:16,  1.08s/it] 98%|█████████▊| 568/582 [10:08<00:14,  1.06s/it] 98%|█████████▊| 569/582 [10:09<00:14,  1.08s/it] 98%|█████████▊| 570/582 [10:10<00:13,  1.08s/it]                                                 {'loss': 1.0853, 'grad_norm': 1.643216609954834, 'learning_rate': 1.7015706806282724e-06, 'epoch': 2.93}
 98%|█████████▊| 570/582 [10:10<00:13,  1.08s/it] 98%|█████████▊| 571/582 [10:11<00:11,  1.08s/it] 98%|█████████▊| 572/582 [10:12<00:10,  1.08s/it] 98%|█████████▊| 573/582 [10:13<00:09,  1.06s/it] 99%|█████████▊| 574/582 [10:14<00:08,  1.06s/it] 99%|█████████▉| 575/582 [10:15<00:07,  1.06s/it] 99%|█████████▉| 576/582 [10:16<00:06,  1.05s/it] 99%|█████████▉| 577/582 [10:17<00:05,  1.06s/it] 99%|█████████▉| 578/582 [10:18<00:04,  1.05s/it] 99%|█████████▉| 579/582 [10:19<00:03,  1.05s/it]100%|█████████▉| 580/582 [10:20<00:02,  1.04s/it]                                                 {'loss': 1.0209, 'grad_norm': 1.297442078590393, 'learning_rate': 3.926701570680628e-07, 'epoch': 2.99}
100%|█████████▉| 580/582 [10:20<00:02,  1.04s/it]100%|█████████▉| 581/582 [10:21<00:01,  1.03s/it]100%|██████████| 582/582 [10:22<00:00,  1.03s/it][INFO|trainer.py:2231] 2024-05-26 13:06:28,767 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 627.8127, 'train_samples_per_second': 14.837, 'train_steps_per_second': 0.927, 'train_loss': 1.2176850534386652, 'epoch': 3.0}
100%|██████████| 582/582 [10:22<00:00,  1.03s/it]100%|██████████| 582/582 [10:22<00:00,  1.07s/it]
[INFO|trainer.py:3203] 2024-05-26 13:06:28,779 >> Saving model checkpoint to sft_checkpoint/llama2-7b-multi_role_train
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/student2021/srcao/base-model/llama2-7b - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-05-26 13:06:28,837 >> tokenizer config file saved in sft_checkpoint/llama2-7b-multi_role_train/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-26 13:06:28,838 >> Special tokens file saved in sft_checkpoint/llama2-7b-multi_role_train/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     1.2177
  train_runtime            = 0:10:27.81
  train_samples_per_second =     14.837
  train_steps_per_second   =      0.927
Figure saved: sft_checkpoint/llama2-7b-multi_role_train/training_loss.png
05/26/2024 13:06:28 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:450] 2024-05-26 13:06:28,958 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-05-26 13:06:30,403] [INFO] [launch.py:348:main] Process 25415 exits successfully.
[2024-05-26 13:06:30,403] [INFO] [launch.py:348:main] Process 25413 exits successfully.
[2024-05-26 13:06:30,404] [INFO] [launch.py:348:main] Process 25414 exits successfully.
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.027 MB uploadedwandb: | 0.024 MB of 0.027 MB uploadedwandb: / 0.024 MB of 0.027 MB uploadedwandb: - 0.024 MB of 0.027 MB uploadedwandb: \ 0.024 MB of 0.027 MB uploadedwandb: | 0.024 MB of 0.027 MB uploadedwandb: / 0.024 MB of 0.027 MB uploadedwandb: - 0.024 MB of 0.027 MB uploadedwandb: \ 0.024 MB of 0.027 MB uploadedwandb: | 0.024 MB of 0.027 MB uploadedwandb: / 0.024 MB of 0.027 MB uploadedwandb: - 0.024 MB of 0.027 MB uploadedwandb: \ 0.024 MB of 0.027 MB uploadedwandb: | 0.024 MB of 0.027 MB uploadedwandb: / 0.024 MB of 0.027 MB uploadedwandb: - 0.024 MB of 0.027 MB uploadedwandb: \ 0.024 MB of 0.027 MB uploadedwandb: | 0.024 MB of 0.027 MB uploadedwandb: / 0.024 MB of 0.027 MB uploadedwandb: - 0.024 MB of 0.027 MB uploadedwandb: \ 0.024 MB of 0.027 MB uploadedwandb: | 0.024 MB of 0.027 MB uploadedwandb: / 0.027 MB of 0.027 MB uploadedwandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███
wandb:   train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███
wandb:     train/grad_norm ▁▁▁▂▂▂▂▂▂▃▂▂▂▄▂▃▃▄▃▃▃▃▃▄▃▇▃▄▃▄▃▃▃▅▃▃█▆▃▄
wandb: train/learning_rate ▁▂▂▃▃▄▄▅▅▆▆▇▇███▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁
wandb:          train/loss █▇▇▇▇▆▆▄▄▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               total_flos 6.430540461139558e+16
wandb:              train/epoch 3.0
wandb:        train/global_step 582
wandb:          train/grad_norm 1.29744
wandb:      train/learning_rate 0.0
wandb:               train/loss 1.0209
wandb:               train_loss 1.21769
wandb:            train_runtime 627.8127
wandb: train_samples_per_second 14.837
wandb:   train_steps_per_second 0.927
wandb: 
wandb: 🚀 View run solar-universe-193 at: https://wandb.ai/srcao-bingo/huggingface/runs/eveuy2pn/workspace
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240526_125602-eveuy2pn/logs
[2024-05-26 13:07:00,437] [INFO] [launch.py:348:main] Process 25412 exits successfully.
[2024-05-26 13:28:22,007] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
05/26/2024 13:28:26 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1934] 2024-05-26 13:28:26,887 >> PyTorch: setting up devices
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/26/2024 13:28:26 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/26/2024 13:28:26 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=True,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=predict_res/sft-llama2-7b-multi_role_test/runs/May26_13-28-26_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=predict_res/sft-llama2-7b-multi_role_test,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=8,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=predict_res/sft-llama2-7b-multi_role_test,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2082] 2024-05-26 13:28:26,889 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2082] 2024-05-26 13:28:26,889 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2082] 2024-05-26 13:28:26,889 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2082] 2024-05-26 13:28:26,889 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2082] 2024-05-26 13:28:26,889 >> loading file tokenizer.json
[INFO|configuration_utils.py:724] 2024-05-26 13:28:26,986 >> loading configuration file /home/student2021/srcao/base-model/llama2-7b/config.json
[INFO|configuration_utils.py:789] 2024-05-26 13:28:26,988 >> Model config LlamaConfig {
  "_name_or_path": "/home/student2021/srcao/base-model/llama2-7b",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3280] 2024-05-26 13:28:27,007 >> loading weights file /home/student2021/srcao/base-model/llama2-7b/model.safetensors.index.json
[INFO|modeling_utils.py:1417] 2024-05-26 13:28:27,008 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-05-26 13:28:27,008 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.87it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.20it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.15it/s]
[INFO|modeling_utils.py:4024] 2024-05-26 13:28:27,515 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-05-26 13:28:27,516 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/student2021/srcao/base-model/llama2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:881] 2024-05-26 13:28:27,520 >> loading configuration file /home/student2021/srcao/base-model/llama2-7b/generation_config.json
[INFO|configuration_utils.py:928] 2024-05-26 13:28:27,520 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

05/26/2024 13:28:27 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
05/26/2024 13:28:30 - INFO - llmtuner.model.adapter - Merged 1 adapter(s).
05/26/2024 13:28:30 - INFO - llmtuner.model.adapter - Loaded adapter(s): sft_checkpoint/llama2-7b-multi_role_train
05/26/2024 13:28:30 - INFO - llmtuner.model.loader - trainable params: 0 || all params: 6738415616 || trainable%: 0.0000
05/26/2024 13:28:30 - INFO - llmtuner.model.loader - This IS expected that the trainable params is 0 if you are using model for inference only.
05/26/2024 13:28:30 - INFO - llmtuner.data.template - Add pad token: </s>
05/26/2024 13:28:30 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-test.json...
05/26/2024 13:28:30 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
Using custom data configuration default-1732ef4d8e472d8e
Loading Dataset Infos from /home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/student2021/.cache/huggingface/datasets/json/default-1732ef4d8e472d8e/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
Downloading and preparing dataset json/default to /home/student2021/.cache/huggingface/datasets/json/default-1732ef4d8e472d8e/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05...
Downloading took 0.0 min
Checksum Computation took 0.0 min
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 346 examples [00:00, 55293.35 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/student2021/.cache/huggingface/datasets/json/default-1732ef4d8e472d8e/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05. Subsequent calls will reuse this data.
Converting format of dataset:   0%|          | 0/346 [00:00<?, ? examples/s]Caching processed dataset at /home/student2021/.cache/huggingface/datasets/json/default-1732ef4d8e472d8e/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-3a722b0603a82b88.arrow
Converting format of dataset: 100%|██████████| 346/346 [00:00<00:00, 39927.07 examples/s]
Running tokenizer on dataset:   0%|          | 0/346 [00:00<?, ? examples/s]Caching processed dataset at /home/student2021/.cache/huggingface/datasets/json/default-1732ef4d8e472d8e/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-cb7dac5383958fc2.arrow
Running tokenizer on dataset: 100%|██████████| 346/346 [00:00<00:00, 699.72 examples/s]Running tokenizer on dataset: 100%|██████████| 346/346 [00:00<00:00, 697.10 examples/s]
[INFO|training_args.py:1934] 2024-05-26 13:28:31,672 >> PyTorch: setting up devices
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:607] 2024-05-26 13:28:35,243 >> Using auto half precision backend
[INFO|trainer.py:3512] 2024-05-26 13:28:35,251 >> ***** Running Prediction *****
[INFO|trainer.py:3514] 2024-05-26 13:28:35,251 >>   Num examples = 346
[INFO|trainer.py:3517] 2024-05-26 13:28:35,251 >>   Batch size = 1
input_ids:
[12968, 29901, 13355, 1598, 322, 5353, 278, 2775, 537, 310, 5046, 269, 12358, 327, 7384, 322, 4768, 2129, 2729, 373, 278, 7928, 434, 29889, 13, 2677, 29901, 8565, 1558, 291, 1048, 29871, 29896, 29941, 29896, 29889, 10575, 727, 367, 5046, 13071, 363, 1098, 2548, 4086, 5198, 4455, 11104, 9808, 13, 13, 7900, 22137, 29901, 29871]
inputs:
Human: Identify and discuss the severity of age stereotypes and biases based on the dialogue.
Context: Discussion about 131. Should there be age limits for attending language immersion programs?.

Assistant: 
  0%|          | 0/346 [00:00<?, ?it/s]  1%|          | 2/346 [00:04<12:30,  2.18s/it]  1%|          | 3/346 [00:10<22:49,  3.99s/it]  1%|          | 4/346 [00:15<23:07,  4.06s/it]  1%|▏         | 5/346 [00:19<22:59,  4.04s/it]  2%|▏         | 6/346 [00:24<26:24,  4.66s/it]  2%|▏         | 7/346 [00:26<21:06,  3.74s/it]  2%|▏         | 8/346 [00:29<19:36,  3.48s/it]  3%|▎         | 9/346 [00:34<21:27,  3.82s/it]  3%|▎         | 10/346 [00:39<23:56,  4.27s/it]  3%|▎         | 11/346 [00:42<20:48,  3.73s/it]  3%|▎         | 12/346 [00:49<26:20,  4.73s/it]  4%|▍         | 13/346 [00:51<22:24,  4.04s/it]  4%|▍         | 14/346 [00:51<15:50,  2.86s/it]  4%|▍         | 15/346 [00:51<11:16,  2.04s/it]  5%|▍         | 16/346 [00:51<08:05,  1.47s/it]  5%|▍         | 17/346 [00:56<12:41,  2.31s/it]  5%|▌         | 18/346 [01:02<19:12,  3.51s/it]  5%|▌         | 19/346 [01:07<20:53,  3.83s/it]  6%|▌         | 20/346 [01:13<25:00,  4.60s/it]  6%|▌         | 21/346 [01:13<17:40,  3.26s/it]  6%|▋         | 22/346 [01:13<12:33,  2.33s/it]  7%|▋         | 23/346 [01:13<08:59,  1.67s/it]  7%|▋         | 24/346 [01:14<06:30,  1.21s/it]  7%|▋         | 25/346 [01:17<09:36,  1.79s/it]  8%|▊         | 26/346 [01:17<06:55,  1.30s/it]  8%|▊         | 27/346 [01:20<09:26,  1.77s/it]  8%|▊         | 28/346 [01:20<06:48,  1.28s/it]  8%|▊         | 29/346 [01:25<13:00,  2.46s/it]  9%|▊         | 30/346 [01:31<19:05,  3.63s/it]  9%|▉         | 31/346 [01:32<13:32,  2.58s/it]  9%|▉         | 32/346 [01:37<17:21,  3.32s/it] 10%|▉         | 33/346 [01:39<15:41,  3.01s/it] 10%|▉         | 34/346 [01:42<16:00,  3.08s/it] 10%|█         | 35/346 [01:46<17:11,  3.32s/it] 10%|█         | 36/346 [01:54<24:03,  4.66s/it] 11%|█         | 37/346 [01:59<24:15,  4.71s/it] 11%|█         | 38/346 [02:01<20:43,  4.04s/it] 11%|█▏        | 39/346 [02:06<21:15,  4.15s/it] 12%|█▏        | 40/346 [02:13<26:58,  5.29s/it] 12%|█▏        | 41/346 [02:18<25:01,  4.92s/it] 12%|█▏        | 42/346 [02:21<22:18,  4.40s/it] 12%|█▏        | 43/346 [02:27<24:48,  4.91s/it] 13%|█▎        | 44/346 [02:33<27:14,  5.41s/it] 13%|█▎        | 45/346 [02:36<22:51,  4.56s/it] 13%|█▎        | 46/346 [02:41<23:00,  4.60s/it] 14%|█▎        | 47/346 [02:44<20:19,  4.08s/it] 14%|█▍        | 48/346 [02:48<20:22,  4.10s/it] 14%|█▍        | 49/346 [02:54<23:18,  4.71s/it] 14%|█▍        | 50/346 [02:59<23:33,  4.78s/it] 15%|█▍        | 51/346 [02:59<16:38,  3.39s/it] 15%|█▌        | 52/346 [03:02<16:18,  3.33s/it] 15%|█▌        | 53/346 [03:06<17:19,  3.55s/it] 16%|█▌        | 54/346 [03:09<16:21,  3.36s/it] 16%|█▌        | 55/346 [03:14<18:38,  3.85s/it] 16%|█▌        | 56/346 [03:14<13:12,  2.73s/it] 16%|█▋        | 57/346 [03:17<13:34,  2.82s/it] 17%|█▋        | 58/346 [03:21<15:16,  3.18s/it] 17%|█▋        | 59/346 [03:21<10:51,  2.27s/it] 17%|█▋        | 60/346 [03:30<20:20,  4.27s/it] 18%|█▊        | 61/346 [03:35<20:17,  4.27s/it] 18%|█▊        | 62/346 [03:37<18:02,  3.81s/it] 18%|█▊        | 63/346 [03:48<26:59,  5.72s/it] 18%|█▊        | 64/346 [03:54<28:27,  6.05s/it] 19%|█▉        | 65/346 [03:59<25:45,  5.50s/it] 19%|█▉        | 66/346 [04:02<22:49,  4.89s/it] 19%|█▉        | 67/346 [04:02<16:07,  3.47s/it] 20%|█▉        | 68/346 [04:06<16:55,  3.65s/it] 20%|█▉        | 69/346 [04:10<16:40,  3.61s/it] 20%|██        | 70/346 [04:15<18:15,  3.97s/it] 21%|██        | 71/346 [04:18<17:19,  3.78s/it] 21%|██        | 72/346 [04:18<12:16,  2.69s/it] 21%|██        | 73/346 [04:18<08:45,  1.93s/it] 21%|██▏       | 74/346 [04:24<13:38,  3.01s/it] 22%|██▏       | 75/346 [04:24<09:42,  2.15s/it] 22%|██▏       | 76/346 [04:32<17:11,  3.82s/it] 22%|██▏       | 77/346 [04:35<16:36,  3.70s/it] 23%|██▎       | 78/346 [04:39<16:32,  3.70s/it] 23%|██▎       | 79/346 [04:43<17:33,  3.95s/it] 23%|██▎       | 80/346 [04:46<16:12,  3.66s/it] 23%|██▎       | 81/346 [04:49<15:07,  3.43s/it] 24%|██▎       | 82/346 [04:49<10:44,  2.44s/it] 24%|██▍       | 83/346 [04:57<17:18,  3.95s/it] 24%|██▍       | 84/346 [04:57<12:16,  2.81s/it] 25%|██▍       | 85/346 [04:57<08:44,  2.01s/it] 25%|██▍       | 86/346 [05:02<12:01,  2.77s/it] 25%|██▌       | 87/346 [05:05<12:16,  2.84s/it] 25%|██▌       | 88/346 [05:08<13:02,  3.03s/it] 26%|██▌       | 89/346 [05:08<09:16,  2.17s/it] 26%|██▌       | 90/346 [05:13<12:12,  2.86s/it] 26%|██▋       | 91/346 [05:15<11:59,  2.82s/it] 27%|██▋       | 92/346 [05:17<10:55,  2.58s/it] 27%|██▋       | 93/346 [05:20<11:10,  2.65s/it] 27%|██▋       | 94/346 [05:25<14:01,  3.34s/it] 27%|██▋       | 95/346 [05:33<19:05,  4.56s/it] 28%|██▊       | 96/346 [05:36<17:32,  4.21s/it] 28%|██▊       | 97/346 [05:38<15:21,  3.70s/it] 28%|██▊       | 98/346 [05:43<15:58,  3.87s/it] 29%|██▊       | 99/346 [05:49<19:20,  4.70s/it] 29%|██▉       | 100/346 [05:54<19:16,  4.70s/it] 29%|██▉       | 101/346 [05:54<13:36,  3.33s/it] 29%|██▉       | 102/346 [06:00<17:00,  4.18s/it] 30%|██▉       | 103/346 [06:04<16:25,  4.06s/it] 30%|███       | 104/346 [06:09<17:04,  4.24s/it] 30%|███       | 105/346 [06:09<12:04,  3.01s/it] 31%|███       | 106/346 [06:14<14:37,  3.66s/it] 31%|███       | 107/346 [06:17<13:18,  3.34s/it] 31%|███       | 108/346 [06:17<09:26,  2.38s/it] 32%|███▏      | 109/346 [06:20<10:10,  2.57s/it] 32%|███▏      | 110/346 [06:23<11:06,  2.83s/it] 32%|███▏      | 111/346 [06:28<13:45,  3.51s/it] 32%|███▏      | 112/346 [06:29<09:45,  2.50s/it] 33%|███▎      | 113/346 [06:34<12:57,  3.34s/it] 33%|███▎      | 114/346 [06:39<14:45,  3.82s/it] 33%|███▎      | 115/346 [06:41<13:14,  3.44s/it] 34%|███▎      | 116/346 [06:41<09:23,  2.45s/it] 34%|███▍      | 117/346 [06:45<10:18,  2.70s/it] 34%|███▍      | 118/346 [06:50<12:39,  3.33s/it] 34%|███▍      | 119/346 [06:54<14:16,  3.77s/it] 35%|███▍      | 120/346 [06:59<14:47,  3.93s/it] 35%|███▍      | 121/346 [07:02<14:19,  3.82s/it] 35%|███▌      | 122/346 [07:06<14:08,  3.79s/it] 36%|███▌      | 123/346 [07:10<13:53,  3.74s/it] 36%|███▌      | 124/346 [07:15<15:39,  4.23s/it] 36%|███▌      | 125/346 [07:20<16:53,  4.59s/it] 36%|███▋      | 126/346 [07:26<18:09,  4.95s/it] 37%|███▋      | 127/346 [07:26<12:48,  3.51s/it] 37%|███▋      | 128/346 [07:29<11:57,  3.29s/it] 37%|███▋      | 129/346 [07:33<12:48,  3.54s/it] 38%|███▊      | 130/346 [07:37<13:16,  3.69s/it] 38%|███▊      | 131/346 [07:40<11:45,  3.28s/it] 38%|███▊      | 132/346 [07:40<08:20,  2.34s/it] 38%|███▊      | 133/346 [07:45<10:55,  3.08s/it] 39%|███▊      | 134/346 [07:45<07:45,  2.20s/it] 39%|███▉      | 135/346 [07:45<05:33,  1.58s/it] 39%|███▉      | 136/346 [07:51<09:57,  2.84s/it] 40%|███▉      | 137/346 [07:55<11:22,  3.26s/it] 40%|███▉      | 138/346 [07:58<11:11,  3.23s/it] 40%|████      | 139/346 [08:06<16:15,  4.71s/it] 40%|████      | 140/346 [08:11<16:44,  4.87s/it] 41%|████      | 141/346 [08:12<11:48,  3.46s/it] 41%|████      | 142/346 [08:18<14:17,  4.20s/it] 41%|████▏     | 143/346 [08:22<14:47,  4.37s/it] 42%|████▏     | 144/346 [08:26<14:27,  4.29s/it] 42%|████▏     | 145/346 [08:31<14:39,  4.38s/it] 42%|████▏     | 146/346 [08:37<16:18,  4.89s/it] 42%|████▏     | 147/346 [08:40<14:46,  4.45s/it] 43%|████▎     | 148/346 [08:44<13:38,  4.13s/it] 43%|████▎     | 149/346 [08:50<15:46,  4.81s/it] 43%|████▎     | 150/346 [08:50<11:07,  3.41s/it] 44%|████▎     | 151/346 [08:57<14:09,  4.36s/it] 44%|████▍     | 152/346 [09:04<16:13,  5.02s/it] 44%|████▍     | 153/346 [09:07<14:17,  4.45s/it] 45%|████▍     | 154/346 [09:07<10:05,  3.15s/it] 45%|████▍     | 155/346 [09:10<10:23,  3.26s/it] 45%|████▌     | 156/346 [09:13<09:34,  3.03s/it] 45%|████▌     | 157/346 [09:16<10:05,  3.20s/it] 46%|████▌     | 158/346 [09:22<12:12,  3.90s/it] 46%|████▌     | 159/346 [09:25<11:39,  3.74s/it] 46%|████▌     | 160/346 [09:28<10:56,  3.53s/it] 47%|████▋     | 161/346 [09:28<07:45,  2.52s/it] 47%|████▋     | 162/346 [09:34<10:07,  3.30s/it] 47%|████▋     | 163/346 [09:34<07:10,  2.35s/it] 47%|████▋     | 164/346 [09:37<08:12,  2.71s/it] 48%|████▊     | 165/346 [09:37<05:50,  1.94s/it] 48%|████▊     | 166/346 [09:41<06:51,  2.29s/it] 48%|████▊     | 167/346 [09:45<08:25,  2.82s/it] 49%|████▊     | 168/346 [09:50<10:37,  3.58s/it] 49%|████▉     | 169/346 [09:54<10:43,  3.63s/it] 49%|████▉     | 170/346 [10:01<13:29,  4.60s/it] 49%|████▉     | 171/346 [10:06<14:07,  4.84s/it] 50%|████▉     | 172/346 [10:10<13:24,  4.62s/it] 50%|█████     | 173/346 [10:14<13:00,  4.51s/it] 50%|█████     | 174/346 [10:14<09:10,  3.20s/it] 51%|█████     | 175/346 [10:18<09:17,  3.26s/it] 51%|█████     | 176/346 [10:23<11:05,  3.92s/it] 51%|█████     | 177/346 [10:30<12:58,  4.61s/it] 51%|█████▏    | 178/346 [10:34<12:25,  4.44s/it] 52%|█████▏    | 179/346 [10:39<13:27,  4.83s/it] 52%|█████▏    | 180/346 [10:46<14:56,  5.40s/it] 52%|█████▏    | 181/346 [10:46<10:30,  3.82s/it] 53%|█████▎    | 182/346 [10:53<12:40,  4.64s/it] 53%|█████▎    | 183/346 [11:00<14:24,  5.30s/it] 53%|█████▎    | 184/346 [11:05<14:42,  5.45s/it] 53%|█████▎    | 185/346 [11:08<12:22,  4.61s/it] 54%|█████▍    | 186/346 [11:11<11:18,  4.24s/it] 54%|█████▍    | 187/346 [11:18<12:53,  4.86s/it] 54%|█████▍    | 188/346 [11:24<14:16,  5.42s/it] 55%|█████▍    | 189/346 [11:29<13:11,  5.04s/it] 55%|█████▍    | 190/346 [11:35<14:21,  5.52s/it] 55%|█████▌    | 191/346 [11:35<10:06,  3.91s/it] 55%|█████▌    | 192/346 [11:39<09:58,  3.89s/it] 56%|█████▌    | 193/346 [11:42<08:55,  3.50s/it] 56%|█████▌    | 194/346 [11:46<09:18,  3.68s/it] 56%|█████▋    | 195/346 [11:46<06:35,  2.62s/it] 57%|█████▋    | 196/346 [11:46<04:41,  1.88s/it] 57%|█████▋    | 197/346 [11:46<03:22,  1.36s/it] 57%|█████▋    | 198/346 [11:52<06:21,  2.58s/it] 58%|█████▊    | 199/346 [11:56<07:31,  3.07s/it] 58%|█████▊    | 200/346 [11:56<05:19,  2.19s/it] 58%|█████▊    | 201/346 [12:01<07:13,  2.99s/it] 58%|█████▊    | 202/346 [12:08<09:45,  4.07s/it] 59%|█████▊    | 203/346 [12:11<09:31,  4.00s/it] 59%|█████▉    | 204/346 [12:19<11:55,  5.04s/it] 59%|█████▉    | 205/346 [12:22<10:48,  4.60s/it] 60%|█████▉    | 206/346 [12:23<07:36,  3.26s/it] 60%|█████▉    | 207/346 [12:26<07:32,  3.26s/it] 60%|██████    | 208/346 [12:32<09:21,  4.07s/it] 60%|██████    | 209/346 [12:36<09:40,  4.23s/it] 61%|██████    | 210/346 [12:42<10:21,  4.57s/it] 61%|██████    | 211/346 [12:42<07:17,  3.24s/it] 61%|██████▏   | 212/346 [12:42<05:09,  2.31s/it] 62%|██████▏   | 213/346 [12:46<06:17,  2.84s/it] 62%|██████▏   | 214/346 [12:52<08:33,  3.89s/it] 62%|██████▏   | 215/346 [12:56<08:18,  3.81s/it] 62%|██████▏   | 216/346 [12:58<07:09,  3.30s/it] 63%|██████▎   | 217/346 [13:04<08:24,  3.91s/it] 63%|██████▎   | 218/346 [13:10<10:11,  4.78s/it] 63%|██████▎   | 219/346 [13:16<10:46,  5.09s/it] 64%|██████▎   | 220/346 [13:16<07:34,  3.61s/it] 64%|██████▍   | 221/346 [13:21<07:51,  3.77s/it] 64%|██████▍   | 222/346 [13:26<08:45,  4.24s/it] 64%|██████▍   | 223/346 [13:32<10:03,  4.91s/it] 65%|██████▍   | 224/346 [13:32<07:04,  3.48s/it] 65%|██████▌   | 225/346 [13:37<07:44,  3.84s/it] 65%|██████▌   | 226/346 [13:39<06:42,  3.35s/it] 66%|██████▌   | 227/346 [13:42<06:18,  3.18s/it] 66%|██████▌   | 228/346 [13:48<07:49,  3.98s/it] 66%|██████▌   | 229/346 [13:48<05:30,  2.83s/it] 66%|██████▋   | 230/346 [13:53<06:36,  3.42s/it] 67%|██████▋   | 231/346 [13:56<06:19,  3.30s/it] 67%|██████▋   | 232/346 [13:56<04:28,  2.35s/it] 67%|██████▋   | 233/346 [14:01<05:57,  3.16s/it] 68%|██████▊   | 234/346 [14:04<05:49,  3.12s/it] 68%|██████▊   | 235/346 [14:09<06:33,  3.55s/it] 68%|██████▊   | 236/346 [14:09<04:38,  2.53s/it] 68%|██████▊   | 237/346 [14:12<05:00,  2.76s/it] 69%|██████▉   | 238/346 [14:18<06:23,  3.55s/it] 69%|██████▉   | 239/346 [14:23<07:24,  4.16s/it] 69%|██████▉   | 240/346 [14:29<08:17,  4.70s/it] 70%|██████▉   | 241/346 [14:35<09:00,  5.15s/it] 70%|██████▉   | 242/346 [14:40<08:49,  5.09s/it] 70%|███████   | 243/346 [14:47<09:34,  5.58s/it] 71%|███████   | 244/346 [14:53<09:42,  5.71s/it] 71%|███████   | 245/346 [14:59<09:55,  5.90s/it] 71%|███████   | 246/346 [15:01<07:51,  4.72s/it] 71%|███████▏  | 247/346 [15:01<05:31,  3.35s/it] 72%|███████▏  | 248/346 [15:05<05:41,  3.48s/it] 72%|███████▏  | 249/346 [15:09<05:38,  3.49s/it] 72%|███████▏  | 250/346 [15:15<07:06,  4.44s/it] 73%|███████▎  | 251/346 [15:16<04:59,  3.15s/it] 73%|███████▎  | 252/346 [15:16<03:31,  2.25s/it] 73%|███████▎  | 253/346 [15:19<04:06,  2.65s/it] 73%|███████▎  | 254/346 [15:22<04:18,  2.81s/it] 74%|███████▎  | 255/346 [15:27<04:52,  3.22s/it] 74%|███████▍  | 256/346 [15:32<05:37,  3.75s/it] 74%|███████▍  | 257/346 [15:38<06:53,  4.64s/it] 75%|███████▍  | 258/346 [15:42<06:24,  4.37s/it] 75%|███████▍  | 259/346 [15:43<05:03,  3.49s/it] 75%|███████▌  | 260/346 [15:47<05:11,  3.63s/it] 75%|███████▌  | 261/346 [15:48<03:39,  2.58s/it] 76%|███████▌  | 262/346 [15:51<03:59,  2.85s/it] 76%|███████▌  | 263/346 [15:57<05:08,  3.71s/it] 76%|███████▋  | 264/346 [16:02<05:40,  4.15s/it] 77%|███████▋  | 265/346 [16:07<06:05,  4.51s/it] 77%|███████▋  | 266/346 [16:13<06:22,  4.78s/it] 77%|███████▋  | 267/346 [16:15<05:25,  4.12s/it] 77%|███████▋  | 268/346 [16:18<04:56,  3.80s/it] 78%|███████▊  | 269/346 [16:22<04:39,  3.63s/it] 78%|███████▊  | 270/346 [16:25<04:35,  3.62s/it] 78%|███████▊  | 271/346 [16:25<03:13,  2.58s/it] 79%|███████▊  | 272/346 [16:25<02:16,  1.85s/it] 79%|███████▉  | 273/346 [16:31<03:28,  2.86s/it] 79%|███████▉  | 274/346 [16:33<03:19,  2.78s/it] 79%|███████▉  | 275/346 [16:33<02:21,  1.99s/it] 80%|███████▉  | 276/346 [16:37<02:50,  2.43s/it] 80%|████████  | 277/346 [16:37<02:00,  1.75s/it] 80%|████████  | 278/346 [16:42<02:57,  2.61s/it] 81%|████████  | 279/346 [16:48<04:11,  3.76s/it] 81%|████████  | 280/346 [16:53<04:39,  4.24s/it] 81%|████████  | 281/346 [16:54<03:15,  3.01s/it] 82%|████████▏ | 282/346 [16:54<02:27,  2.30s/it] 82%|████████▏ | 283/346 [16:58<02:55,  2.79s/it] 82%|████████▏ | 284/346 [17:04<03:52,  3.76s/it] 82%|████████▏ | 285/346 [17:07<03:39,  3.60s/it] 83%|████████▎ | 286/346 [17:11<03:26,  3.45s/it] 83%|████████▎ | 287/346 [17:11<02:25,  2.46s/it] 83%|████████▎ | 288/346 [17:14<02:29,  2.58s/it] 84%|████████▎ | 289/346 [17:16<02:21,  2.49s/it] 84%|████████▍ | 290/346 [17:21<03:06,  3.34s/it] 84%|████████▍ | 291/346 [17:21<02:10,  2.38s/it] 84%|████████▍ | 292/346 [17:26<02:46,  3.08s/it] 85%|████████▍ | 293/346 [17:30<02:56,  3.33s/it] 85%|████████▍ | 294/346 [17:35<03:22,  3.90s/it] 85%|████████▌ | 295/346 [17:37<02:41,  3.17s/it] 86%|████████▌ | 296/346 [17:40<02:45,  3.31s/it] 86%|████████▌ | 297/346 [17:42<02:22,  2.92s/it] 86%|████████▌ | 298/346 [17:47<02:47,  3.50s/it] 86%|████████▋ | 299/346 [17:50<02:40,  3.41s/it] 87%|████████▋ | 300/346 [17:54<02:39,  3.47s/it] 87%|████████▋ | 301/346 [17:59<02:52,  3.83s/it] 87%|████████▋ | 302/346 [18:03<02:59,  4.08s/it] 88%|████████▊ | 303/346 [18:08<03:07,  4.35s/it] 88%|████████▊ | 304/346 [18:11<02:42,  3.87s/it] 88%|████████▊ | 305/346 [18:17<03:01,  4.43s/it] 88%|████████▊ | 306/346 [18:20<02:41,  4.03s/it] 89%|████████▊ | 307/346 [18:26<03:05,  4.76s/it] 89%|████████▉ | 308/346 [18:32<03:10,  5.01s/it] 89%|████████▉ | 309/346 [18:32<02:11,  3.55s/it] 90%|████████▉ | 310/346 [18:35<02:01,  3.36s/it] 90%|████████▉ | 311/346 [18:41<02:25,  4.15s/it] 90%|█████████ | 312/346 [18:46<02:26,  4.30s/it] 90%|█████████ | 313/346 [18:52<02:45,  5.03s/it] 91%|█████████ | 314/346 [18:54<02:12,  4.13s/it] 91%|█████████ | 315/346 [18:58<02:02,  3.95s/it] 91%|█████████▏| 316/346 [19:02<02:01,  4.04s/it] 92%|█████████▏| 317/346 [19:08<02:13,  4.60s/it] 92%|█████████▏| 318/346 [19:08<01:31,  3.26s/it] 92%|█████████▏| 319/346 [19:13<01:44,  3.86s/it] 92%|█████████▏| 320/346 [19:15<01:25,  3.29s/it] 93%|█████████▎| 321/346 [19:19<01:26,  3.46s/it] 93%|█████████▎| 322/346 [19:19<00:59,  2.47s/it] 93%|█████████▎| 323/346 [19:20<00:40,  1.77s/it] 94%|█████████▎| 324/346 [19:23<00:49,  2.24s/it] 94%|█████████▍| 325/346 [19:30<01:17,  3.70s/it] 94%|█████████▍| 326/346 [19:38<01:39,  4.96s/it] 95%|█████████▍| 327/346 [19:42<01:30,  4.76s/it] 95%|█████████▍| 328/346 [19:42<01:00,  3.38s/it] 95%|█████████▌| 329/346 [19:49<01:12,  4.27s/it] 95%|█████████▌| 330/346 [19:53<01:10,  4.39s/it] 96%|█████████▌| 331/346 [19:59<01:10,  4.72s/it] 96%|█████████▌| 332/346 [20:07<01:22,  5.87s/it] 96%|█████████▌| 333/346 [20:08<00:53,  4.15s/it] 97%|█████████▋| 334/346 [20:11<00:48,  4.00s/it] 97%|█████████▋| 335/346 [20:15<00:42,  3.84s/it] 97%|█████████▋| 336/346 [20:19<00:39,  4.00s/it] 97%|█████████▋| 337/346 [20:19<00:25,  2.84s/it] 98%|█████████▊| 338/346 [20:21<00:20,  2.56s/it] 98%|█████████▊| 339/346 [20:21<00:12,  1.84s/it] 98%|█████████▊| 340/346 [20:26<00:16,  2.81s/it] 99%|█████████▊| 341/346 [20:30<00:15,  3.17s/it] 99%|█████████▉| 342/346 [20:37<00:16,  4.22s/it] 99%|█████████▉| 343/346 [20:40<00:11,  3.83s/it] 99%|█████████▉| 344/346 [20:45<00:08,  4.08s/it]100%|█████████▉| 345/346 [20:49<00:04,  4.33s/it]100%|██████████| 346/346 [20:50<00:00,  3.08s/it]Building prefix dict from the default dictionary ...
Dumping model to file cache /tmp/jieba.cache
Loading model cost 0.629 seconds.
Prefix dict has been built successfully.
100%|██████████| 346/346 [21:05<00:00,  3.66s/it]
***** predict metrics *****
  predict_bleu-4             =    27.3733
  predict_rouge-1            =    29.4583
  predict_rouge-2            =    10.9795
  predict_rouge-l            =     19.133
  predict_runtime            = 0:21:06.40
  predict_samples_per_second =      0.273
  predict_steps_per_second   =      0.273
05/26/2024 13:49:41 - INFO - llmtuner.train.sft.trainer - Saving prediction results to predict_res/sft-llama2-7b-multi_role_test/generated_predictions.jsonl
[2024-05-28 06:45:58,728] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 06:46:07,619] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-28 06:46:07,644] [INFO] [runner.py:568:main] cmd = /home/student2021/anaconda3/envs/llama_factory/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None src/train_bash.py --stage sft --do_train --model_name_or_path /home/student2021/srcao/base-model/llama2-7b --dataset multi_role_train --template default --finetuning_type lora --lora_alpha 16 --lora_dropout 0.05 --lora_rank 8 --lora_target q_proj,v_proj --output_dir sft_checkpoint/llama2-7b-multi_role_train --overwrite_cache --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --lr_scheduler_type linear --warmup_steps 200 --max_length 1024 --logging_steps 10 --save_steps 1000 --learning_rate 5e-5 --max_samples 1000 --num_train_epochs 3.0 --plot_loss --fp16 --report_to=wandb
[2024-05-28 06:46:09,496] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 06:46:11,230] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-05-28 06:46:11,230] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-05-28 06:46:11,230] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-05-28 06:46:11,230] [INFO] [launch.py:163:main] dist_world_size=4
[2024-05-28 06:46:11,231] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-05-28 06:46:11,232] [INFO] [launch.py:253:main] process 4728 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=0', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--max_samples', '1000', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-28 06:46:11,233] [INFO] [launch.py:253:main] process 4729 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=1', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--max_samples', '1000', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-28 06:46:11,234] [INFO] [launch.py:253:main] process 4730 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=2', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--max_samples', '1000', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-28 06:46:11,234] [INFO] [launch.py:253:main] process 4731 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=3', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--max_samples', '1000', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-28 06:46:29,370] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 06:46:29,370] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 06:46:29,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 06:46:29,378] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
05/28/2024 06:46:44 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/28/2024 06:46:44 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/28/2024 06:46:44 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/28/2024 06:46:44 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1934] 2024-05-28 06:46:44,537 >> PyTorch: setting up devices
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 25, in run_exp
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/hparams/parser.py", line 200, in get_train_args
    raise ValueError("Output directory already exists and is not empty. Please set `overwrite_output_dir`.")
ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 25, in run_exp
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/hparams/parser.py", line 200, in get_train_args
    raise ValueError("Output directory already exists and is not empty. Please set `overwrite_output_dir`.")
ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 25, in run_exp
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/hparams/parser.py", line 200, in get_train_args
    raise ValueError("Output directory already exists and is not empty. Please set `overwrite_output_dir`.")
ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 25, in run_exp
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/hparams/parser.py", line 200, in get_train_args
    raise ValueError("Output directory already exists and is not empty. Please set `overwrite_output_dir`.")
ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.
[2024-05-28 06:46:48,278] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 4728
[2024-05-28 06:46:48,292] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 4729
[2024-05-28 06:46:48,301] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 4730
[2024-05-28 06:46:48,301] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 4731
[2024-05-28 06:46:48,307] [ERROR] [launch.py:322:sigkill_handler] ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=3', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--max_samples', '1000', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb'] exits with return code = 1
[2024-05-28 06:47:20,933] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 06:47:22,797] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-28 06:47:22,820] [INFO] [runner.py:568:main] cmd = /home/student2021/anaconda3/envs/llama_factory/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None src/train_bash.py --stage sft --do_train --model_name_or_path /home/student2021/srcao/base-model/llama2-7b --dataset multi_role_train --template default --finetuning_type lora --lora_alpha 16 --lora_dropout 0.05 --lora_rank 8 --lora_target q_proj,v_proj --output_dir sft_checkpoint/llama2-7b-multi_role_train-1000 --overwrite_cache --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --lr_scheduler_type linear --warmup_steps 200 --max_length 1024 --logging_steps 10 --save_steps 1000 --learning_rate 5e-5 --max_samples 1000 --num_train_epochs 3.0 --plot_loss --fp16 --report_to=wandb
[2024-05-28 06:47:24,679] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 06:47:26,455] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-05-28 06:47:26,455] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-05-28 06:47:26,455] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-05-28 06:47:26,455] [INFO] [launch.py:163:main] dist_world_size=4
[2024-05-28 06:47:26,455] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-05-28 06:47:26,456] [INFO] [launch.py:253:main] process 5642 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=0', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train-1000', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--max_samples', '1000', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-28 06:47:26,456] [INFO] [launch.py:253:main] process 5643 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=1', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train-1000', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--max_samples', '1000', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-28 06:47:26,457] [INFO] [launch.py:253:main] process 5644 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=2', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train-1000', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--max_samples', '1000', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-28 06:47:26,457] [INFO] [launch.py:253:main] process 5645 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=3', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train-1000', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--max_samples', '1000', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-28 06:47:30,569] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 06:47:30,574] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 06:47:30,607] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 06:47:30,647] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
05/28/2024 06:47:36 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1934] 2024-05-28 06:47:36,634 >> PyTorch: setting up devices
05/28/2024 06:47:36 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/28/2024 06:47:36 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/28/2024 06:47:36 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/28/2024 06:47:36 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/28/2024 06:47:36 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train-1000/runs/May28_06-47-35_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train-1000,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train-1000,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2082] 2024-05-28 06:47:36,693 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2082] 2024-05-28 06:47:36,693 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2082] 2024-05-28 06:47:36,693 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2082] 2024-05-28 06:47:36,693 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2082] 2024-05-28 06:47:36,693 >> loading file tokenizer.json
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/28/2024 06:47:36 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/28/2024 06:47:36 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train-1000/runs/May28_06-47-35_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train-1000,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train-1000,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/28/2024 06:47:36 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/28/2024 06:47:36 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/28/2024 06:47:36 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train-1000/runs/May28_06-47-35_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train-1000,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train-1000,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
05/28/2024 06:47:36 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train-1000/runs/May28_06-47-35_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train-1000,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train-1000,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
[INFO|configuration_utils.py:724] 2024-05-28 06:47:36,913 >> loading configuration file /home/student2021/srcao/base-model/llama2-7b/config.json
[INFO|configuration_utils.py:789] 2024-05-28 06:47:36,914 >> Model config LlamaConfig {
  "_name_or_path": "/home/student2021/srcao/base-model/llama2-7b",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3280] 2024-05-28 06:47:37,218 >> loading weights file /home/student2021/srcao/base-model/llama2-7b/model.safetensors.index.json
[INFO|modeling_utils.py:1417] 2024-05-28 06:47:37,221 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-05-28 06:47:37,224 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.14s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.13s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.11s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.17s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.16s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.17s/it]
[INFO|modeling_utils.py:4024] 2024-05-28 06:47:44,745 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-05-28 06:47:44,745 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/student2021/srcao/base-model/llama2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
05/28/2024 06:47:44 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/28/2024 06:47:44 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
05/28/2024 06:47:44 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/28/2024 06:47:44 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/28/2024 06:47:44 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
05/28/2024 06:47:44 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
[INFO|configuration_utils.py:881] 2024-05-28 06:47:44,748 >> loading configuration file /home/student2021/srcao/base-model/llama2-7b/generation_config.json
[INFO|configuration_utils.py:928] 2024-05-28 06:47:44,749 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

05/28/2024 06:47:44 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/28/2024 06:47:44 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
05/28/2024 06:47:44 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/28/2024 06:47:44 - INFO - llmtuner.data.template - Add pad token: </s>
05/28/2024 06:47:44 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/28/2024 06:47:44 - INFO - llmtuner.data.template - Add pad token: </s>
05/28/2024 06:47:44 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/28/2024 06:47:44 - INFO - llmtuner.data.template - Add pad token: </s>
05/28/2024 06:47:44 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/28/2024 06:47:44 - INFO - llmtuner.data.template - Add pad token: </s>
05/28/2024 06:47:44 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/28/2024 06:47:44 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
Using custom data configuration default-a0e2d6f8a6470c31
Loading Dataset Infos from /home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
Found cached dataset json (/home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
Loading Dataset info from /home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
Converting format of dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]Caching processed dataset at /home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-989ecb9c1db2a22c.arrow
Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 10215.19 examples/s]
05/28/2024 06:47:50 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/28/2024 06:47:50 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/28/2024 06:47:50 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/28/2024 06:47:50 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
05/28/2024 06:47:50 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
05/28/2024 06:47:50 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
Running tokenizer on dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 52906.28 examples/s]
Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 55159.18 examples/s]
Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 54662.44 examples/s]
Caching processed dataset at /home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-3cb41857d1730b34.arrow
Running tokenizer on dataset: 100%|██████████| 1000/1000 [00:01<00:00, 662.39 examples/s]Running tokenizer on dataset: 100%|██████████| 1000/1000 [00:01<00:00, 660.80 examples/s]
input_ids:
[12968, 29901, 13355, 1598, 322, 5353, 278, 2775, 537, 310, 5046, 269, 12358, 327, 7384, 322, 4768, 2129, 2729, 373, 278, 7928, 434, 29889, 13, 2677, 29901, 8565, 1558, 291, 1048, 29871, 29906, 29955, 29945, 29889, 10575, 727, 367, 5046, 13071, 363, 5221, 1218, 297, 3058, 28709, 1288, 14188, 9808, 13, 13, 7900, 22137, 29901, 29871, 1094, 263, 29871, 29906, 29900, 29899, 6360, 29899, 1025, 5375, 29892, 306, 11188, 278, 3736, 1103, 3145, 7258, 491, 278, 29871, 29946, 29900, 29899, 6360, 29899, 1025, 5375, 313, 17662, 29897, 322, 278, 29871, 29953, 29900, 29899, 6360, 29899, 1025, 5375, 313, 1025, 29897, 373, 5046, 13071, 363, 28709, 1288, 14188, 29889, 739, 29915, 29879, 18853, 304, 2050, 1716, 15332, 21838, 322, 278, 7037, 25722, 29884, 362, 310, 269, 12358, 327, 7384, 746, 5353, 292, 1316, 25091, 29889, 21605, 5046, 13071, 508, 367, 263, 982, 304, 7536, 277, 675, 15332, 322, 5557, 19039, 5161, 2039, 363, 15724, 1058, 1122, 451, 505, 278, 1021, 9128, 633, 9770, 470, 286, 1337, 537, 11174, 29889, 2398, 29892, 372, 29915, 29879, 7618, 1455, 304, 2948, 1438, 13071, 411, 4771, 24858, 304, 4772, 2313, 20386, 7606, 23274, 2729, 14419, 368, 373, 5046, 29889, 383, 4015, 263, 17346, 393, 5662, 1973, 15332, 1550, 2504, 11427, 20978, 2068, 363, 15724, 310, 599, 24646, 338, 1820, 29889, 3374, 366, 1716, 363, 3033, 6751, 297, 445, 6593, 1319, 10679, 29889, 2]
inputs:
Human: Identify and discuss the severity of age stereotypes and biases based on the dialogue.
Context: Discussion about 275. Should there be age limits for participating in certain recreational activities?.

Assistant:  As a 20-year-old individual, I appreciate the perspectives shared by the 40-year-old individual (middle) and the 60-year-old individual (old) on age limits for recreational activities. It's essential to consider both safety concerns and the potential perpetuation of stereotypes when discussing such restrictions. Setting age limits can be a way to prioritize safety and prevent unnecessary risks for individuals who may not have the same physical abilities or maturity levels. However, it's crucial to approach these limits with sensitivity to avoid discriminatory practices based solely on age. Finding a balance that ensures safety while promoting inclusivity for individuals of all ages is key. Thank you both for engaging in this meaningful discussion.</s>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1094, 263, 29871, 29906, 29900, 29899, 6360, 29899, 1025, 5375, 29892, 306, 11188, 278, 3736, 1103, 3145, 7258, 491, 278, 29871, 29946, 29900, 29899, 6360, 29899, 1025, 5375, 313, 17662, 29897, 322, 278, 29871, 29953, 29900, 29899, 6360, 29899, 1025, 5375, 313, 1025, 29897, 373, 5046, 13071, 363, 28709, 1288, 14188, 29889, 739, 29915, 29879, 18853, 304, 2050, 1716, 15332, 21838, 322, 278, 7037, 25722, 29884, 362, 310, 269, 12358, 327, 7384, 746, 5353, 292, 1316, 25091, 29889, 21605, 5046, 13071, 508, 367, 263, 982, 304, 7536, 277, 675, 15332, 322, 5557, 19039, 5161, 2039, 363, 15724, 1058, 1122, 451, 505, 278, 1021, 9128, 633, 9770, 470, 286, 1337, 537, 11174, 29889, 2398, 29892, 372, 29915, 29879, 7618, 1455, 304, 2948, 1438, 13071, 411, 4771, 24858, 304, 4772, 2313, 20386, 7606, 23274, 2729, 14419, 368, 373, 5046, 29889, 383, 4015, 263, 17346, 393, 5662, 1973, 15332, 1550, 2504, 11427, 20978, 2068, 363, 15724, 310, 599, 24646, 338, 1820, 29889, 3374, 366, 1716, 363, 3033, 6751, 297, 445, 6593, 1319, 10679, 29889, 2]
labels:
As a 20-year-old individual, I appreciate the perspectives shared by the 40-year-old individual (middle) and the 60-year-old individual (old) on age limits for recreational activities. It's essential to consider both safety concerns and the potential perpetuation of stereotypes when discussing such restrictions. Setting age limits can be a way to prioritize safety and prevent unnecessary risks for individuals who may not have the same physical abilities or maturity levels. However, it's crucial to approach these limits with sensitivity to avoid discriminatory practices based solely on age. Finding a balance that ensures safety while promoting inclusivity for individuals of all ages is key. Thank you both for engaging in this meaningful discussion.</s>
[INFO|training_args.py:1934] 2024-05-28 06:47:52,213 >> PyTorch: setting up devices
Running tokenizer on dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Running tokenizer on dataset: 100%|██████████| 1000/1000 [00:01<00:00, 664.37 examples/s]Running tokenizer on dataset: 100%|██████████| 1000/1000 [00:01<00:00, 662.33 examples/s]
Running tokenizer on dataset: 100%|██████████| 1000/1000 [00:01<00:00, 659.03 examples/s]Running tokenizer on dataset: 100%|██████████| 1000/1000 [00:01<00:00, 657.10 examples/s]Running tokenizer on dataset: 100%|██████████| 1000/1000 [00:01<00:00, 656.97 examples/s]
Running tokenizer on dataset: 100%|██████████| 1000/1000 [00:01<00:00, 655.10 examples/s]
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Traceback (most recent call last):
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 14, in <module>
    main()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/tuner.py", line 31, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/student2021/srcao/LLaMA-Factory-main/src/llmtuner/train/sft/workflow.py", line 57, in run_sft
    trainer = CustomSeq2SeqTrainer(
  File "/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/trainer_seq2seq.py", line 57, in __init__
    super().__init__(
  File "/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/trainer.py", line 495, in __init__
    self._move_model_to_device(model, args.device)
  File "/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/trainer.py", line 736, in _move_model_to_device
    model = model.to(device)
  File "/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 31.75 GiB total capacity; 8.04 GiB already allocated; 64.94 MiB free; 8.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-05-28 06:49:17,584] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 5642
[2024-05-28 06:49:17,585] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 5643
[2024-05-28 06:49:18,134] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 5644
[2024-05-28 06:49:18,484] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 5645
[2024-05-28 06:49:18,818] [ERROR] [launch.py:322:sigkill_handler] ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=3', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train-1000', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--max_samples', '1000', '--num_train_epochs', '3.0', '--plot_loss', '--fp16', '--report_to=wandb'] exits with return code = 1
[2024-05-28 07:42:53,295] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 07:42:55,142] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-28 07:42:55,164] [INFO] [runner.py:568:main] cmd = /home/student2021/anaconda3/envs/llama_factory/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None src/train_bash.py --stage sft --do_train --model_name_or_path /home/student2021/srcao/base-model/llama2-7b --dataset multi_role_train --template default --finetuning_type lora --lora_alpha 16 --lora_dropout 0.05 --lora_rank 8 --lora_target q_proj,v_proj --output_dir sft_checkpoint/llama2-7b-multi_role_train-3000-1.0 --overwrite_cache --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --lr_scheduler_type linear --warmup_steps 200 --max_length 1024 --logging_steps 10 --save_steps 1000 --learning_rate 5e-5 --max_samples 3000 --num_train_epochs 1.0 --plot_loss --fp16 --report_to=wandb
[2024-05-28 07:42:57,018] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 07:42:58,717] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-05-28 07:42:58,717] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-05-28 07:42:58,717] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-05-28 07:42:58,717] [INFO] [launch.py:163:main] dist_world_size=4
[2024-05-28 07:42:58,717] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-05-28 07:42:58,718] [INFO] [launch.py:253:main] process 13704 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=0', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train-3000-1.0', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--max_samples', '3000', '--num_train_epochs', '1.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-28 07:42:58,718] [INFO] [launch.py:253:main] process 13705 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=1', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train-3000-1.0', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--max_samples', '3000', '--num_train_epochs', '1.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-28 07:42:58,719] [INFO] [launch.py:253:main] process 13706 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=2', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train-3000-1.0', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--max_samples', '3000', '--num_train_epochs', '1.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-28 07:42:58,719] [INFO] [launch.py:253:main] process 13707 spawned with command: ['/home/student2021/anaconda3/envs/llama_factory/bin/python', '-u', 'src/train_bash.py', '--local_rank=3', '--stage', 'sft', '--do_train', '--model_name_or_path', '/home/student2021/srcao/base-model/llama2-7b', '--dataset', 'multi_role_train', '--template', 'default', '--finetuning_type', 'lora', '--lora_alpha', '16', '--lora_dropout', '0.05', '--lora_rank', '8', '--lora_target', 'q_proj,v_proj', '--output_dir', 'sft_checkpoint/llama2-7b-multi_role_train-3000-1.0', '--overwrite_cache', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '4', '--lr_scheduler_type', 'linear', '--warmup_steps', '200', '--max_length', '1024', '--logging_steps', '10', '--save_steps', '1000', '--learning_rate', '5e-5', '--max_samples', '3000', '--num_train_epochs', '1.0', '--plot_loss', '--fp16', '--report_to=wandb']
[2024-05-28 07:43:02,795] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 07:43:02,796] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 07:43:02,800] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-28 07:43:02,825] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
05/28/2024 07:43:08 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/28/2024 07:43:08 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1934] 2024-05-28 07:43:08,668 >> PyTorch: setting up devices
05/28/2024 07:43:08 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
05/28/2024 07:43:08 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/28/2024 07:43:08 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/28/2024 07:43:08 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/28/2024 07:43:08 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train-3000-1.0/runs/May28_07-43-07_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train-3000-1.0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train-3000-1.0,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/28/2024 07:43:08 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train-3000-1.0/runs/May28_07-43-07_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train-3000-1.0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train-3000-1.0,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
05/28/2024 07:43:08 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/28/2024 07:43:08 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train-3000-1.0/runs/May28_07-43-07_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train-3000-1.0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train-3000-1.0,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
05/28/2024 07:43:08 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
05/28/2024 07:43:08 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=sft_checkpoint/llama2-7b-multi_role_train-3000-1.0/runs/May28_07-43-07_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=sft_checkpoint/llama2-7b-multi_role_train-3000-1.0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sft_checkpoint/llama2-7b-multi_role_train-3000-1.0,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=200,
weight_decay=0.0,
)
[INFO|tokenization_utils_base.py:2082] 2024-05-28 07:43:08,734 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2082] 2024-05-28 07:43:08,734 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2082] 2024-05-28 07:43:08,735 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2082] 2024-05-28 07:43:08,735 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2082] 2024-05-28 07:43:08,735 >> loading file tokenizer.json
[INFO|configuration_utils.py:724] 2024-05-28 07:43:08,848 >> loading configuration file /home/student2021/srcao/base-model/llama2-7b/config.json
[INFO|configuration_utils.py:789] 2024-05-28 07:43:08,849 >> Model config LlamaConfig {
  "_name_or_path": "/home/student2021/srcao/base-model/llama2-7b",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3280] 2024-05-28 07:43:08,869 >> loading weights file /home/student2021/srcao/base-model/llama2-7b/model.safetensors.index.json
[INFO|modeling_utils.py:1417] 2024-05-28 07:43:08,869 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-05-28 07:43:08,870 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.43it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.39it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.37it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.36it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.59it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.56it/s]
05/28/2024 07:43:12 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/28/2024 07:43:12 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.58it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.55it/s]
[INFO|modeling_utils.py:4024] 2024-05-28 07:43:12,045 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-05-28 07:43:12,045 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/student2021/srcao/base-model/llama2-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:881] 2024-05-28 07:43:12,048 >> loading configuration file /home/student2021/srcao/base-model/llama2-7b/generation_config.json
[INFO|configuration_utils.py:928] 2024-05-28 07:43:12,048 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

05/28/2024 07:43:12 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/28/2024 07:43:12 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.55it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.52it/s]
05/28/2024 07:43:12 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/28/2024 07:43:12 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.58it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.54it/s]
05/28/2024 07:43:12 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
05/28/2024 07:43:12 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
05/28/2024 07:43:12 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/28/2024 07:43:12 - INFO - llmtuner.data.template - Add pad token: </s>
05/28/2024 07:43:12 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/28/2024 07:43:12 - INFO - llmtuner.data.template - Add pad token: </s>
05/28/2024 07:43:12 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/28/2024 07:43:12 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
05/28/2024 07:43:12 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/28/2024 07:43:12 - INFO - llmtuner.data.template - Add pad token: </s>
05/28/2024 07:43:12 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
05/28/2024 07:43:12 - INFO - llmtuner.data.template - Add pad token: </s>
Using custom data configuration default-a0e2d6f8a6470c31
Loading Dataset Infos from /home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
Found cached dataset json (/home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
Loading Dataset info from /home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
Converting format of dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]Caching processed dataset at /home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-07802a83219aee51.arrow
Converting format of dataset: 100%|██████████| 3000/3000 [00:00<00:00, 70505.54 examples/s]
05/28/2024 07:43:14 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/28/2024 07:43:14 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/28/2024 07:43:14 - INFO - llmtuner.data.loader - Loading dataset multi-role-sft-train.json...
05/28/2024 07:43:14 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
05/28/2024 07:43:14 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
05/28/2024 07:43:14 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
Running tokenizer on dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]Converting format of dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]Converting format of dataset: 100%|██████████| 3000/3000 [00:00<00:00, 73000.70 examples/s]
Converting format of dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]Converting format of dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]Converting format of dataset: 100%|██████████| 3000/3000 [00:00<00:00, 77992.45 examples/s]
Converting format of dataset: 100%|██████████| 3000/3000 [00:00<00:00, 62189.43 examples/s]
Caching processed dataset at /home/student2021/.cache/huggingface/datasets/json/default-a0e2d6f8a6470c31/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-8efc0cb2f25aa9cb.arrow
Running tokenizer on dataset:  33%|███▎      | 1000/3000 [00:01<00:02, 669.04 examples/s]Running tokenizer on dataset:  67%|██████▋   | 2000/3000 [00:02<00:01, 679.69 examples/s]Running tokenizer on dataset: 100%|██████████| 3000/3000 [00:04<00:00, 676.63 examples/s]Running tokenizer on dataset: 100%|██████████| 3000/3000 [00:04<00:00, 675.80 examples/s]
input_ids:
[12968, 29901, 13355, 1598, 322, 5353, 278, 2775, 537, 310, 5046, 269, 12358, 327, 7384, 322, 4768, 2129, 2729, 373, 278, 7928, 434, 29889, 13, 2677, 29901, 8565, 1558, 291, 1048, 29871, 29906, 29955, 29945, 29889, 10575, 727, 367, 5046, 13071, 363, 5221, 1218, 297, 3058, 28709, 1288, 14188, 9808, 13, 13, 7900, 22137, 29901, 29871, 1094, 263, 29871, 29906, 29900, 29899, 6360, 29899, 1025, 5375, 29892, 306, 11188, 278, 3736, 1103, 3145, 7258, 491, 278, 29871, 29946, 29900, 29899, 6360, 29899, 1025, 5375, 313, 17662, 29897, 322, 278, 29871, 29953, 29900, 29899, 6360, 29899, 1025, 5375, 313, 1025, 29897, 373, 5046, 13071, 363, 28709, 1288, 14188, 29889, 739, 29915, 29879, 18853, 304, 2050, 1716, 15332, 21838, 322, 278, 7037, 25722, 29884, 362, 310, 269, 12358, 327, 7384, 746, 5353, 292, 1316, 25091, 29889, 21605, 5046, 13071, 508, 367, 263, 982, 304, 7536, 277, 675, 15332, 322, 5557, 19039, 5161, 2039, 363, 15724, 1058, 1122, 451, 505, 278, 1021, 9128, 633, 9770, 470, 286, 1337, 537, 11174, 29889, 2398, 29892, 372, 29915, 29879, 7618, 1455, 304, 2948, 1438, 13071, 411, 4771, 24858, 304, 4772, 2313, 20386, 7606, 23274, 2729, 14419, 368, 373, 5046, 29889, 383, 4015, 263, 17346, 393, 5662, 1973, 15332, 1550, 2504, 11427, 20978, 2068, 363, 15724, 310, 599, 24646, 338, 1820, 29889, 3374, 366, 1716, 363, 3033, 6751, 297, 445, 6593, 1319, 10679, 29889, 2]
inputs:
Human: Identify and discuss the severity of age stereotypes and biases based on the dialogue.
Context: Discussion about 275. Should there be age limits for participating in certain recreational activities?.

Assistant:  As a 20-year-old individual, I appreciate the perspectives shared by the 40-year-old individual (middle) and the 60-year-old individual (old) on age limits for recreational activities. It's essential to consider both safety concerns and the potential perpetuation of stereotypes when discussing such restrictions. Setting age limits can be a way to prioritize safety and prevent unnecessary risks for individuals who may not have the same physical abilities or maturity levels. However, it's crucial to approach these limits with sensitivity to avoid discriminatory practices based solely on age. Finding a balance that ensures safety while promoting inclusivity for individuals of all ages is key. Thank you both for engaging in this meaningful discussion.</s>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1094, 263, 29871, 29906, 29900, 29899, 6360, 29899, 1025, 5375, 29892, 306, 11188, 278, 3736, 1103, 3145, 7258, 491, 278, 29871, 29946, 29900, 29899, 6360, 29899, 1025, 5375, 313, 17662, 29897, 322, 278, 29871, 29953, 29900, 29899, 6360, 29899, 1025, 5375, 313, 1025, 29897, 373, 5046, 13071, 363, 28709, 1288, 14188, 29889, 739, 29915, 29879, 18853, 304, 2050, 1716, 15332, 21838, 322, 278, 7037, 25722, 29884, 362, 310, 269, 12358, 327, 7384, 746, 5353, 292, 1316, 25091, 29889, 21605, 5046, 13071, 508, 367, 263, 982, 304, 7536, 277, 675, 15332, 322, 5557, 19039, 5161, 2039, 363, 15724, 1058, 1122, 451, 505, 278, 1021, 9128, 633, 9770, 470, 286, 1337, 537, 11174, 29889, 2398, 29892, 372, 29915, 29879, 7618, 1455, 304, 2948, 1438, 13071, 411, 4771, 24858, 304, 4772, 2313, 20386, 7606, 23274, 2729, 14419, 368, 373, 5046, 29889, 383, 4015, 263, 17346, 393, 5662, 1973, 15332, 1550, 2504, 11427, 20978, 2068, 363, 15724, 310, 599, 24646, 338, 1820, 29889, 3374, 366, 1716, 363, 3033, 6751, 297, 445, 6593, 1319, 10679, 29889, 2]
labels:
As a 20-year-old individual, I appreciate the perspectives shared by the 40-year-old individual (middle) and the 60-year-old individual (old) on age limits for recreational activities. It's essential to consider both safety concerns and the potential perpetuation of stereotypes when discussing such restrictions. Setting age limits can be a way to prioritize safety and prevent unnecessary risks for individuals who may not have the same physical abilities or maturity levels. However, it's crucial to approach these limits with sensitivity to avoid discriminatory practices based solely on age. Finding a balance that ensures safety while promoting inclusivity for individuals of all ages is key. Thank you both for engaging in this meaningful discussion.</s>
[INFO|training_args.py:1934] 2024-05-28 07:43:19,344 >> PyTorch: setting up devices
Running tokenizer on dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Running tokenizer on dataset:  33%|███▎      | 1000/3000 [00:01<00:02, 666.88 examples/s]Running tokenizer on dataset:  33%|███▎      | 1000/3000 [00:01<00:03, 661.46 examples/s]Running tokenizer on dataset:  33%|███▎      | 1000/3000 [00:01<00:03, 644.28 examples/s]Running tokenizer on dataset:  67%|██████▋   | 2000/3000 [00:02<00:01, 675.86 examples/s]Running tokenizer on dataset:  67%|██████▋   | 2000/3000 [00:02<00:01, 671.56 examples/s]Running tokenizer on dataset:  67%|██████▋   | 2000/3000 [00:03<00:01, 656.86 examples/s]Running tokenizer on dataset: 100%|██████████| 3000/3000 [00:04<00:00, 673.25 examples/s]Running tokenizer on dataset: 100%|██████████| 3000/3000 [00:04<00:00, 672.38 examples/s]
Running tokenizer on dataset: 100%|██████████| 3000/3000 [00:04<00:00, 666.96 examples/s]Running tokenizer on dataset: 100%|██████████| 3000/3000 [00:04<00:00, 666.58 examples/s]
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Running tokenizer on dataset: 100%|██████████| 3000/3000 [00:04<00:00, 655.09 examples/s]Running tokenizer on dataset: 100%|██████████| 3000/3000 [00:04<00:00, 653.62 examples/s]
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/transformers/training_args.py:1847: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.
  warnings.warn(
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:607] 2024-05-28 07:44:07,606 >> Using auto half precision backend
[INFO|trainer.py:1969] 2024-05-28 07:44:08,083 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-05-28 07:44:08,083 >>   Num examples = 3,000
[INFO|trainer.py:1971] 2024-05-28 07:44:08,083 >>   Num Epochs = 1
[INFO|trainer.py:1972] 2024-05-28 07:44:08,083 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-05-28 07:44:08,083 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1976] 2024-05-28 07:44:08,083 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1977] 2024-05-28 07:44:08,083 >>   Total optimization steps = 187
[INFO|trainer.py:1978] 2024-05-28 07:44:08,089 >>   Number of trainable parameters = 4,194,304
[INFO|integration_utils.py:723] 2024-05-28 07:44:08,268 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: luli662413. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in /home/student2021/srcao/LLaMA-Factory-main/wandb/run-20240528_074409-d72u1api
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-lake-1
wandb: ⭐️ View project at https://wandb.ai/luli662413/huggingface
wandb: 🚀 View run at https://wandb.ai/luli662413/huggingface/runs/d72u1api/workspace
  0%|          | 0/187 [00:00<?, ?it/s]  1%|          | 1/187 [00:03<12:23,  4.00s/it]  1%|          | 2/187 [00:05<06:59,  2.27s/it]  2%|▏         | 3/187 [00:06<05:15,  1.72s/it]  2%|▏         | 4/187 [00:07<04:32,  1.49s/it]  3%|▎         | 5/187 [00:08<04:01,  1.33s/it]  3%|▎         | 6/187 [00:09<03:42,  1.23s/it]  4%|▎         | 7/187 [00:10<03:31,  1.17s/it]  4%|▍         | 8/187 [00:11<03:23,  1.14s/it]  5%|▍         | 9/187 [00:12<03:21,  1.13s/it]  5%|▌         | 10/187 [00:13<03:17,  1.11s/it]                                                {'loss': 1.8539, 'grad_norm': 0.2964975833892822, 'learning_rate': 2.5e-06, 'epoch': 0.05}
  5%|▌         | 10/187 [00:13<03:17,  1.11s/it]  6%|▌         | 11/187 [00:14<03:20,  1.14s/it]  6%|▋         | 12/187 [00:15<03:13,  1.11s/it]  7%|▋         | 13/187 [00:16<03:09,  1.09s/it]  7%|▋         | 14/187 [00:18<03:07,  1.09s/it]  8%|▊         | 15/187 [00:19<03:04,  1.07s/it]  9%|▊         | 16/187 [00:20<03:02,  1.07s/it]  9%|▉         | 17/187 [00:21<03:00,  1.06s/it] 10%|▉         | 18/187 [00:22<03:00,  1.07s/it] 10%|█         | 19/187 [00:23<02:58,  1.06s/it] 11%|█         | 20/187 [00:24<02:56,  1.06s/it]                                                {'loss': 1.8146, 'grad_norm': 0.3136788010597229, 'learning_rate': 5e-06, 'epoch': 0.11}
 11%|█         | 20/187 [00:24<02:56,  1.06s/it] 11%|█         | 21/187 [00:25<02:54,  1.05s/it] 12%|█▏        | 22/187 [00:26<02:53,  1.05s/it] 12%|█▏        | 23/187 [00:27<02:54,  1.06s/it] 13%|█▎        | 24/187 [00:28<02:52,  1.06s/it] 13%|█▎        | 25/187 [00:29<02:50,  1.05s/it] 14%|█▍        | 26/187 [00:30<02:49,  1.05s/it] 14%|█▍        | 27/187 [00:31<02:47,  1.05s/it] 15%|█▍        | 28/187 [00:32<02:46,  1.05s/it] 16%|█▌        | 29/187 [00:33<02:46,  1.06s/it] 16%|█▌        | 30/187 [00:34<02:45,  1.05s/it]                                                {'loss': 1.9011, 'grad_norm': 0.3270252048969269, 'learning_rate': 7.5e-06, 'epoch': 0.16}
 16%|█▌        | 30/187 [00:34<02:45,  1.05s/it] 17%|█▋        | 31/187 [00:35<02:43,  1.05s/it] 17%|█▋        | 32/187 [00:36<02:43,  1.05s/it] 18%|█▊        | 33/187 [00:37<02:40,  1.04s/it] 18%|█▊        | 34/187 [00:39<02:39,  1.05s/it] 19%|█▊        | 35/187 [00:40<02:38,  1.04s/it] 19%|█▉        | 36/187 [00:41<02:39,  1.05s/it] 20%|█▉        | 37/187 [00:42<02:39,  1.06s/it] 20%|██        | 38/187 [00:43<02:36,  1.05s/it] 21%|██        | 39/187 [00:44<02:34,  1.05s/it] 21%|██▏       | 40/187 [00:45<02:34,  1.05s/it]                                                {'loss': 1.7935, 'grad_norm': 0.3782724142074585, 'learning_rate': 1e-05, 'epoch': 0.21}
 21%|██▏       | 40/187 [00:45<02:34,  1.05s/it] 22%|██▏       | 41/187 [00:46<02:39,  1.09s/it] 22%|██▏       | 42/187 [00:47<02:37,  1.08s/it] 23%|██▎       | 43/187 [00:48<02:36,  1.09s/it] 24%|██▎       | 44/187 [00:49<02:33,  1.07s/it] 24%|██▍       | 45/187 [00:50<02:31,  1.07s/it] 25%|██▍       | 46/187 [00:51<02:30,  1.07s/it] 25%|██▌       | 47/187 [00:52<02:28,  1.06s/it] 26%|██▌       | 48/187 [00:53<02:28,  1.07s/it] 26%|██▌       | 49/187 [00:55<02:33,  1.11s/it] 27%|██▋       | 50/187 [00:56<02:30,  1.10s/it]                                                {'loss': 1.7784, 'grad_norm': 0.4572581648826599, 'learning_rate': 1.25e-05, 'epoch': 0.27}
 27%|██▋       | 50/187 [00:56<02:30,  1.10s/it] 27%|██▋       | 51/187 [00:57<02:28,  1.09s/it] 28%|██▊       | 52/187 [00:58<02:25,  1.08s/it] 28%|██▊       | 53/187 [00:59<02:22,  1.06s/it] 29%|██▉       | 54/187 [01:00<02:21,  1.06s/it] 29%|██▉       | 55/187 [01:01<02:20,  1.07s/it] 30%|██▉       | 56/187 [01:02<02:19,  1.07s/it] 30%|███       | 57/187 [01:03<02:22,  1.10s/it] 31%|███       | 58/187 [01:04<02:20,  1.09s/it] 32%|███▏      | 59/187 [01:05<02:18,  1.08s/it] 32%|███▏      | 60/187 [01:06<02:16,  1.08s/it]                                                {'loss': 1.776, 'grad_norm': 0.5265453457832336, 'learning_rate': 1.5e-05, 'epoch': 0.32}
 32%|███▏      | 60/187 [01:06<02:16,  1.08s/it] 33%|███▎      | 61/187 [01:08<02:14,  1.07s/it] 33%|███▎      | 62/187 [01:09<02:12,  1.06s/it] 34%|███▎      | 63/187 [01:10<02:11,  1.06s/it] 34%|███▍      | 64/187 [01:11<02:09,  1.06s/it] 35%|███▍      | 65/187 [01:12<02:10,  1.07s/it] 35%|███▌      | 66/187 [01:13<02:12,  1.10s/it] 36%|███▌      | 67/187 [01:14<02:11,  1.09s/it] 36%|███▋      | 68/187 [01:15<02:13,  1.12s/it] 37%|███▋      | 69/187 [01:16<02:10,  1.11s/it] 37%|███▋      | 70/187 [01:17<02:07,  1.09s/it]                                                {'loss': 1.7221, 'grad_norm': 0.5070095658302307, 'learning_rate': 1.75e-05, 'epoch': 0.37}
 37%|███▋      | 70/187 [01:17<02:07,  1.09s/it] 38%|███▊      | 71/187 [01:18<02:05,  1.08s/it] 39%|███▊      | 72/187 [01:19<02:04,  1.08s/it] 39%|███▉      | 73/187 [01:21<02:02,  1.08s/it] 40%|███▉      | 74/187 [01:22<02:04,  1.10s/it] 40%|████      | 75/187 [01:23<02:01,  1.08s/it] 41%|████      | 76/187 [01:24<02:02,  1.11s/it] 41%|████      | 77/187 [01:25<02:05,  1.14s/it] 42%|████▏     | 78/187 [01:26<02:03,  1.13s/it] 42%|████▏     | 79/187 [01:27<02:01,  1.12s/it] 43%|████▎     | 80/187 [01:28<01:59,  1.12s/it]                                                {'loss': 1.7182, 'grad_norm': 0.6131771802902222, 'learning_rate': 2e-05, 'epoch': 0.43}
 43%|████▎     | 80/187 [01:28<01:59,  1.12s/it] 43%|████▎     | 81/187 [01:30<01:59,  1.12s/it] 44%|████▍     | 82/187 [01:31<01:57,  1.12s/it] 44%|████▍     | 83/187 [01:32<01:55,  1.11s/it] 45%|████▍     | 84/187 [01:33<01:54,  1.11s/it] 45%|████▌     | 85/187 [01:34<01:51,  1.09s/it] 46%|████▌     | 86/187 [01:35<01:49,  1.08s/it] 47%|████▋     | 87/187 [01:36<01:47,  1.07s/it] 47%|████▋     | 88/187 [01:37<01:46,  1.07s/it] 48%|████▊     | 89/187 [01:38<01:44,  1.06s/it] 48%|████▊     | 90/187 [01:39<01:42,  1.06s/it]                                                {'loss': 1.6378, 'grad_norm': 0.6864504218101501, 'learning_rate': 2.2250000000000002e-05, 'epoch': 0.48}
 48%|████▊     | 90/187 [01:39<01:42,  1.06s/it] 49%|████▊     | 91/187 [01:40<01:43,  1.07s/it] 49%|████▉     | 92/187 [01:41<01:41,  1.07s/it] 50%|████▉     | 93/187 [01:42<01:40,  1.07s/it] 50%|█████     | 94/187 [01:44<01:39,  1.07s/it] 51%|█████     | 95/187 [01:45<01:37,  1.06s/it] 51%|█████▏    | 96/187 [01:46<01:36,  1.06s/it] 52%|█████▏    | 97/187 [01:47<01:36,  1.07s/it] 52%|█████▏    | 98/187 [01:48<01:34,  1.07s/it] 53%|█████▎    | 99/187 [01:49<01:36,  1.10s/it] 53%|█████▎    | 100/187 [01:50<01:34,  1.09s/it]                                                 {'loss': 1.5533, 'grad_norm': 1.38911771774292, 'learning_rate': 2.4750000000000002e-05, 'epoch': 0.53}
 53%|█████▎    | 100/187 [01:50<01:34,  1.09s/it] 54%|█████▍    | 101/187 [01:51<01:32,  1.08s/it] 55%|█████▍    | 102/187 [01:52<01:33,  1.10s/it] 55%|█████▌    | 103/187 [01:53<01:31,  1.09s/it] 56%|█████▌    | 104/187 [01:54<01:32,  1.11s/it] 56%|█████▌    | 105/187 [01:56<01:30,  1.10s/it] 57%|█████▋    | 106/187 [01:57<01:28,  1.09s/it] 57%|█████▋    | 107/187 [01:58<01:26,  1.08s/it] 58%|█████▊    | 108/187 [01:59<01:24,  1.07s/it] 58%|█████▊    | 109/187 [02:00<01:24,  1.09s/it] 59%|█████▉    | 110/187 [02:01<01:22,  1.08s/it]                                                 {'loss': 1.5067, 'grad_norm': 0.8450387120246887, 'learning_rate': 2.725e-05, 'epoch': 0.59}
 59%|█████▉    | 110/187 [02:01<01:22,  1.08s/it] 59%|█████▉    | 111/187 [02:02<01:21,  1.08s/it] 60%|█████▉    | 112/187 [02:03<01:19,  1.07s/it] 60%|██████    | 113/187 [02:04<01:18,  1.06s/it] 61%|██████    | 114/187 [02:05<01:18,  1.07s/it] 61%|██████▏   | 115/187 [02:06<01:17,  1.07s/it] 62%|██████▏   | 116/187 [02:07<01:16,  1.08s/it] 63%|██████▎   | 117/187 [02:08<01:14,  1.07s/it] 63%|██████▎   | 118/187 [02:09<01:14,  1.07s/it] 64%|██████▎   | 119/187 [02:10<01:12,  1.06s/it] 64%|██████▍   | 120/187 [02:12<01:12,  1.08s/it]                                                 {'loss': 1.3741, 'grad_norm': 0.9679073691368103, 'learning_rate': 2.975e-05, 'epoch': 0.64}
 64%|██████▍   | 120/187 [02:12<01:12,  1.08s/it] 65%|██████▍   | 121/187 [02:13<01:11,  1.08s/it] 65%|██████▌   | 122/187 [02:14<01:09,  1.08s/it] 66%|██████▌   | 123/187 [02:15<01:09,  1.09s/it] 66%|██████▋   | 124/187 [02:16<01:11,  1.14s/it] 67%|██████▋   | 125/187 [02:17<01:09,  1.11s/it] 67%|██████▋   | 126/187 [02:18<01:07,  1.10s/it] 68%|██████▊   | 127/187 [02:19<01:05,  1.09s/it] 68%|██████▊   | 128/187 [02:20<01:05,  1.11s/it] 69%|██████▉   | 129/187 [02:22<01:05,  1.12s/it] 70%|██████▉   | 130/187 [02:23<01:03,  1.12s/it]                                                 {'loss': 1.2924, 'grad_norm': 0.5882244110107422, 'learning_rate': 3.2250000000000005e-05, 'epoch': 0.69}
 70%|██████▉   | 130/187 [02:23<01:03,  1.12s/it] 70%|███████   | 131/187 [02:24<01:01,  1.10s/it] 71%|███████   | 132/187 [02:25<00:59,  1.09s/it] 71%|███████   | 133/187 [02:26<00:57,  1.07s/it] 72%|███████▏  | 134/187 [02:27<00:57,  1.08s/it] 72%|███████▏  | 135/187 [02:28<00:56,  1.09s/it] 73%|███████▎  | 136/187 [02:29<00:54,  1.07s/it] 73%|███████▎  | 137/187 [02:30<00:53,  1.06s/it] 74%|███████▍  | 138/187 [02:31<00:52,  1.07s/it] 74%|███████▍  | 139/187 [02:32<00:50,  1.06s/it] 75%|███████▍  | 140/187 [02:33<00:49,  1.06s/it]                                                 {'loss': 1.2737, 'grad_norm': 1.089496374130249, 'learning_rate': 3.475e-05, 'epoch': 0.75}
 75%|███████▍  | 140/187 [02:33<00:49,  1.06s/it] 75%|███████▌  | 141/187 [02:34<00:48,  1.06s/it] 76%|███████▌  | 142/187 [02:35<00:47,  1.06s/it] 76%|███████▋  | 143/187 [02:37<00:46,  1.06s/it] 77%|███████▋  | 144/187 [02:38<00:45,  1.05s/it] 78%|███████▊  | 145/187 [02:39<00:44,  1.05s/it] 78%|███████▊  | 146/187 [02:40<00:43,  1.06s/it] 79%|███████▊  | 147/187 [02:41<00:42,  1.06s/it] 79%|███████▉  | 148/187 [02:42<00:41,  1.06s/it] 80%|███████▉  | 149/187 [02:43<00:40,  1.07s/it] 80%|████████  | 150/187 [02:44<00:39,  1.06s/it]                                                 {'loss': 1.244, 'grad_norm': 0.6728262305259705, 'learning_rate': 3.7250000000000004e-05, 'epoch': 0.8}
 80%|████████  | 150/187 [02:44<00:39,  1.06s/it] 81%|████████  | 151/187 [02:45<00:38,  1.06s/it] 81%|████████▏ | 152/187 [02:46<00:37,  1.06s/it] 82%|████████▏ | 153/187 [02:47<00:36,  1.06s/it] 82%|████████▏ | 154/187 [02:48<00:35,  1.09s/it] 83%|████████▎ | 155/187 [02:49<00:34,  1.07s/it] 83%|████████▎ | 156/187 [02:50<00:33,  1.09s/it] 84%|████████▍ | 157/187 [02:51<00:32,  1.09s/it] 84%|████████▍ | 158/187 [02:53<00:31,  1.08s/it] 85%|████████▌ | 159/187 [02:54<00:31,  1.13s/it] 86%|████████▌ | 160/187 [02:55<00:30,  1.12s/it]                                                 {'loss': 1.2038, 'grad_norm': 0.8401338458061218, 'learning_rate': 3.9750000000000004e-05, 'epoch': 0.85}
 86%|████████▌ | 160/187 [02:55<00:30,  1.12s/it] 86%|████████▌ | 161/187 [02:56<00:29,  1.14s/it] 87%|████████▋ | 162/187 [02:57<00:27,  1.11s/it] 87%|████████▋ | 163/187 [02:58<00:26,  1.09s/it] 88%|████████▊ | 164/187 [02:59<00:24,  1.07s/it] 88%|████████▊ | 165/187 [03:00<00:23,  1.07s/it] 89%|████████▉ | 166/187 [03:01<00:22,  1.07s/it] 89%|████████▉ | 167/187 [03:02<00:21,  1.07s/it] 90%|████████▉ | 168/187 [03:03<00:20,  1.06s/it] 90%|█████████ | 169/187 [03:04<00:19,  1.06s/it] 91%|█████████ | 170/187 [03:06<00:17,  1.05s/it]                                                 {'loss': 1.2591, 'grad_norm': 0.9307658672332764, 'learning_rate': 4.2250000000000004e-05, 'epoch': 0.91}
 91%|█████████ | 170/187 [03:06<00:17,  1.05s/it] 91%|█████████▏| 171/187 [03:07<00:17,  1.07s/it] 92%|█████████▏| 172/187 [03:08<00:15,  1.07s/it] 93%|█████████▎| 173/187 [03:09<00:14,  1.07s/it] 93%|█████████▎| 174/187 [03:10<00:13,  1.07s/it] 94%|█████████▎| 175/187 [03:11<00:12,  1.06s/it] 94%|█████████▍| 176/187 [03:12<00:11,  1.06s/it] 95%|█████████▍| 177/187 [03:13<00:10,  1.07s/it] 95%|█████████▌| 178/187 [03:14<00:09,  1.07s/it] 96%|█████████▌| 179/187 [03:15<00:08,  1.10s/it] 96%|█████████▋| 180/187 [03:16<00:07,  1.09s/it]                                                 {'loss': 1.1836, 'grad_norm': 2.6011102199554443, 'learning_rate': 4.4750000000000004e-05, 'epoch': 0.96}
 96%|█████████▋| 180/187 [03:16<00:07,  1.09s/it] 97%|█████████▋| 181/187 [03:17<00:06,  1.07s/it] 97%|█████████▋| 182/187 [03:18<00:05,  1.06s/it] 98%|█████████▊| 183/187 [03:19<00:04,  1.05s/it] 98%|█████████▊| 184/187 [03:21<00:03,  1.05s/it] 99%|█████████▉| 185/187 [03:22<00:02,  1.07s/it] 99%|█████████▉| 186/187 [03:23<00:01,  1.07s/it]100%|██████████| 187/187 [03:24<00:00,  1.07s/it][INFO|trainer.py:2231] 2024-05-28 07:47:40,692 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 212.6031, 'train_samples_per_second': 14.111, 'train_steps_per_second': 0.88, 'train_loss': 1.5336051420731978, 'epoch': 1.0}
100%|██████████| 187/187 [03:24<00:00,  1.07s/it]100%|██████████| 187/187 [03:24<00:00,  1.09s/it]
[INFO|trainer.py:3203] 2024-05-28 07:47:40,712 >> Saving model checkpoint to sft_checkpoint/llama2-7b-multi_role_train-3000-1.0
/home/student2021/anaconda3/envs/llama_factory/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /home/student2021/srcao/base-model/llama2-7b - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2502] 2024-05-28 07:47:40,819 >> tokenizer config file saved in sft_checkpoint/llama2-7b-multi_role_train-3000-1.0/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-28 07:47:40,820 >> Special tokens file saved in sft_checkpoint/llama2-7b-multi_role_train-3000-1.0/special_tokens_map.json
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     1.5336
  train_runtime            = 0:03:32.60
  train_samples_per_second =     14.111
  train_steps_per_second   =       0.88
Figure saved: sft_checkpoint/llama2-7b-multi_role_train-3000-1.0/training_loss.png
05/28/2024 07:47:41 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:450] 2024-05-28 07:47:41,171 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-05-28 07:47:42,033] [INFO] [launch.py:348:main] Process 13707 exits successfully.
[2024-05-28 07:47:43,039] [INFO] [launch.py:348:main] Process 13706 exits successfully.
[2024-05-28 07:47:43,039] [INFO] [launch.py:348:main] Process 13705 exits successfully.
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.014 MB of 0.024 MB uploadedwandb: | 0.014 MB of 0.024 MB uploadedwandb: / 0.024 MB of 0.024 MB uploadedwandb: 
wandb: Run history:
wandb:         train/epoch ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██
wandb:   train/global_step ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██
wandb:     train/grad_norm ▁▁▁▁▁▂▂▂▂▄▃▃▂▃▂▃▃█
wandb: train/learning_rate ▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██
wandb:          train/loss █▇█▇▇▇▆▆▅▅▄▃▂▂▂▁▂▁
wandb: 
wandb: Run summary:
wandb:               total_flos 2.066036840988672e+16
wandb:              train/epoch 1.0
wandb:        train/global_step 187
wandb:          train/grad_norm 2.60111
wandb:      train/learning_rate 4e-05
wandb:               train/loss 1.1836
wandb:               train_loss 1.53361
wandb:            train_runtime 212.6031
wandb: train_samples_per_second 14.111
wandb:   train_steps_per_second 0.88
wandb: 
wandb: 🚀 View run dutiful-lake-1 at: https://wandb.ai/luli662413/huggingface/runs/d72u1api/workspace
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240528_074409-d72u1api/logs
[2024-05-28 07:47:52,050] [INFO] [launch.py:348:main] Process 13704 exits successfully.
